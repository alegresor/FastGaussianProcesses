
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../examples/simple/fgp_dnb2/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>API - FastGPs</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.342714a4.min.css">
      
      
  
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
  
  <style>:root{--md-admonition-icon--note:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M1%207.775V2.75C1%201.784%201.784%201%202.75%201h5.025c.464%200%20.91.184%201.238.513l6.25%206.25a1.75%201.75%200%200%201%200%202.474l-5.026%205.026a1.75%201.75%200%200%201-2.474%200l-6.25-6.25A1.75%201.75%200%200%201%201%207.775m1.5%200c0%20.066.026.13.073.177l6.25%206.25a.25.25%200%200%200%20.354%200l5.025-5.025a.25.25%200%200%200%200-.354l-6.25-6.25a.25.25%200%200%200-.177-.073H2.75a.25.25%200%200%200-.25.25ZM6%205a1%201%200%201%201%200%202%201%201%200%200%201%200-2%22/%3E%3C/svg%3E');--md-admonition-icon--abstract:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2.5%201.75v11.5c0%20.138.112.25.25.25h3.17a.75.75%200%200%201%200%201.5H2.75A1.75%201.75%200%200%201%201%2013.25V1.75C1%20.784%201.784%200%202.75%200h8.5C12.216%200%2013%20.784%2013%201.75v7.736a.75.75%200%200%201-1.5%200V1.75a.25.25%200%200%200-.25-.25h-8.5a.25.25%200%200%200-.25.25m13.274%209.537zl-4.557%204.45a.75.75%200%200%201-1.055-.008l-1.943-1.95a.75.75%200%200%201%201.062-1.058l1.419%201.425%204.026-3.932a.75.75%200%201%201%201.048%201.074M4.75%204h4.5a.75.75%200%200%201%200%201.5h-4.5a.75.75%200%200%201%200-1.5M4%207.75A.75.75%200%200%201%204.75%207h2a.75.75%200%200%201%200%201.5h-2A.75.75%200%200%201%204%207.75%22/%3E%3C/svg%3E');--md-admonition-icon--info:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M0%208a8%208%200%201%201%2016%200A8%208%200%200%201%200%208m8-6.5a6.5%206.5%200%201%200%200%2013%206.5%206.5%200%200%200%200-13M6.5%207.75A.75.75%200%200%201%207.25%207h1a.75.75%200%200%201%20.75.75v2.75h.25a.75.75%200%200%201%200%201.5h-2a.75.75%200%200%201%200-1.5h.25v-2h-.25a.75.75%200%200%201-.75-.75M8%206a1%201%200%201%201%200-2%201%201%200%200%201%200%202%22/%3E%3C/svg%3E');--md-admonition-icon--tip:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M3.499.75a.75.75%200%200%201%201.5%200v.996C5.9%202.903%206.793%203.65%207.662%204.376l.24.202c-.036-.694.055-1.422.426-2.163C9.1.873%2010.794-.045%2012.622.26%2014.408.558%2016%201.94%2016%204.25c0%201.278-.954%202.575-2.44%202.734l.146.508.065.22c.203.701.412%201.455.476%202.226.142%201.707-.4%203.03-1.487%203.898C11.714%2014.671%2010.27%2015%208.75%2015h-6a.75.75%200%200%201%200-1.5h1.376a4.5%204.5%200%200%201-.563-1.191%203.84%203.84%200%200%201-.05-2.063%204.65%204.65%200%200%201-2.025-.293.75.75%200%200%201%20.525-1.406c1.357.507%202.376-.006%202.698-.318l.009-.01a.747.747%200%200%201%201.06%200%20.75.75%200%200%201-.012%201.074c-.912.92-.992%201.835-.768%202.586.221.74.745%201.337%201.196%201.621H8.75c1.343%200%202.398-.296%203.074-.836.635-.507%201.036-1.31.928-2.602-.05-.603-.216-1.224-.422-1.93l-.064-.221c-.12-.407-.246-.84-.353-1.29a2.4%202.4%200%200%201-.507-.441%203.1%203.1%200%200%201-.633-1.248.75.75%200%200%201%201.455-.364c.046.185.144.436.31.627.146.168.353.305.712.305.738%200%201.25-.615%201.25-1.25%200-1.47-.95-2.315-2.123-2.51-1.172-.196-2.227.387-2.706%201.345-.46.92-.27%201.774.019%203.062l.042.19.01.05c.348.443.666.949.94%201.553a.75.75%200%201%201-1.365.62c-.553-1.217-1.32-1.94-2.3-2.768L6.7%205.527c-.814-.68-1.75-1.462-2.692-2.619a3.7%203.7%200%200%200-1.023.88c-.406.495-.663%201.036-.722%201.508.116.122.306.21.591.239.388.038.797-.06%201.032-.19a.75.75%200%200%201%20.728%201.31c-.515.287-1.23.439-1.906.373-.682-.067-1.473-.38-1.879-1.193L.75%205.677V5.5c0-.984.48-1.94%201.077-2.664.46-.559%201.05-1.055%201.673-1.353z%22/%3E%3C/svg%3E');--md-admonition-icon--success:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M13.78%204.22a.75.75%200%200%201%200%201.06l-7.25%207.25a.75.75%200%200%201-1.06%200L2.22%209.28a.75.75%200%200%201%20.018-1.042.75.75%200%200%201%201.042-.018L6%2010.94l6.72-6.72a.75.75%200%200%201%201.06%200%22/%3E%3C/svg%3E');--md-admonition-icon--question:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M0%208a8%208%200%201%201%2016%200A8%208%200%200%201%200%208m8-6.5a6.5%206.5%200%201%200%200%2013%206.5%206.5%200%200%200%200-13M6.92%206.085h.001a.749.749%200%201%201-1.342-.67c.169-.339.436-.701.849-.977C6.845%204.16%207.369%204%208%204a2.76%202.76%200%200%201%201.637.525c.503.377.863.965.863%201.725%200%20.448-.115.83-.329%201.15-.205.307-.47.513-.692.662-.109.072-.22.138-.313.195l-.006.004a6%206%200%200%200-.26.16%201%201%200%200%200-.276.245.75.75%200%200%201-1.248-.832c.184-.264.42-.489.692-.661q.154-.1.313-.195l.007-.004c.1-.061.182-.11.258-.161a1%201%200%200%200%20.277-.245C8.96%206.514%209%206.427%209%206.25a.61.61%200%200%200-.262-.525A1.27%201.27%200%200%200%208%205.5c-.369%200-.595.09-.74.187a1%201%200%200%200-.34.398M9%2011a1%201%200%201%201-2%200%201%201%200%200%201%202%200%22/%3E%3C/svg%3E');--md-admonition-icon--warning:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M6.457%201.047c.659-1.234%202.427-1.234%203.086%200l6.082%2011.378A1.75%201.75%200%200%201%2014.082%2015H1.918a1.75%201.75%200%200%201-1.543-2.575Zm1.763.707a.25.25%200%200%200-.44%200L1.698%2013.132a.25.25%200%200%200%20.22.368h12.164a.25.25%200%200%200%20.22-.368Zm.53%203.996v2.5a.75.75%200%200%201-1.5%200v-2.5a.75.75%200%200%201%201.5%200M9%2011a1%201%200%201%201-2%200%201%201%200%200%201%202%200%22/%3E%3C/svg%3E');--md-admonition-icon--failure:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2.344%202.343za8%208%200%200%201%2011.314%2011.314A8.002%208.002%200%200%201%20.234%2010.089a8%208%200%200%201%202.11-7.746m1.06%2010.253a6.5%206.5%200%201%200%209.108-9.275%206.5%206.5%200%200%200-9.108%209.275M6.03%204.97%208%206.94l1.97-1.97a.749.749%200%200%201%201.275.326.75.75%200%200%201-.215.734L9.06%208l1.97%201.97a.749.749%200%200%201-.326%201.275.75.75%200%200%201-.734-.215L8%209.06l-1.97%201.97a.749.749%200%200%201-1.275-.326.75.75%200%200%201%20.215-.734L6.94%208%204.97%206.03a.75.75%200%200%201%20.018-1.042.75.75%200%200%201%201.042-.018%22/%3E%3C/svg%3E');--md-admonition-icon--danger:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M9.504.43a1.516%201.516%200%200%201%202.437%201.713L10.415%205.5h2.123c1.57%200%202.346%201.909%201.22%203.004l-7.34%207.142a1.25%201.25%200%200%201-.871.354h-.302a1.25%201.25%200%200%201-1.157-1.723L5.633%2010.5H3.462c-1.57%200-2.346-1.909-1.22-3.004zm1.047%201.074L3.286%208.571A.25.25%200%200%200%203.462%209H6.75a.75.75%200%200%201%20.694%201.034l-1.713%204.188%206.982-6.793A.25.25%200%200%200%2012.538%207H9.25a.75.75%200%200%201-.683-1.06l2.008-4.418.003-.006-.004-.009-.006-.006-.008-.001q-.005%200-.009.004%22/%3E%3C/svg%3E');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M4.72.22a.75.75%200%200%201%201.06%200l1%20.999a3.5%203.5%200%200%201%202.441%200l.999-1a.748.748%200%200%201%201.265.332.75.75%200%200%201-.205.729l-.775.776c.616.63.995%201.493.995%202.444v.327q0%20.15-.025.292c.408.14.764.392%201.029.722l1.968-.787a.75.75%200%200%201%20.556%201.392L13%207.258V9h2.25a.75.75%200%200%201%200%201.5H13v.5q-.002.615-.141%201.186l2.17.868a.75.75%200%200%201-.557%201.392l-2.184-.873A5%205%200%200%201%208%2016a5%205%200%200%201-4.288-2.427l-2.183.873a.75.75%200%200%201-.558-1.392l2.17-.868A5%205%200%200%201%203%2011v-.5H.75a.75.75%200%200%201%200-1.5H3V7.258L.971%206.446a.75.75%200%200%201%20.558-1.392l1.967.787c.265-.33.62-.583%201.03-.722a1.7%201.7%200%200%201-.026-.292V4.5c0-.951.38-1.814.995-2.444L4.72%201.28a.75.75%200%200%201%200-1.06m.53%206.28a.75.75%200%200%200-.75.75V11a3.5%203.5%200%201%200%207%200V7.25a.75.75%200%200%200-.75-.75ZM6.173%205h3.654A.17.17%200%200%200%2010%204.827V4.5a2%202%200%201%200-4%200v.327c0%20.096.077.173.173.173%22/%3E%3C/svg%3E');--md-admonition-icon--example:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5%205.782V2.5h-.25a.75.75%200%200%201%200-1.5h6.5a.75.75%200%200%201%200%201.5H11v3.282l3.666%205.76C15.619%2013.04%2014.543%2015%2012.767%2015H3.233c-1.776%200-2.852-1.96-1.899-3.458Zm-2.4%206.565a.75.75%200%200%200%20.633%201.153h9.534a.75.75%200%200%200%20.633-1.153L12.225%2010.5h-8.45ZM9.5%202.5h-3V6c0%20.143-.04.283-.117.403L4.73%209h6.54L9.617%206.403A.75.75%200%200%201%209.5%206Z%22/%3E%3C/svg%3E');--md-admonition-icon--quote:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M1.75%202.5h10.5a.75.75%200%200%201%200%201.5H1.75a.75.75%200%200%201%200-1.5m4%205h8.5a.75.75%200%200%201%200%201.5h-8.5a.75.75%200%200%201%200-1.5m0%205h8.5a.75.75%200%200%201%200%201.5h-8.5a.75.75%200%200%201%200-1.5M2.5%207.75v6a.75.75%200%200%201-1.5%200v-6a.75.75%200%200%201%201.5%200%22/%3E%3C/svg%3E');}</style>



    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#api" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="FastGPs" class="md-header__button md-logo" aria-label="FastGPs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            FastGPs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              API
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="FastGPs" class="md-nav__button md-logo" aria-label="FastGPs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    FastGPs
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    API
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    API
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP" class="md-nav__link">
    <span class="md-ellipsis">
      AbstractGP
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AbstractGP">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.scale" class="md-nav__link">
    <span class="md-ellipsis">
      scale
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.lengthscales" class="md-nav__link">
    <span class="md-ellipsis">
      lengthscales
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.noise" class="md-nav__link">
    <span class="md-ellipsis">
      noise
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.factor_task_kernel" class="md-nav__link">
    <span class="md-ellipsis">
      factor_task_kernel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.noise_task_kernel" class="md-nav__link">
    <span class="md-ellipsis">
      noise_task_kernel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.gram_matrix_tasks" class="md-nav__link">
    <span class="md-ellipsis">
      gram_matrix_tasks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.coeffs" class="md-nav__link">
    <span class="md-ellipsis">
      coeffs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.x" class="md-nav__link">
    <span class="md-ellipsis">
      x
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.y" class="md-nav__link">
    <span class="md-ellipsis">
      y
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.fit" class="md-nav__link">
    <span class="md-ellipsis">
      fit
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.get_x_next" class="md-nav__link">
    <span class="md-ellipsis">
      get_x_next
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.add_y_next" class="md-nav__link">
    <span class="md-ellipsis">
      add_y_next
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.post_mean" class="md-nav__link">
    <span class="md-ellipsis">
      post_mean
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.post_var" class="md-nav__link">
    <span class="md-ellipsis">
      post_var
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.post_cov" class="md-nav__link">
    <span class="md-ellipsis">
      post_cov
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.post_error" class="md-nav__link">
    <span class="md-ellipsis">
      post_error
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.post_ci" class="md-nav__link">
    <span class="md-ellipsis">
      post_ci
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.post_cubature_mean" class="md-nav__link">
    <span class="md-ellipsis">
      post_cubature_mean
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.post_cubature_var" class="md-nav__link">
    <span class="md-ellipsis">
      post_cubature_var
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.post_cubature_cov" class="md-nav__link">
    <span class="md-ellipsis">
      post_cubature_cov
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.post_cubature_error" class="md-nav__link">
    <span class="md-ellipsis">
      post_cubature_error
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_gp.AbstractGP.post_cubature_ci" class="md-nav__link">
    <span class="md-ellipsis">
      post_cubature_ci
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fastgps.standard_gp.StandardGP" class="md-nav__link">
    <span class="md-ellipsis">
      StandardGP
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fastgps.abstract_fast_gp.AbstractFastGP" class="md-nav__link">
    <span class="md-ellipsis">
      AbstractFastGP
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AbstractFastGP">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_fast_gp.AbstractFastGP.ft" class="md-nav__link">
    <span class="md-ellipsis">
      ft
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fastgps.abstract_fast_gp.AbstractFastGP.ift" class="md-nav__link">
    <span class="md-ellipsis">
      ift
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fastgps.fast_gp_lattice.FastGPLattice" class="md-nav__link">
    <span class="md-ellipsis">
      FastGPLattice
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fastgps.fast_gp_digital_net_b2.FastGPDigitalNetB2" class="md-nav__link">
    <span class="md-ellipsis">
      FastGPDigitalNetB2
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Standard Examples
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Standard Examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/simple/fgp_dnb2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fast GP Net
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/simple/fgp_lattice/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fast GP Lattice
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/simple/standard_gp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Standard GP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/simple/compare_gps_plot/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Compare GPs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Multitask Examples
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Multitask Examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/multitask/fgp_dnb2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fast GP Net
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/multitask/fgp_lattice/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fast GP Lattice
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/multitask/standard_gp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Standard GP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/multitask/compare_gps_plot/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Compare GPs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Batch Multitask Examples
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Batch Multitask Examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/batch_multitask/fgp_dnb2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fast GP Net
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/batch_multitask/fgp_lattice/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fast GP Lattice
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/batch_multitask/standard_gp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Standard GP
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Derivative Informed
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Derivative Informed
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/derivative_informed/fgp_dnb2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fast GP Net
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/derivative_informed/fgp_lattice/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fast GP Lattice
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/derivative_informed/standard_gp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Standard GP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/derivative_informed/compare_gps_plot/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Compare GPs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Publications
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Publications
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/probnum25_paper/probnum25_paper/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ProbNum25
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="api">API</h1>


<div class="doc doc-object doc-module">




    <div class="doc doc-contents first">









  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="fastgps.abstract_gp.AbstractGP" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">AbstractGP</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">AbstractGP</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span> <span class="n">num_tasks</span><span class="p">,</span> <span class="n">default_task</span><span class="p">,</span> <span class="n">solo_task</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">lengthscales</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">factor_task_kernel</span><span class="p">,</span> <span class="n">rank_factor_task_kernel</span><span class="p">,</span> <span class="n">noise_task_kernel</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">tfs_scale</span><span class="p">,</span> <span class="n">tfs_lengthscales</span><span class="p">,</span> <span class="n">tfs_noise</span><span class="p">,</span> <span class="n">tfs_factor_task_kernel</span><span class="p">,</span> <span class="n">tfs_noise_task_kernel</span><span class="p">,</span> <span class="n">requires_grad_scale</span><span class="p">,</span> <span class="n">requires_grad_lengthscales</span><span class="p">,</span> <span class="n">requires_grad_noise</span><span class="p">,</span> <span class="n">requires_grad_factor_task_kernel</span><span class="p">,</span> <span class="n">requires_grad_noise_task_kernel</span><span class="p">,</span> <span class="n">shape_batch</span><span class="p">,</span> <span class="n">shape_scale</span><span class="p">,</span> <span class="n">shape_lengthscales</span><span class="p">,</span> <span class="n">shape_noise</span><span class="p">,</span> <span class="n">shape_factor_task_kernel</span><span class="p">,</span> <span class="n">shape_noise_task_kernel</span><span class="p">,</span> <span class="n">derivatives</span><span class="p">,</span> <span class="n">derivatives_coeffs</span><span class="p">,</span> <span class="n">adaptive_nugget</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>








                  <details class="quote">
                    <summary>Source code in <code>fastgps/abstract_gp.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">seqs</span><span class="p">,</span>
        <span class="n">num_tasks</span><span class="p">,</span>
        <span class="n">default_task</span><span class="p">,</span>
        <span class="n">solo_task</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">lengthscales</span><span class="p">,</span>
        <span class="n">noise</span><span class="p">,</span>
        <span class="n">factor_task_kernel</span><span class="p">,</span>
        <span class="n">rank_factor_task_kernel</span><span class="p">,</span>
        <span class="n">noise_task_kernel</span><span class="p">,</span>
        <span class="n">device</span><span class="p">,</span>
        <span class="n">tfs_scale</span><span class="p">,</span>
        <span class="n">tfs_lengthscales</span><span class="p">,</span>
        <span class="n">tfs_noise</span><span class="p">,</span>
        <span class="n">tfs_factor_task_kernel</span><span class="p">,</span>
        <span class="n">tfs_noise_task_kernel</span><span class="p">,</span>
        <span class="n">requires_grad_scale</span><span class="p">,</span> 
        <span class="n">requires_grad_lengthscales</span><span class="p">,</span> 
        <span class="n">requires_grad_noise</span><span class="p">,</span>
        <span class="n">requires_grad_factor_task_kernel</span><span class="p">,</span>
        <span class="n">requires_grad_noise_task_kernel</span><span class="p">,</span>
        <span class="n">shape_batch</span><span class="p">,</span>
        <span class="n">shape_scale</span><span class="p">,</span> 
        <span class="n">shape_lengthscales</span><span class="p">,</span>
        <span class="n">shape_noise</span><span class="p">,</span>
        <span class="n">shape_factor_task_kernel</span><span class="p">,</span> 
        <span class="n">shape_noise_task_kernel</span><span class="p">,</span>
        <span class="n">derivatives</span><span class="p">,</span>
        <span class="n">derivatives_coeffs</span><span class="p">,</span>
        <span class="n">adaptive_nugget</span><span class="p">,</span>
    <span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">get_default_dtype</span><span class="p">()</span><span class="o">==</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="s2">&quot;fast transforms do not work without torch.float64 precision&quot;</span> 
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">num_tasks</span><span class="o">&gt;</span><span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span> <span class="o">=</span> <span class="n">num_tasks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">default_task</span> <span class="o">=</span> <span class="n">default_task</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">solo_task</span> <span class="o">=</span> <span class="n">solo_task</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> <span class="n">seqs</span><span class="o">.</span><span class="n">shape</span><span class="o">==</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">,)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">seqs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">d</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">seqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">d</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">seqs</span> <span class="o">=</span> <span class="n">seqs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># derivatives</span>
    <span class="k">if</span> <span class="n">derivatives</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">derivatives_coeffs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">rank_factor_task_kernel</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">tfs_noise_task_kernel</span> <span class="o">=</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">noise_task_kernel</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">if</span> <span class="n">derivatives</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">derivatives</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">)]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">derivatives</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span> <span class="n">derivatives</span> <span class="o">=</span> <span class="p">[</span><span class="n">derivatives</span><span class="p">]</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">derivatives</span><span class="p">,</span><span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">derivatives</span><span class="p">)</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span>
    <span class="n">derivatives</span> <span class="o">=</span> <span class="p">[</span><span class="n">deriv</span><span class="p">[</span><span class="kc">None</span><span class="p">,:]</span> <span class="k">if</span> <span class="n">deriv</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">1</span> <span class="k">else</span> <span class="n">deriv</span> <span class="k">for</span> <span class="n">deriv</span> <span class="ow">in</span> <span class="n">derivatives</span><span class="p">]</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">((</span><span class="n">derivatives</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">2</span> <span class="ow">and</span> <span class="n">derivatives</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span> <span class="o">=</span> <span class="n">derivatives</span>
    <span class="k">if</span> <span class="n">derivatives_coeffs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">derivatives_coeffs</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">)]</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">derivatives_coeffs</span><span class="p">,</span><span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">derivatives_coeffs</span><span class="p">)</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">((</span><span class="n">derivatives_coeffs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">1</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">derivatives_coeffs</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span><span class="o">==</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">derivatives_coeffs</span> <span class="o">=</span> <span class="n">derivatives_coeffs</span>
    <span class="c1"># shape_batch </span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape_batch</span><span class="p">,(</span><span class="nb">list</span><span class="p">,</span><span class="nb">tuple</span><span class="p">)):</span> <span class="n">shape_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">shape_batch</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape_batch</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">shape_batch</span> <span class="o">=</span> <span class="n">shape_batch</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ndim_batch</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape_batch</span><span class="p">)</span>
    <span class="c1"># scale</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">),</span> <span class="s2">&quot;scale must be a scalar or torch.Tensor&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span> <span class="n">shape_scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape_scale</span><span class="p">,(</span><span class="nb">list</span><span class="p">,</span><span class="nb">tuple</span><span class="p">)):</span> <span class="n">shape_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">shape_scale</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape_scale</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">)</span> <span class="ow">and</span> <span class="n">shape_scale</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape_scale</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">:</span> <span class="k">assert</span> <span class="n">shape_scale</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="n">shape_batch</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape_scale</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):]</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">scale</span><span class="p">):</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape_scale</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">scale</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;scale must be positive&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tfs_scale</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span> <span class="ow">and</span> <span class="nb">callable</span><span class="p">(</span><span class="n">tfs_scale</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">and</span> <span class="nb">callable</span><span class="p">(</span><span class="n">tfs_scale</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="s2">&quot;tfs_scale should be a tuple of two callables, the transform and inverse transform&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tf_scale</span> <span class="o">=</span> <span class="n">tfs_scale</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">raw_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">tfs_scale</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">scale</span><span class="p">),</span><span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad_scale</span><span class="p">)</span>
    <span class="c1"># lengthscales</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">lengthscales</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lengthscales</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">),</span> <span class="s2">&quot;lengthscales must be a scalar or torch.Tensor&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lengthscales</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span> <span class="n">shape_lengthscales</span> <span class="o">=</span> <span class="n">lengthscales</span><span class="o">.</span><span class="n">shape</span> 
    <span class="k">if</span> <span class="n">shape_lengthscales</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">shape_lengthscales</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">])</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape_lengthscales</span><span class="p">,(</span><span class="nb">list</span><span class="p">,</span><span class="nb">tuple</span><span class="p">)):</span> <span class="n">shape_lengthscales</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">shape_lengthscales</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape_lengthscales</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">shape_lengthscales</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="ow">or</span> <span class="n">shape_lengthscales</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape_lengthscales</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">:</span> <span class="k">assert</span> <span class="n">shape_lengthscales</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="n">shape_batch</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape_lengthscales</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):]</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">lengthscales</span><span class="p">):</span> <span class="n">lengthscales</span> <span class="o">=</span> <span class="n">lengthscales</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape_lengthscales</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">lengthscales</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;lengthscales must be positive&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tfs_lengthscales</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span> <span class="ow">and</span> <span class="nb">callable</span><span class="p">(</span><span class="n">tfs_lengthscales</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">and</span> <span class="nb">callable</span><span class="p">(</span><span class="n">tfs_lengthscales</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="s2">&quot;tfs_lengthscales should be a tuple of two callables, the transform and inverse transform&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tf_lengthscales</span> <span class="o">=</span> <span class="n">tfs_lengthscales</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">raw_lengthscales</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">tfs_lengthscales</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">lengthscales</span><span class="p">),</span><span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad_lengthscales</span><span class="p">)</span>
    <span class="c1"># noise</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">),</span> <span class="s2">&quot;noise must be a scalar or torch.Tensor&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span> <span class="n">shape_noise</span> <span class="o">=</span> <span class="n">noise</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape_noise</span><span class="p">,(</span><span class="nb">list</span><span class="p">,</span><span class="nb">tuple</span><span class="p">)):</span> <span class="n">shape_noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">shape_noise</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape_noise</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">)</span> <span class="ow">and</span> <span class="n">shape_noise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape_noise</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">:</span> <span class="k">assert</span> <span class="n">shape_noise</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="n">shape_batch</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape_noise</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):]</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">noise</span><span class="p">):</span> <span class="n">noise</span> <span class="o">=</span> <span class="n">noise</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape_noise</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">noise</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;noise must be positive&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tfs_noise</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span> <span class="ow">and</span> <span class="nb">callable</span><span class="p">(</span><span class="n">tfs_noise</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">and</span> <span class="nb">callable</span><span class="p">(</span><span class="n">tfs_noise</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="s2">&quot;tfs_scale should be a tuple of two callables, the transform and inverse transform&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tf_noise</span> <span class="o">=</span> <span class="n">tfs_noise</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">raw_noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">tfs_noise</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">noise</span><span class="p">),</span><span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad_noise</span><span class="p">)</span>
    <span class="c1"># factor_task_kernel</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">factor_task_kernel</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">factor_task_kernel</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">),</span> <span class="s2">&quot;factor_task_kernel must be a scalar or torch.Tensor&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">factor_task_kernel</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span> <span class="n">shape_factor_task_kernel</span> <span class="o">=</span> <span class="n">factor_task_kernel</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">shape_factor_task_kernel</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">rank_factor_task_kernel</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">rank_factor_task_kernel</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="o">==</span><span class="mi">1</span> <span class="k">else</span> <span class="mi">1</span> 
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">rank_factor_task_kernel</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="mi">0</span><span class="o">&lt;=</span><span class="n">rank_factor_task_kernel</span><span class="o">&lt;=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span>
        <span class="n">shape_factor_task_kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">,</span><span class="n">rank_factor_task_kernel</span><span class="p">])</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape_factor_task_kernel</span><span class="p">,(</span><span class="nb">list</span><span class="p">,</span><span class="nb">tuple</span><span class="p">)):</span> <span class="n">shape_factor_task_kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">shape_factor_task_kernel</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape_factor_task_kernel</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">)</span> <span class="ow">and</span> <span class="mi">0</span><span class="o">&lt;=</span><span class="n">shape_factor_task_kernel</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">&lt;=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span> <span class="ow">and</span> <span class="n">shape_factor_task_kernel</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape_factor_task_kernel</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">2</span><span class="p">:</span> <span class="k">assert</span> <span class="n">shape_factor_task_kernel</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="o">==</span><span class="n">shape_batch</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape_factor_task_kernel</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span><span class="p">):]</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">factor_task_kernel</span><span class="p">):</span> <span class="n">factor_task_kernel</span> <span class="o">=</span> <span class="n">factor_task_kernel</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape_factor_task_kernel</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tfs_factor_task_kernel</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span> <span class="ow">and</span> <span class="nb">callable</span><span class="p">(</span><span class="n">tfs_factor_task_kernel</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">and</span> <span class="nb">callable</span><span class="p">(</span><span class="n">tfs_factor_task_kernel</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="s2">&quot;tfs_factor_task_kernel should be a tuple of two callables, the transform and inverse transform&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tf_factor_task_kernel</span> <span class="o">=</span> <span class="n">tfs_factor_task_kernel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">requires_grad_factor_task_kernel</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">requires_grad_factor_task_kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="o">&gt;</span><span class="mi">1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">raw_factor_task_kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">tfs_factor_task_kernel</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">factor_task_kernel</span><span class="p">),</span><span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad_factor_task_kernel</span><span class="p">)</span>
    <span class="c1"># noise_task_kernel</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">noise_task_kernel</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">noise_task_kernel</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">),</span> <span class="s2">&quot;noise_task_kernel must be a scalar or torch.Tensor&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">noise_task_kernel</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span> <span class="n">shape_noise_task_kernel</span> <span class="o">=</span> <span class="n">noise_task_kernel</span><span class="o">.</span><span class="n">shape</span> 
    <span class="k">if</span> <span class="n">shape_noise_task_kernel</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">shape_noise_task_kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">])</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape_noise_task_kernel</span><span class="p">,(</span><span class="nb">list</span><span class="p">,</span><span class="nb">tuple</span><span class="p">)):</span> <span class="n">shape_noise_task_kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">shape_noise_task_kernel</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape_noise_task_kernel</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">shape_noise_task_kernel</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span> <span class="ow">or</span> <span class="n">shape_noise_task_kernel</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape_noise_task_kernel</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">:</span> <span class="k">assert</span> <span class="n">shape_noise_task_kernel</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="n">shape_batch</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape_noise_task_kernel</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):]</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">noise_task_kernel</span><span class="p">):</span> <span class="n">noise_task_kernel</span> <span class="o">=</span> <span class="n">noise_task_kernel</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape_noise_task_kernel</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">noise_task_kernel</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;noise_task_kernel must be positive&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tfs_noise_task_kernel</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span> <span class="ow">and</span> <span class="nb">callable</span><span class="p">(</span><span class="n">tfs_noise_task_kernel</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">and</span> <span class="nb">callable</span><span class="p">(</span><span class="n">tfs_noise_task_kernel</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="s2">&quot;tfs_noise_task_kernel should be a tuple of two callables, the transform and inverse transform&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tf_noise_task_kernel</span> <span class="o">=</span> <span class="n">tfs_noise_task_kernel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">requires_grad_noise_task_kernel</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">requires_grad_noise_task_kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="o">&gt;</span><span class="mi">1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">raw_noise_task_kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">tfs_noise_task_kernel</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">noise_task_kernel</span><span class="p">),</span><span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad_noise_task_kernel</span><span class="p">)</span>
    <span class="c1"># storage and dynamic caches</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_y</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">)]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">xxb_seqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">_XXbSeq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">seqs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">)],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">coeffs_cache</span> <span class="o">=</span> <span class="n">_CoeffsCache</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">task_cov_cache</span> <span class="o">=</span> <span class="n">_TaskCovCache</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inv_log_det_cache_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># derivative multitask setting checks </span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives_coeffs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">!=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">)):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">raw_noise_task_kernel</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">raw_factor_task_kernel</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">assert</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gram_matrix_tasks</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">adaptive_nugget</span> <span class="o">=</span> <span class="n">adaptive_nugget</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="fastgps.abstract_gp.AbstractGP.scale" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">scale</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">scale</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Kernel scale parameter.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="fastgps.abstract_gp.AbstractGP.lengthscales" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">lengthscales</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">lengthscales</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Kernel lengthscale parameter.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="fastgps.abstract_gp.AbstractGP.noise" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">noise</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">noise</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Noise parameter.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="fastgps.abstract_gp.AbstractGP.factor_task_kernel" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">factor_task_kernel</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">factor_task_kernel</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Factor for the task kernel parameter.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="fastgps.abstract_gp.AbstractGP.noise_task_kernel" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">noise_task_kernel</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">noise_task_kernel</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Noise for the task kernel parameter.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="fastgps.abstract_gp.AbstractGP.gram_matrix_tasks" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">gram_matrix_tasks</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">gram_matrix_tasks</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Gram matrix for the task kernel.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="fastgps.abstract_gp.AbstractGP.coeffs" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">coeffs</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">coeffs</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Coefficients <span class="arithmatex">\(\mathsf{K}^{-1} \boldsymbol{y}\)</span>.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="fastgps.abstract_gp.AbstractGP.x" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">x</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">x</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Current sampling locations. 
A <code>torch.Tensor</code> for single task problems.
A <code>list</code> for multitask problems.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="fastgps.abstract_gp.AbstractGP.y" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">y</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">y</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Current sampling values. 
A <code>torch.Tensor</code> for single task problems.
A <code>list</code> for multitask problems.</p>

    </div>

</div>



<div class="doc doc-object doc-function">


<h4 id="fastgps.abstract_gp.AbstractGP.fit" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">fit</span>


</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">fit</span><span class="p">(</span><span class="n">loss_metric</span><span class="o">=</span><span class="s1">&#39;MLL&#39;</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stop_crit_improvement_threshold</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">stop_crit_wait_iterations</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">store_hists</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">store_loss_hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">store_scale_hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">store_lengthscales_hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">store_noise_hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">store_task_kernel_hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">verbose_indent</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cv_weights</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>loss_metric</code>
            </td>
            <td>
                  <code><span title="str">str</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>either "MLL" (Marginal Log Likelihood) or "CV" (Cross Validation) or "GCV" (Generalized CV)</p>
              </div>
            </td>
            <td>
                  <code>&#39;MLL&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>iterations</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of optimization iterations</p>
              </div>
            </td>
            <td>
                  <code>5000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lr</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>learning rate for default optimizer</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>optimizer</code>
            </td>
            <td>
                  <code><span title="torch.optim.Optimizer">Optimizer</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>optimizer defaulted to <code>torch.optim.Rprop(self.parameters(),lr=lr)</code></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stop_crit_improvement_threshold</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>stop fitting when the maximum number of iterations is reached or the best loss is note reduced by <code>stop_crit_improvement_threshold</code> for <code>stop_crit_wait_iterations</code> iterations </p>
              </div>
            </td>
            <td>
                  <code>0.05</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stop_crit_wait_iterations</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of iterations to wait for improved loss before early stopping, see the argument description for <code>stop_crit_improvement_threshold</code></p>
              </div>
            </td>
            <td>
                  <code>10</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>store_hists</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True then store all hists, otherwise specify individually with the following arguments </p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>store_loss_hist</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if <code>True</code>, store and return iteration data for loss</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>store_scale_hist</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if <code>True</code>, store and return iteration data for the kernel scale parameter</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>store_lengthscales_hist</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if <code>True</code>, store and return iteration data for the kernel lengthscale parameters</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>store_noise_hist</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if <code>True</code>, store and return iteration data for noise</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>store_task_kernel_hist</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if <code>True</code>, store and return iteration data for the task kernel</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>verbose</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>log every <code>verbose</code> iterations, set to <code>0</code> for silent mode</p>
              </div>
            </td>
            <td>
                  <code>5</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>verbose_indent</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>size of the indent to be applied when logging, helpful for logging multiple models</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>masks</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>only optimize outputs corresponding to <code>y[...,*masks]</code></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cv_weights</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="str">str</span>, <span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>weights for cross validation</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>data</code></td>            <td>
                  <code><span title="dict">dict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>iteration data which, dependeing on storage arguments, may include keys in 
<div class="highlight"><pre><span></span><code><span class="p">[</span><span class="s2">&quot;loss_hist&quot;</span><span class="p">,</span><span class="s2">&quot;scale_hist&quot;</span><span class="p">,</span><span class="s2">&quot;lengthscales_hist&quot;</span><span class="p">,</span><span class="s2">&quot;noise_hist&quot;</span><span class="p">,</span><span class="s2">&quot;task_kernel_hist&quot;</span><span class="p">]</span>
</code></pre></div></p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>fastgps/abstract_gp.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
    <span class="n">loss_metric</span><span class="p">:</span><span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;MLL&quot;</span><span class="p">,</span>
    <span class="n">iterations</span><span class="p">:</span><span class="nb">int</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">,</span>
    <span class="n">lr</span><span class="p">:</span><span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_crit_improvement_threshold</span><span class="p">:</span><span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-2</span><span class="p">,</span>
    <span class="n">stop_crit_wait_iterations</span><span class="p">:</span><span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">store_hists</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">store_loss_hist</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> 
    <span class="n">store_scale_hist</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> 
    <span class="n">store_lengthscales_hist</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">store_noise_hist</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">store_task_kernel_hist</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span><span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">verbose_indent</span><span class="p">:</span><span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">masks</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cv_weights</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        loss_metric (str): either &quot;MLL&quot; (Marginal Log Likelihood) or &quot;CV&quot; (Cross Validation) or &quot;GCV&quot; (Generalized CV)</span>
<span class="sd">        iterations (int): number of optimization iterations</span>
<span class="sd">        lr (float): learning rate for default optimizer</span>
<span class="sd">        optimizer (torch.optim.Optimizer): optimizer defaulted to `torch.optim.Rprop(self.parameters(),lr=lr)`</span>
<span class="sd">        stop_crit_improvement_threshold (float): stop fitting when the maximum number of iterations is reached or the best loss is note reduced by `stop_crit_improvement_threshold` for `stop_crit_wait_iterations` iterations </span>
<span class="sd">        stop_crit_wait_iterations (int): number of iterations to wait for improved loss before early stopping, see the argument description for `stop_crit_improvement_threshold`</span>
<span class="sd">        store_hists (bool): if True then store all hists, otherwise specify individually with the following arguments </span>
<span class="sd">        store_loss_hist (bool): if `True`, store and return iteration data for loss</span>
<span class="sd">        store_scale_hist (bool): if `True`, store and return iteration data for the kernel scale parameter</span>
<span class="sd">        store_lengthscales_hist (bool): if `True`, store and return iteration data for the kernel lengthscale parameters</span>
<span class="sd">        store_noise_hist (bool): if `True`, store and return iteration data for noise</span>
<span class="sd">        store_task_kernel_hist (bool): if `True`, store and return iteration data for the task kernel</span>
<span class="sd">        verbose (int): log every `verbose` iterations, set to `0` for silent mode</span>
<span class="sd">        verbose_indent (int): size of the indent to be applied when logging, helpful for logging multiple models</span>
<span class="sd">        masks (torch.Tensor): only optimize outputs corresponding to `y[...,*masks]`</span>
<span class="sd">        cv_weights (Union[str,torch.Tensor]): weights for cross validation</span>

<span class="sd">    Returns:</span>
<span class="sd">        data (dict): iteration data which, dependeing on storage arguments, may include keys in </span>
<span class="sd">            ```python</span>
<span class="sd">            [&quot;loss_hist&quot;,&quot;scale_hist&quot;,&quot;lengthscales_hist&quot;,&quot;noise_hist&quot;,&quot;task_kernel_hist&quot;]</span>
<span class="sd">            ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">loss_metric</span><span class="p">,</span><span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">loss_metric</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;MLL&quot;</span><span class="p">,</span><span class="s2">&quot;GCV&quot;</span><span class="p">,</span><span class="s2">&quot;CV&quot;</span><span class="p">]</span> 
    <span class="k">assert</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;cannot fit without data&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iterations</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">iterations</span><span class="o">&gt;=</span><span class="mi">0</span>
    <span class="k">if</span> <span class="n">optimizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_default_optimizer</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">store_hists</span><span class="p">,</span><span class="nb">bool</span><span class="p">),</span> <span class="s2">&quot;require bool store_mll_hist&quot;</span> 
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">store_loss_hist</span><span class="p">,</span><span class="nb">bool</span><span class="p">),</span> <span class="s2">&quot;require bool store_loss_hist&quot;</span> 
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">store_scale_hist</span><span class="p">,</span><span class="nb">bool</span><span class="p">),</span> <span class="s2">&quot;require bool store_scale_hist&quot;</span> 
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">store_lengthscales_hist</span><span class="p">,</span><span class="nb">bool</span><span class="p">),</span> <span class="s2">&quot;require bool store_lengthscales_hist&quot;</span> 
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">store_noise_hist</span><span class="p">,</span><span class="nb">bool</span><span class="p">),</span> <span class="s2">&quot;require bool store_noise_hist&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">store_task_kernel_hist</span><span class="p">,</span><span class="nb">bool</span><span class="p">),</span> <span class="s2">&quot;require bool store_task_kernel_hist&quot;</span>
    <span class="k">assert</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">verbose</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">verbose</span><span class="p">,</span><span class="nb">bool</span><span class="p">))</span> <span class="ow">and</span> <span class="n">verbose</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;require verbose is a non-negative int&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">verbose_indent</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">verbose_indent</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;require verbose_indent is a non-negative int&quot;</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">stop_crit_improvement_threshold</span><span class="p">)</span> <span class="ow">and</span> <span class="mi">0</span><span class="o">&lt;</span><span class="n">stop_crit_improvement_threshold</span><span class="p">,</span> <span class="s2">&quot;require stop_crit_improvement_threshold is a positive float&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stop_crit_wait_iterations</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">stop_crit_wait_iterations</span><span class="o">&gt;</span><span class="mi">0</span>
    <span class="k">assert</span> <span class="n">masks</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">))</span>
    <span class="n">loss_metric</span> <span class="o">=</span> <span class="n">loss_metric</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span>
    <span class="n">logtol</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">stop_crit_improvement_threshold</span><span class="p">)</span>
    <span class="n">store_loss_hist</span> <span class="o">=</span> <span class="n">store_hists</span> <span class="ow">or</span> <span class="n">store_loss_hist</span>
    <span class="n">store_scale_hist</span> <span class="o">=</span> <span class="n">store_hists</span> <span class="ow">or</span> <span class="p">(</span><span class="n">store_scale_hist</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">raw_scale</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
    <span class="n">store_lengthscales_hist</span> <span class="o">=</span> <span class="n">store_hists</span> <span class="ow">or</span> <span class="p">(</span><span class="n">store_lengthscales_hist</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">raw_lengthscales</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
    <span class="n">store_noise_hist</span> <span class="o">=</span> <span class="n">store_hists</span> <span class="ow">or</span> <span class="p">(</span><span class="n">store_noise_hist</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">raw_noise</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
    <span class="n">store_task_kernel_hist</span> <span class="o">=</span> <span class="n">store_hists</span> <span class="ow">or</span> <span class="p">(</span><span class="n">store_task_kernel_hist</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">raw_factor_task_kernel</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">raw_noise_task_kernel</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">store_loss_hist</span><span class="p">:</span> <span class="n">loss_hist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">iterations</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">store_scale_hist</span><span class="p">:</span> <span class="n">scale_hist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">iterations</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">raw_scale</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">store_lengthscales_hist</span><span class="p">:</span> <span class="n">lengthscales_hist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">iterations</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">raw_lengthscales</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">store_noise_hist</span><span class="p">:</span> <span class="n">noise_hist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">iterations</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">raw_noise</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">store_task_kernel_hist</span><span class="p">:</span> <span class="n">task_kernel_hist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">iterations</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">gram_matrix_tasks</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">masks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">masks</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">masks</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">2</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">masks</span><span class="p">)</span><span class="o">&lt;=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape_batch</span><span class="p">)</span>
        <span class="n">d_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape_batch</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span><span class="o">*</span><span class="n">masks</span><span class="p">]</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">d_out</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape_batch</span><span class="p">)</span><span class="o">.</span><span class="n">prod</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="n">_s</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%16s</span><span class="s2"> | </span><span class="si">%-10s</span><span class="s2"> | </span><span class="si">%-10s</span><span class="s2"> | </span><span class="si">%-10s</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span><span class="s2">&quot;iter of </span><span class="si">%.1e</span><span class="s2">&quot;</span><span class="o">%</span><span class="n">iterations</span><span class="p">,</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span><span class="s2">&quot;term1&quot;</span><span class="p">,</span><span class="s2">&quot;term2&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="o">*</span><span class="n">verbose_indent</span><span class="o">+</span><span class="n">_s</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="o">*</span><span class="n">verbose_indent</span><span class="o">+</span><span class="s2">&quot;~&quot;</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">_s</span><span class="p">))</span>
    <span class="n">mll_const</span> <span class="o">=</span> <span class="n">d_out</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
    <span class="n">stop_crit_best_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">inf</span> 
    <span class="n">stop_crit_save_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">inf</span> 
    <span class="n">stop_crit_iterations_without_improvement_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;FASTGP_FORCE_RECOMPILE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;True&quot;</span>
    <span class="n">inv_log_det_cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_inv_log_det_cache</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">loss_metric</span><span class="o">==</span><span class="s2">&quot;GCV&quot;</span><span class="p">:</span>
            <span class="n">numer</span><span class="p">,</span><span class="n">denom</span> <span class="o">=</span> <span class="n">inv_log_det_cache</span><span class="o">.</span><span class="n">get_gcv_numer_denom</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">masks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">term1</span> <span class="o">=</span> <span class="n">numer</span> 
                <span class="n">term2</span> <span class="o">=</span> <span class="n">denom</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">term1</span> <span class="o">=</span> <span class="n">numer</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="o">*</span><span class="n">masks</span><span class="p">,:]</span>
                <span class="n">term2</span> <span class="o">=</span> <span class="n">denom</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape_batch</span><span class="p">)</span><span class="o">+</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="o">...</span><span class="p">,</span><span class="o">*</span><span class="n">masks</span><span class="p">,:]</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">term1</span><span class="o">/</span><span class="n">term2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
            <span class="n">metric_val</span> <span class="o">=</span> <span class="n">loss</span>
        <span class="k">elif</span> <span class="n">loss_metric</span><span class="o">==</span><span class="s2">&quot;MLL&quot;</span><span class="p">:</span>
            <span class="n">norm_term</span><span class="p">,</span><span class="n">logdet</span> <span class="o">=</span> <span class="n">inv_log_det_cache</span><span class="o">.</span><span class="n">get_norm_term_logdet_term</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">masks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">term1</span> <span class="o">=</span> <span class="n">norm_term</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
                <span class="n">term2</span> <span class="o">=</span> <span class="n">d_out</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">logdet</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">prod</span><span class="p">()</span><span class="o">*</span><span class="n">logdet</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">term1</span> <span class="o">=</span> <span class="n">norm_term</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="o">*</span><span class="n">masks</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
                <span class="n">term2</span> <span class="o">=</span> <span class="n">logdet</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape_batch</span><span class="p">)</span><span class="o">+</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="o">...</span><span class="p">,</span><span class="o">*</span><span class="n">masks</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">term1</span><span class="o">+</span><span class="n">term2</span><span class="o">+</span><span class="n">mll_const</span><span class="p">)</span>
            <span class="n">metric_val</span> <span class="o">=</span> <span class="o">-</span><span class="n">loss</span>
        <span class="k">elif</span> <span class="n">loss_metric</span><span class="o">==</span><span class="s2">&quot;CV&quot;</span><span class="p">:</span>
            <span class="n">coeffs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coeffs</span>
            <span class="k">del</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;FASTGP_FORCE_RECOMPILE&quot;</span><span class="p">]</span>
            <span class="n">inv_diag</span> <span class="o">=</span> <span class="n">inv_log_det_cache</span><span class="o">.</span><span class="n">get_inv_diag</span><span class="p">()</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;FASTGP_FORCE_RECOMPILE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;True&quot;</span>
            <span class="n">term1</span> <span class="o">=</span> <span class="n">term2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">squared_sums</span> <span class="o">=</span> <span class="p">((</span><span class="n">coeffs</span><span class="o">/</span><span class="n">inv_diag</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">*</span><span class="n">cv_weights</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">masks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">squared_sums</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">squared_sums</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="o">*</span><span class="n">masks</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
            <span class="n">metric_val</span> <span class="o">=</span> <span class="n">loss</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;loss_metric parsing implementation error&quot;</span>
        <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">&lt;</span><span class="n">stop_crit_best_loss</span><span class="p">:</span>
            <span class="n">stop_crit_best_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">best_params</span> <span class="o">=</span> <span class="p">{</span><span class="n">param</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span><span class="n">param</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()}</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">stop_crit_save_loss</span><span class="o">-</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span><span class="o">&gt;</span><span class="n">logtol</span><span class="p">:</span>
            <span class="n">stop_crit_iterations_without_improvement_loss</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">stop_crit_save_loss</span> <span class="o">=</span> <span class="n">stop_crit_best_loss</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">stop_crit_iterations_without_improvement_loss</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">break_condition</span> <span class="o">=</span> <span class="n">i</span><span class="o">==</span><span class="n">iterations</span> <span class="ow">or</span> <span class="n">stop_crit_iterations_without_improvement_loss</span><span class="o">==</span><span class="n">stop_crit_wait_iterations</span>
        <span class="k">if</span> <span class="n">store_loss_hist</span><span class="p">:</span> <span class="n">loss_hist</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">metric_val</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">store_scale_hist</span><span class="p">:</span> <span class="n">scale_hist</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">scale_hist</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">store_lengthscales_hist</span><span class="p">:</span> <span class="n">lengthscales_hist</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lengthscales</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">lengthscales_hist</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">store_noise_hist</span><span class="p">:</span> <span class="n">noise_hist</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">noise_hist</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">store_task_kernel_hist</span><span class="p">:</span> <span class="n">task_kernel_hist</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gram_matrix_tasks</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">task_kernel_hist</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="p">(</span><span class="n">i</span><span class="o">%</span><span class="n">verbose</span><span class="o">==</span><span class="mi">0</span> <span class="ow">or</span> <span class="n">break_condition</span><span class="p">):</span>
            <span class="n">_s</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%16.2e</span><span class="s2"> | </span><span class="si">%-10.2e</span><span class="s2"> | </span><span class="si">%-10.2e</span><span class="s2"> | </span><span class="si">%-10.2e</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span><span class="n">term1</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">if</span> <span class="n">term1</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="o">==</span><span class="mi">1</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span><span class="n">term2</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">if</span> <span class="n">term2</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="o">==</span><span class="mi">1</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="o">*</span><span class="n">verbose_indent</span><span class="o">+</span><span class="n">_s</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">break_condition</span><span class="p">:</span> <span class="k">break</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">pname</span><span class="p">,</span><span class="n">pdata</span> <span class="ow">in</span> <span class="n">best_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">pname</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">pdata</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">pname</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">))</span>
    <span class="k">del</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;FASTGP_FORCE_RECOMPILE&quot;</span><span class="p">]</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;iterations&quot;</span><span class="p">:</span><span class="n">i</span><span class="p">}</span>
    <span class="k">if</span> <span class="n">store_loss_hist</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;loss_hist&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss_hist</span><span class="p">[:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
    <span class="k">if</span> <span class="n">store_scale_hist</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;scale_hist&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">scale_hist</span><span class="p">[:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
    <span class="k">if</span> <span class="n">store_lengthscales_hist</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;lengthscales_hist&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lengthscales_hist</span><span class="p">[:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
    <span class="k">if</span> <span class="n">store_noise_hist</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;noise_hist&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">noise_hist</span><span class="p">[:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
    <span class="k">if</span> <span class="n">store_task_kernel_hist</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;task_kernel_hist&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">task_kernel_hist</span><span class="p">[:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">data</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="fastgps.abstract_gp.AbstractGP.get_x_next" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">get_x_next</span>


</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">get_x_next</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Get the next sampling locations. </p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>n</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>maximum sample index per task</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>task</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>task index</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>x_next</code></td>            <td>
                  <code><span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>, <span title="typing.List">List</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>next samples in the sequence</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>fastgps/abstract_gp.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_x_next</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">task</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the next sampling locations. </span>

<span class="sd">    Args:</span>
<span class="sd">        n (Union[int,torch.Tensor]): maximum sample index per task</span>
<span class="sd">        task (Union[int,torch.Tensor]): task index</span>

<span class="sd">    Returns:</span>
<span class="sd">        x_next (Union[torch.Tensor,List]): next samples in the sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n</span><span class="p">,(</span><span class="nb">int</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)):</span> <span class="n">n</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">n</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> 
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="nb">list</span><span class="p">):</span> <span class="n">n</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">task</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">task</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_task</span>
    <span class="n">inttask</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">task</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">inttask</span><span class="p">:</span> <span class="n">task</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">task</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">task</span><span class="p">,</span><span class="nb">list</span><span class="p">):</span> <span class="n">task</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">task</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">task</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">n</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="n">task</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">1</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">==</span><span class="nb">len</span><span class="p">(</span><span class="n">task</span><span class="p">)</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">n</span><span class="o">&gt;=</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">[</span><span class="n">task</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;maximum sequence index must be greater than the current number of samples&quot;</span>
    <span class="n">x_next</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">xxb_seqs</span><span class="p">[</span><span class="n">l</span><span class="p">][</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">[</span><span class="n">l</span><span class="p">]:</span><span class="n">n</span><span class="p">[</span><span class="n">i</span><span class="p">]][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">task</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">x_next</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">inttask</span> <span class="k">else</span> <span class="n">x_next</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="fastgps.abstract_gp.AbstractGP.add_y_next" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">add_y_next</span>


</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">add_y_next</span><span class="p">(</span><span class="n">y_next</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Add samples to the GP. </p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>y_next</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>, <span title="typing.List">List</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>new function evaluations at next sampling locations</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>task</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>task index</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>fastgps/abstract_gp.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">add_y_next</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_next</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="n">List</span><span class="p">],</span> <span class="n">task</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Add samples to the GP. </span>

<span class="sd">    Args:</span>
<span class="sd">        y_next (Union[torch.Tensor,List]): new function evaluations at next sampling locations</span>
<span class="sd">        task (Union[int,torch.Tensor]): task index</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y_next</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span> <span class="n">y_next</span> <span class="o">=</span> <span class="p">[</span><span class="n">y_next</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">task</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">task</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_task</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">task</span><span class="p">,</span><span class="nb">int</span><span class="p">):</span> <span class="n">task</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">task</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">task</span><span class="p">,</span><span class="nb">list</span><span class="p">):</span> <span class="n">task</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">task</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y_next</span><span class="p">,</span><span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">task</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">task</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">1</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_next</span><span class="p">)</span><span class="o">==</span><span class="nb">len</span><span class="p">(</span><span class="n">task</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">y_next</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">shape_batch</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_next</span><span class="p">)))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">task</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">[</span><span class="n">l</span><span class="p">],</span><span class="n">y_next</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">)],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inv_log_det_cache_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">key</span><span class="p">)</span><span class="o">&lt;</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">inv_log_det_cache_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="fastgps.abstract_gp.AbstractGP.post_mean" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">post_mean</span>


</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">post_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">eval</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Posterior mean. </p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span>[<span title="N">N</span>, <span title="fastgps.abstract_gp.AbstractGP.d">d</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>sampling locations</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>task</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>[<span title="T">T</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>task index</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eval</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if <code>True</code>, disable gradients, otherwise use <code>torch.is_grad_enabled()</code></p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>pmean</code></td>            <td>
                  <code><span title="torch.Tensor">Tensor</span>[..., <span title="T">T</span>, <span title="N">N</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>posterior mean</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>fastgps/abstract_gp.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">post_mean</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">task</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">eval</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Posterior mean. </span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor[N,d]): sampling locations</span>
<span class="sd">        task (Union[int,torch.Tensor[T]]): task index</span>
<span class="sd">        eval (bool): if `True`, disable gradients, otherwise use `torch.is_grad_enabled()`</span>

<span class="sd">    Returns:</span>
<span class="sd">        pmean (torch.Tensor[...,T,N]): posterior mean</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">coeffs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coeffs</span>
    <span class="n">kmat_tasks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gram_matrix_tasks</span>
    <span class="k">if</span> <span class="nb">eval</span><span class="p">:</span>
        <span class="n">incoming_grad_enabled</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">2</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="s2">&quot;x must a torch.Tensor with shape (-1,d)&quot;</span>
    <span class="k">if</span> <span class="n">task</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">task</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_task</span>
    <span class="n">inttask</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">task</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">inttask</span><span class="p">:</span> <span class="n">task</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">task</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">task</span><span class="p">,</span><span class="nb">list</span><span class="p">):</span> <span class="n">task</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">task</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">task</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">task</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span> <span class="ow">and</span> <span class="p">(</span><span class="n">task</span><span class="o">&lt;</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
    <span class="n">kmat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">kmat_tasks</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="n">task</span><span class="p">[</span><span class="n">l0</span><span class="p">],</span><span class="n">l1</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="kc">None</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_kernel</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="kc">None</span><span class="p">,:],</span><span class="bp">self</span><span class="o">.</span><span class="n">get_xb</span><span class="p">(</span><span class="n">l1</span><span class="p">)[</span><span class="kc">None</span><span class="p">,:,:],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span><span class="p">[</span><span class="n">task</span><span class="p">[</span><span class="n">l0</span><span class="p">]],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span><span class="p">[</span><span class="n">l1</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives_coeffs</span><span class="p">[</span><span class="n">task</span><span class="p">[</span><span class="n">l0</span><span class="p">]],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives_coeffs</span><span class="p">[</span><span class="n">l1</span><span class="p">])</span> <span class="k">for</span> <span class="n">l1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">)],</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span><span class="kc">None</span><span class="p">,:,:]</span> <span class="k">for</span> <span class="n">l0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">task</span><span class="p">))],</span><span class="n">dim</span><span class="o">=-</span><span class="mi">3</span><span class="p">)</span>
    <span class="c1">#pmean = (kmat*coeffs[...,None,None,:]).sum(-1)</span>
    <span class="n">pmean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;...i,...i-&gt;...&quot;</span><span class="p">,</span><span class="n">kmat</span><span class="p">,</span><span class="n">coeffs</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="kc">None</span><span class="p">,:])</span>
    <span class="k">if</span> <span class="nb">eval</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">incoming_grad_enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pmean</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="mi">0</span><span class="p">,:]</span> <span class="k">if</span> <span class="n">inttask</span> <span class="k">else</span> <span class="n">pmean</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="fastgps.abstract_gp.AbstractGP.post_var" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">post_var</span>


</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">post_var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">eval</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Posterior variance.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span>[<span title="N">N</span>, <span title="fastgps.abstract_gp.AbstractGP.d">d</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>sampling locations</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>task</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>[<span title="T">T</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>task indices</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>[<span title="fastgps.abstract_gp.AbstractGP.num_tasks">num_tasks</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of points at which to evaluate the posterior cubature variance.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eval</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if <code>True</code>, disable gradients, otherwise use <code>torch.is_grad_enabled()</code></p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>pvar</code></td>            <td>
                  <code><span title="torch.Tensor">Tensor</span>[<span title="T">T</span>, <span title="N">N</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>posterior variance</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>fastgps/abstract_gp.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">post_var</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">task</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">eval</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Posterior variance.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor[N,d]): sampling locations</span>
<span class="sd">        task (Union[int,torch.Tensor[T]]): task indices</span>
<span class="sd">        n (Union[int,torch.Tensor[num_tasks]]): number of points at which to evaluate the posterior cubature variance.</span>
<span class="sd">        eval (bool): if `True`, disable gradients, otherwise use `torch.is_grad_enabled()`</span>

<span class="sd">    Returns:</span>
<span class="sd">        pvar (torch.Tensor[T,N]): posterior variance</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">n</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="nb">int</span><span class="p">):</span> <span class="n">n</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">n</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">2</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="s2">&quot;x must a torch.Tensor with shape (-1,d)&quot;</span>
    <span class="n">kmat_tasks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gram_matrix_tasks</span>
    <span class="k">if</span> <span class="nb">eval</span><span class="p">:</span>
        <span class="n">incoming_grad_enabled</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">task</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">task</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_task</span>
    <span class="n">inttask</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">task</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">inttask</span><span class="p">:</span> <span class="n">task</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">task</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">task</span><span class="p">,</span><span class="nb">list</span><span class="p">):</span> <span class="n">task</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">task</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">task</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">task</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span> <span class="ow">and</span> <span class="p">(</span><span class="n">task</span><span class="o">&lt;</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
    <span class="n">kmat_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">kmat_tasks</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="n">task</span><span class="p">[</span><span class="n">l0</span><span class="p">],</span><span class="n">task</span><span class="p">[</span><span class="n">l0</span><span class="p">],</span><span class="kc">None</span><span class="p">,</span><span class="kc">None</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_kernel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span><span class="p">[</span><span class="n">task</span><span class="p">[</span><span class="n">l0</span><span class="p">]],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span><span class="p">[</span><span class="n">task</span><span class="p">[</span><span class="n">l0</span><span class="p">]],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives_coeffs</span><span class="p">[</span><span class="n">task</span><span class="p">[</span><span class="n">l0</span><span class="p">]],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives_coeffs</span><span class="p">[</span><span class="n">task</span><span class="p">[</span><span class="n">l0</span><span class="p">]])[</span><span class="o">...</span><span class="p">,</span><span class="kc">None</span><span class="p">,:]</span> <span class="k">for</span> <span class="n">l0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">task</span><span class="p">))],</span><span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">kmat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">kmat_tasks</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="n">task</span><span class="p">[</span><span class="n">l0</span><span class="p">],</span><span class="n">l1</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="kc">None</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_kernel</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="kc">None</span><span class="p">,:],</span><span class="bp">self</span><span class="o">.</span><span class="n">get_xb</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">[</span><span class="n">l1</span><span class="p">])[</span><span class="kc">None</span><span class="p">,:,:],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span><span class="p">[</span><span class="n">task</span><span class="p">[</span><span class="n">l0</span><span class="p">]],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span><span class="p">[</span><span class="n">l1</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives_coeffs</span><span class="p">[</span><span class="n">task</span><span class="p">[</span><span class="n">l0</span><span class="p">]],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives_coeffs</span><span class="p">[</span><span class="n">l1</span><span class="p">])</span> <span class="k">for</span> <span class="n">l1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">)],</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span><span class="kc">None</span><span class="p">,:,:]</span> <span class="k">for</span> <span class="n">l0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">task</span><span class="p">))],</span><span class="n">dim</span><span class="o">=-</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">kmat_perm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="n">kmat</span><span class="p">,[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">kmat</span><span class="o">.</span><span class="n">ndim</span><span class="o">-</span><span class="mi">3</span><span class="p">)]</span><span class="o">+</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">t_perm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_inv_log_det_cache</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">gram_matrix_solve</span><span class="p">(</span><span class="n">kmat_perm</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="n">t_perm</span><span class="p">,[</span><span class="mi">2</span><span class="o">+</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">t_perm</span><span class="o">.</span><span class="n">ndim</span><span class="o">-</span><span class="mi">3</span><span class="p">)]</span><span class="o">+</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">diag</span> <span class="o">=</span> <span class="n">kmat_new</span><span class="o">-</span><span class="p">(</span><span class="n">t</span><span class="o">*</span><span class="n">kmat</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">diag</span><span class="p">[</span><span class="n">diag</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> 
    <span class="k">if</span> <span class="nb">eval</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">incoming_grad_enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">diag</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="mi">0</span><span class="p">,:]</span> <span class="k">if</span> <span class="n">inttask</span> <span class="k">else</span> <span class="n">diag</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="fastgps.abstract_gp.AbstractGP.post_cov" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">post_cov</span>


</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">post_cov</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">task0</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">task1</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">eval</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Posterior covariance. </p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x0</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span>[<span title="N">N</span>, <span title="fastgps.abstract_gp.AbstractGP.d">d</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>left sampling locations</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>x1</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span>[<span title="M">M</span>, <span title="fastgps.abstract_gp.AbstractGP.d">d</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>right sampling locations</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>task0</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>[<span title="T1">T1</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>left task index</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>task1</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>[<span title="T2">T2</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>right task index</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>[<span title="fastgps.abstract_gp.AbstractGP.num_tasks">num_tasks</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of points at which to evaluate the posterior cubature variance.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eval</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if <code>True</code>, disable gradients, otherwise use <code>torch.is_grad_enabled()</code></p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>pcov</code></td>            <td>
                  <code><span title="torch.Tensor">Tensor</span>[<span title="T1">T1</span>, <span title="T2">T2</span>, <span title="N">N</span>, <span title="M">M</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>posterior covariance matrix</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>fastgps/abstract_gp.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">post_cov</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x0</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">x1</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">task0</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">task1</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">eval</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Posterior covariance. </span>

<span class="sd">    Args:</span>
<span class="sd">        x0 (torch.Tensor[N,d]): left sampling locations</span>
<span class="sd">        x1 (torch.Tensor[M,d]): right sampling locations</span>
<span class="sd">        task0 (Union[int,torch.Tensor[T1]]): left task index</span>
<span class="sd">        task1 (Union[int,torch.Tensor[T2]]): right task index</span>
<span class="sd">        n (Union[int,torch.Tensor[num_tasks]]): number of points at which to evaluate the posterior cubature variance.</span>
<span class="sd">        eval (bool): if `True`, disable gradients, otherwise use `torch.is_grad_enabled()`</span>

<span class="sd">    Returns:</span>
<span class="sd">        pcov (torch.Tensor[T1,T2,N,M]): posterior covariance matrix</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">n</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="nb">int</span><span class="p">):</span> <span class="n">n</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">n</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">x0</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">2</span> <span class="ow">and</span> <span class="n">x0</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="s2">&quot;x must a torch.Tensor with shape (-1,d)&quot;</span>
    <span class="k">assert</span> <span class="n">x1</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">2</span> <span class="ow">and</span> <span class="n">x1</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="s2">&quot;z must a torch.Tensor with shape (-1,d)&quot;</span>
    <span class="n">kmat_tasks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gram_matrix_tasks</span>
    <span class="k">if</span> <span class="nb">eval</span><span class="p">:</span>
        <span class="n">incoming_grad_enabled</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">task0</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">task0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_task</span>
    <span class="n">inttask0</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">task0</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">inttask0</span><span class="p">:</span> <span class="n">task0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">task0</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">task0</span><span class="p">,</span><span class="nb">list</span><span class="p">):</span> <span class="n">task0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">task0</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">task0</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">task0</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span> <span class="ow">and</span> <span class="p">(</span><span class="n">task0</span><span class="o">&lt;</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">task1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">task1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_task</span>
    <span class="n">inttask1</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">task1</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">inttask1</span><span class="p">:</span> <span class="n">task1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">task1</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">task1</span><span class="p">,</span><span class="nb">list</span><span class="p">):</span> <span class="n">task1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">task1</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">task1</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">task1</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span> <span class="ow">and</span> <span class="p">(</span><span class="n">task1</span><span class="o">&lt;</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
    <span class="n">equal</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">x1</span><span class="p">)</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">task0</span><span class="p">,</span><span class="n">task1</span><span class="p">)</span>
    <span class="n">kmat_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">kmat_tasks</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="n">task0</span><span class="p">[</span><span class="n">l0</span><span class="p">],</span><span class="n">task1</span><span class="p">[</span><span class="n">l1</span><span class="p">],</span><span class="kc">None</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="kc">None</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_kernel</span><span class="p">(</span><span class="n">x0</span><span class="p">[:,</span><span class="kc">None</span><span class="p">,:],</span><span class="n">x1</span><span class="p">[</span><span class="kc">None</span><span class="p">,:,:],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span><span class="p">[</span><span class="n">task0</span><span class="p">[</span><span class="n">l0</span><span class="p">]],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span><span class="p">[</span><span class="n">task1</span><span class="p">[</span><span class="n">l1</span><span class="p">]],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives_coeffs</span><span class="p">[</span><span class="n">task0</span><span class="p">[</span><span class="n">l0</span><span class="p">]],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives_coeffs</span><span class="p">[</span><span class="n">task1</span><span class="p">[</span><span class="n">l1</span><span class="p">]])[</span><span class="o">...</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="kc">None</span><span class="p">,:,:]</span> <span class="k">for</span> <span class="n">l1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">task1</span><span class="p">))],</span><span class="n">dim</span><span class="o">=-</span><span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">l0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">task0</span><span class="p">))],</span><span class="n">dim</span><span class="o">=-</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">kmat1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">kmat_tasks</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="n">task0</span><span class="p">[</span><span class="n">l0</span><span class="p">],</span><span class="n">l1</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="kc">None</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_kernel</span><span class="p">(</span><span class="n">x0</span><span class="p">[:,</span><span class="kc">None</span><span class="p">,:],</span><span class="bp">self</span><span class="o">.</span><span class="n">get_xb</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">[</span><span class="n">l1</span><span class="p">])[</span><span class="kc">None</span><span class="p">,:,:],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span><span class="p">[</span><span class="n">task0</span><span class="p">[</span><span class="n">l0</span><span class="p">]],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span><span class="p">[</span><span class="n">l1</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives_coeffs</span><span class="p">[</span><span class="n">task0</span><span class="p">[</span><span class="n">l0</span><span class="p">]],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives_coeffs</span><span class="p">[</span><span class="n">l1</span><span class="p">])</span> <span class="k">for</span> <span class="n">l1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">)],</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span><span class="kc">None</span><span class="p">,:,:]</span> <span class="k">for</span> <span class="n">l0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">task0</span><span class="p">))],</span><span class="n">dim</span><span class="o">=-</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">kmat2</span> <span class="o">=</span> <span class="n">kmat1</span> <span class="k">if</span> <span class="n">equal</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">kmat_tasks</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="n">task1</span><span class="p">[</span><span class="n">l0</span><span class="p">],</span><span class="n">l1</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="kc">None</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">[:,</span><span class="kc">None</span><span class="p">,:],</span><span class="bp">self</span><span class="o">.</span><span class="n">get_xb</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">[</span><span class="n">l1</span><span class="p">])[</span><span class="kc">None</span><span class="p">,:,:],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span><span class="p">[</span><span class="n">task1</span><span class="p">[</span><span class="n">l0</span><span class="p">]],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span><span class="p">[</span><span class="n">l1</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives_coeffs</span><span class="p">[</span><span class="n">task1</span><span class="p">[</span><span class="n">l0</span><span class="p">]],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives_coeffs</span><span class="p">[</span><span class="n">l1</span><span class="p">])</span> <span class="k">for</span> <span class="n">l1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">)],</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span><span class="kc">None</span><span class="p">,:,:]</span> <span class="k">for</span> <span class="n">l0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">task1</span><span class="p">))],</span><span class="n">dim</span><span class="o">=-</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">kmat2_perm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="n">kmat2</span><span class="p">,[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">kmat2</span><span class="o">.</span><span class="n">ndim</span><span class="o">-</span><span class="mi">3</span><span class="p">)]</span><span class="o">+</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">t_perm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_inv_log_det_cache</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">gram_matrix_solve</span><span class="p">(</span><span class="n">kmat2_perm</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="n">t_perm</span><span class="p">,[</span><span class="mi">2</span><span class="o">+</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">t_perm</span><span class="o">.</span><span class="n">ndim</span><span class="o">-</span><span class="mi">3</span><span class="p">)]</span><span class="o">+</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">kmat</span> <span class="o">=</span> <span class="n">kmat_new</span><span class="o">-</span><span class="p">(</span><span class="n">kmat1</span><span class="p">[</span><span class="o">...</span><span class="p">,:,</span><span class="kc">None</span><span class="p">,:,</span><span class="kc">None</span><span class="p">,:]</span><span class="o">*</span><span class="n">t</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="kc">None</span><span class="p">,:,</span><span class="kc">None</span><span class="p">,:,:])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">equal</span><span class="p">:</span>
        <span class="n">tmesh</span><span class="p">,</span><span class="n">nmesh</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">kmat</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x0</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="n">x0</span><span class="o">.</span><span class="n">device</span><span class="p">),</span><span class="n">indexing</span><span class="o">=</span><span class="s2">&quot;ij&quot;</span><span class="p">)</span>            
        <span class="n">tidx</span><span class="p">,</span><span class="n">nidx</span> <span class="o">=</span> <span class="n">tmesh</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span><span class="n">nmesh</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">diag</span> <span class="o">=</span> <span class="n">kmat</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="n">tidx</span><span class="p">,</span><span class="n">tidx</span><span class="p">,</span><span class="n">nidx</span><span class="p">,</span><span class="n">nidx</span><span class="p">]</span>
        <span class="n">diag</span><span class="p">[</span><span class="n">diag</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> 
        <span class="n">kmat</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="n">tidx</span><span class="p">,</span><span class="n">tidx</span><span class="p">,</span><span class="n">nidx</span><span class="p">,</span><span class="n">nidx</span><span class="p">]</span> <span class="o">=</span> <span class="n">diag</span> 
    <span class="k">if</span> <span class="nb">eval</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">incoming_grad_enabled</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">inttask0</span> <span class="ow">and</span> <span class="n">inttask1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">kmat</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,:,:]</span>
    <span class="k">elif</span> <span class="n">inttask0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">inttask1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">kmat</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="mi">0</span><span class="p">,:,:,:]</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">inttask0</span> <span class="ow">and</span> <span class="n">inttask1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">kmat</span><span class="p">[</span><span class="o">...</span><span class="p">,:,</span><span class="mi">0</span><span class="p">,:,:]</span>
    <span class="k">else</span><span class="p">:</span> <span class="c1"># not inttask0 and not inttask1</span>
        <span class="k">return</span> <span class="n">kmat</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="fastgps.abstract_gp.AbstractGP.post_error" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">post_error</span>


</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">post_error</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="nb">eval</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Posterior error. </p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span>[<span title="N">N</span>, <span title="fastgps.abstract_gp.AbstractGP.d">d</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>sampling locations</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>task</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>[<span title="T">T</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>task indices</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>[<span title="fastgps.abstract_gp.AbstractGP.num_tasks">num_tasks</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of points at which to evaluate the posterior cubature variance.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eval</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if <code>True</code>, disable gradients, otherwise use <code>torch.is_grad_enabled()</code></p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>confidence</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>confidence level in <span class="arithmatex">\((0,1)\)</span> for the credible interval</p>
              </div>
            </td>
            <td>
                  <code>0.99</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>cvar</code></td>            <td>
                  <code><span title="torch.Tensor">Tensor</span>[<span title="T">T</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>posterior variance</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>quantile</code></td>            <td>
                  <code><span title="numpy.float64">float64</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <div class="highlight"><pre><span></span><code><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">confidence</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>perror</code></td>            <td>
                  <code><span title="torch.Tensor">Tensor</span>[<span title="T">T</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>posterior error</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>fastgps/abstract_gp.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">post_error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">task</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">confidence</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="nb">eval</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Posterior error. </span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor[N,d]): sampling locations</span>
<span class="sd">        task (Union[int,torch.Tensor[T]]): task indices</span>
<span class="sd">        n (Union[int,torch.Tensor[num_tasks]]): number of points at which to evaluate the posterior cubature variance.</span>
<span class="sd">        eval (bool): if `True`, disable gradients, otherwise use `torch.is_grad_enabled()`</span>
<span class="sd">        confidence (float): confidence level in $(0,1)$ for the credible interval</span>

<span class="sd">    Returns:</span>
<span class="sd">        cvar (torch.Tensor[T]): posterior variance</span>
<span class="sd">        quantile (np.float64):</span>
<span class="sd">            ```python</span>
<span class="sd">            scipy.stats.norm.ppf(1-(1-confidence)/2)</span>
<span class="sd">            ```</span>
<span class="sd">        perror (torch.Tensor[T]): posterior error</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">confidence</span><span class="p">)</span> <span class="ow">and</span> <span class="mi">0</span><span class="o">&lt;</span><span class="n">confidence</span><span class="o">&lt;</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;confidence must be between 0 and 1&quot;</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">confidence</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">pvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span><span class="nb">eval</span><span class="o">=</span><span class="nb">eval</span><span class="p">,)</span>
    <span class="n">pstd</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">pvar</span><span class="p">)</span>
    <span class="n">perror</span> <span class="o">=</span> <span class="n">q</span><span class="o">*</span><span class="n">pstd</span>
    <span class="k">return</span> <span class="n">pvar</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="n">perror</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="fastgps.abstract_gp.AbstractGP.post_ci" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">post_ci</span>


</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">post_ci</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="nb">eval</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Posterior credible interval.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span>[<span title="N">N</span>, <span title="fastgps.abstract_gp.AbstractGP.d">d</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>sampling locations</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>task</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>[<span title="T">T</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>task indices</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>confidence</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>confidence level in <span class="arithmatex">\((0,1)\)</span> for the credible interval</p>
              </div>
            </td>
            <td>
                  <code>0.99</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eval</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if <code>True</code>, disable gradients, otherwise use <code>torch.is_grad_enabled()</code></p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>pmean</code></td>            <td>
                  <code><span title="torch.Tensor">Tensor</span>[..., <span title="T">T</span>, <span title="N">N</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>posterior mean</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>pvar</code></td>            <td>
                  <code><span title="torch.Tensor">Tensor</span>[<span title="T">T</span>, <span title="N">N</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>posterior variance </p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>quantile</code></td>            <td>
                  <code><span title="numpy.float64">float64</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <div class="highlight"><pre><span></span><code><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">confidence</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>pci_low</code></td>            <td>
                  <code><span title="torch.Tensor">Tensor</span>[..., <span title="T">T</span>, <span title="N">N</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>posterior credible interval lower bound</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>pci_high</code></td>            <td>
                  <code><span title="torch.Tensor">Tensor</span>[..., <span title="T">T</span>, <span title="N">N</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>posterior credible interval upper bound</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>fastgps/abstract_gp.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">post_ci</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">task</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">confidence</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="nb">eval</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Posterior credible interval.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor[N,d]): sampling locations</span>
<span class="sd">        task (Union[int,torch.Tensor[T]]): task indices</span>
<span class="sd">        confidence (float): confidence level in $(0,1)$ for the credible interval</span>
<span class="sd">        eval (bool): if `True`, disable gradients, otherwise use `torch.is_grad_enabled()`</span>

<span class="sd">    Returns:</span>
<span class="sd">        pmean (torch.Tensor[...,T,N]): posterior mean</span>
<span class="sd">        pvar (torch.Tensor[T,N]): posterior variance </span>
<span class="sd">        quantile (np.float64):</span>
<span class="sd">            ```python</span>
<span class="sd">            scipy.stats.norm.ppf(1-(1-confidence)/2)</span>
<span class="sd">            ```</span>
<span class="sd">        pci_low (torch.Tensor[...,T,N]): posterior credible interval lower bound</span>
<span class="sd">        pci_high (torch.Tensor[...,T,N]): posterior credible interval upper bound</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">confidence</span><span class="p">)</span> <span class="ow">and</span> <span class="mi">0</span><span class="o">&lt;</span><span class="n">confidence</span><span class="o">&lt;</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;confidence must be between 0 and 1&quot;</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">confidence</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">pmean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span><span class="nb">eval</span><span class="o">=</span><span class="nb">eval</span><span class="p">)</span>
    <span class="n">pvar</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="n">perror</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_error</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span><span class="n">confidence</span><span class="o">=</span><span class="n">confidence</span><span class="p">)</span>
    <span class="n">pci_low</span> <span class="o">=</span> <span class="n">pmean</span><span class="o">-</span><span class="n">q</span><span class="o">*</span><span class="n">perror</span> 
    <span class="n">pci_high</span> <span class="o">=</span> <span class="n">pmean</span><span class="o">+</span><span class="n">q</span><span class="o">*</span><span class="n">perror</span>
    <span class="k">return</span> <span class="n">pmean</span><span class="p">,</span><span class="n">pvar</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="n">pci_low</span><span class="p">,</span><span class="n">pci_high</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="fastgps.abstract_gp.AbstractGP.post_cubature_mean" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">post_cubature_mean</span>


</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">post_cubature_mean</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">eval</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Posterior cubature mean. </p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>eval</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if <code>True</code>, disable gradients, otherwise use <code>torch.is_grad_enabled()</code></p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>task</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>[<span title="T">T</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>task indices</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>pcmean</code></td>            <td>
                  <code><span title="torch.Tensor">Tensor</span>[..., <span title="T">T</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>posterior cubature mean</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>fastgps/abstract_gp.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">post_cubature_mean</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">task</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">eval</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Posterior cubature mean. </span>

<span class="sd">    Args:</span>
<span class="sd">        eval (bool): if `True`, disable gradients, otherwise use `torch.is_grad_enabled()`</span>
<span class="sd">        task (Union[int,torch.Tensor[T]]): task indices</span>

<span class="sd">    Returns:</span>
<span class="sd">        pcmean (torch.Tensor[...,T]): posterior cubature mean</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="fastgps.abstract_gp.AbstractGP.post_cubature_var" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">post_cubature_var</span>


</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">post_cubature_var</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">eval</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Posterior cubature variance. </p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>task</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>[<span title="T">T</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>task indices</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>[<span title="fastgps.abstract_gp.AbstractGP.num_tasks">num_tasks</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of points at which to evaluate the posterior cubature variance.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eval</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if <code>True</code>, disable gradients, otherwise use <code>torch.is_grad_enabled()</code></p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>pcvar</code></td>            <td>
                  <code><span title="torch.Tensor">Tensor</span>[<span title="T">T</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>posterior cubature variance</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>fastgps/abstract_gp.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">post_cubature_var</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">task</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">eval</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Posterior cubature variance. </span>

<span class="sd">    Args:</span>
<span class="sd">        task (Union[int,torch.Tensor[T]]): task indices</span>
<span class="sd">        n (Union[int,torch.Tensor[num_tasks]]): number of points at which to evaluate the posterior cubature variance.</span>
<span class="sd">        eval (bool): if `True`, disable gradients, otherwise use `torch.is_grad_enabled()`</span>

<span class="sd">    Returns:</span>
<span class="sd">        pcvar (torch.Tensor[T]): posterior cubature variance</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="fastgps.abstract_gp.AbstractGP.post_cubature_cov" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">post_cubature_cov</span>


</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">post_cubature_cov</span><span class="p">(</span><span class="n">task0</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">task1</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">eval</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Posterior cubature covariance. </p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>task0</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>[<span title="T1">T1</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>task indices</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>task1</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>[<span title="T2">T2</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>task indices</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>[<span title="fastgps.abstract_gp.AbstractGP.num_tasks">num_tasks</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of points at which to evaluate the posterior cubature covariance.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eval</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if <code>True</code>, disable gradients, otherwise use <code>torch.is_grad_enabled()</code></p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>pcvar</code></td>            <td>
                  <code><span title="torch.Tensor">Tensor</span>[<span title="T1">T1</span>, <span title="T2">T2</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>posterior cubature covariance</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>fastgps/abstract_gp.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">post_cubature_cov</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">task0</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">task1</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">eval</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Posterior cubature covariance. </span>

<span class="sd">    Args:</span>
<span class="sd">        task0 (Union[int,torch.Tensor[T1]]): task indices</span>
<span class="sd">        task1 (Union[int,torch.Tensor[T2]]): task indices</span>
<span class="sd">        n (Union[int,torch.Tensor[num_tasks]]): number of points at which to evaluate the posterior cubature covariance.</span>
<span class="sd">        eval (bool): if `True`, disable gradients, otherwise use `torch.is_grad_enabled()`</span>

<span class="sd">    Returns:</span>
<span class="sd">        pcvar (torch.Tensor[T1,T2]): posterior cubature covariance</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="fastgps.abstract_gp.AbstractGP.post_cubature_error" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">post_cubature_error</span>


</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">post_cubature_error</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="nb">eval</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Posterior cubature error. </p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>task</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>[<span title="T">T</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>task indices</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>[<span title="fastgps.abstract_gp.AbstractGP.num_tasks">num_tasks</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of points at which to evaluate the posterior cubature variance.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eval</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if <code>True</code>, disable gradients, otherwise use <code>torch.is_grad_enabled()</code></p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>confidence</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>confidence level in <span class="arithmatex">\((0,1)\)</span> for the credible interval</p>
              </div>
            </td>
            <td>
                  <code>0.99</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>pcvar</code></td>            <td>
                  <code><span title="torch.Tensor">Tensor</span>[<span title="T">T</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>posterior cubature variance</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>quantile</code></td>            <td>
                  <code><span title="numpy.float64">float64</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <div class="highlight"><pre><span></span><code><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">confidence</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>pcerror</code></td>            <td>
                  <code><span title="torch.Tensor">Tensor</span>[<span title="T">T</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>posterior cubature error</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>fastgps/abstract_gp.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">post_cubature_error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">task</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">confidence</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="nb">eval</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Posterior cubature error. </span>

<span class="sd">    Args:</span>
<span class="sd">        task (Union[int,torch.Tensor[T]]): task indices</span>
<span class="sd">        n (Union[int,torch.Tensor[num_tasks]]): number of points at which to evaluate the posterior cubature variance.</span>
<span class="sd">        eval (bool): if `True`, disable gradients, otherwise use `torch.is_grad_enabled()`</span>
<span class="sd">        confidence (float): confidence level in $(0,1)$ for the credible interval</span>

<span class="sd">    Returns:</span>
<span class="sd">        pcvar (torch.Tensor[T]): posterior cubature variance</span>
<span class="sd">        quantile (np.float64):</span>
<span class="sd">            ```python</span>
<span class="sd">            scipy.stats.norm.ppf(1-(1-confidence)/2)</span>
<span class="sd">            ```</span>
<span class="sd">        pcerror (torch.Tensor[T]): posterior cubature error</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">confidence</span><span class="p">)</span> <span class="ow">and</span> <span class="mi">0</span><span class="o">&lt;</span><span class="n">confidence</span><span class="o">&lt;</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;confidence must be between 0 and 1&quot;</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">confidence</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">pcvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_cubature_var</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span><span class="nb">eval</span><span class="o">=</span><span class="nb">eval</span><span class="p">)</span>
    <span class="n">pcstd</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">pcvar</span><span class="p">)</span>
    <span class="n">pcerror</span> <span class="o">=</span> <span class="n">q</span><span class="o">*</span><span class="n">pcstd</span>
    <span class="k">return</span> <span class="n">pcvar</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="n">pcerror</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="fastgps.abstract_gp.AbstractGP.post_cubature_ci" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">post_cubature_ci</span>


</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">post_cubature_ci</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="nb">eval</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Posterior cubature credible.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>task</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="torch.Tensor">Tensor</span>[<span title="T">T</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>task indices</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>confidence</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>confidence level in <span class="arithmatex">\((0,1)\)</span> for the credible interval</p>
              </div>
            </td>
            <td>
                  <code>0.99</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eval</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if <code>True</code>, disable gradients, otherwise use <code>torch.is_grad_enabled()</code></p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>pcmean</code></td>            <td>
                  <code><span title="torch.Tensor">Tensor</span>[..., <span title="T">T</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>posterior cubature mean</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>pcvar</code></td>            <td>
                  <code><span title="torch.Tensor">Tensor</span>[<span title="T">T</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>posterior cubature variance</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>quantile</code></td>            <td>
                  <code><span title="numpy.float64">float64</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <div class="highlight"><pre><span></span><code><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">confidence</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>pcci_low</code></td>            <td>
                  <code><span title="torch.Tensor">Tensor</span>[..., <span title="T">T</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>posterior cubature credible interval lower bound</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>pcci_high</code></td>            <td>
                  <code><span title="torch.Tensor">Tensor</span>[..., <span title="T">T</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>posterior cubature credible interval upper bound</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>fastgps/abstract_gp.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">post_cubature_ci</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">task</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">confidence</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="nb">eval</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Posterior cubature credible.</span>

<span class="sd">    Args:</span>
<span class="sd">        task (Union[int,torch.Tensor[T]]): task indices</span>
<span class="sd">        confidence (float): confidence level in $(0,1)$ for the credible interval</span>
<span class="sd">        eval (bool): if `True`, disable gradients, otherwise use `torch.is_grad_enabled()`</span>

<span class="sd">    Returns:</span>
<span class="sd">        pcmean (torch.Tensor[...,T]): posterior cubature mean</span>
<span class="sd">        pcvar (torch.Tensor[T]): posterior cubature variance</span>
<span class="sd">        quantile (np.float64):</span>
<span class="sd">            ```python</span>
<span class="sd">            scipy.stats.norm.ppf(1-(1-confidence)/2)</span>
<span class="sd">            ```</span>
<span class="sd">        pcci_low (torch.Tensor[...,T]): posterior cubature credible interval lower bound</span>
<span class="sd">        pcci_high (torch.Tensor[...,T]): posterior cubature credible interval upper bound</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">confidence</span><span class="p">)</span> <span class="ow">and</span> <span class="mi">0</span><span class="o">&lt;</span><span class="n">confidence</span><span class="o">&lt;</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;confidence must be between 0 and 1&quot;</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">confidence</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">pcmean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_cubature_mean</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span><span class="nb">eval</span><span class="o">=</span><span class="nb">eval</span><span class="p">)</span> 
    <span class="n">pcvar</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="n">pcerror</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_cubature_error</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span><span class="n">confidence</span><span class="o">=</span><span class="n">confidence</span><span class="p">,</span><span class="nb">eval</span><span class="o">=</span><span class="nb">eval</span><span class="p">)</span>
    <span class="n">pcci_low</span> <span class="o">=</span> <span class="n">pcmean</span><span class="o">-</span><span class="n">pcerror</span>
    <span class="n">pcci_high</span> <span class="o">=</span> <span class="n">pcmean</span><span class="o">+</span><span class="n">pcerror</span>
    <span class="k">return</span> <span class="n">pcmean</span><span class="p">,</span><span class="n">pcvar</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="n">pcci_low</span><span class="p">,</span><span class="n">pcci_high</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">




    <div class="doc doc-contents first">









  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="fastgps.standard_gp.StandardGP" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">StandardGP</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">StandardGP</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span> <span class="n">num_tasks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed_for_seq</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">lengthscales</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">factor_task_kernel</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">rank_factor_task_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">noise_task_kernel</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">tfs_scale</span><span class="o">=</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">tfs_lengthscales</span><span class="o">=</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">tfs_noise</span><span class="o">=</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">tfs_factor_task_kernel</span><span class="o">=</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">),</span> <span class="n">tfs_noise_task_kernel</span><span class="o">=</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">requires_grad_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">requires_grad_lengthscales</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">requires_grad_noise</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">requires_grad_factor_task_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">requires_grad_noise_task_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape_batch</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([]),</span> <span class="n">shape_scale</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span> <span class="n">shape_lengthscales</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape_noise</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span> <span class="n">shape_factor_task_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape_noise_task_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">derivatives</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">derivatives_coeffs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kernel_class</span><span class="o">=</span><span class="s1">&#39;Gaussian&#39;</span><span class="p">,</span> <span class="n">adaptive_nugget</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">compile_dist_func</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">compile_dist_func_kwargs</span><span class="o">=</span><span class="p">{})</span>
</code></pre></div>

    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="AbstractGP (fastgps.abstract_gp.AbstractGP)" href="#fastgps.abstract_gp.AbstractGP">AbstractGP</a></code></p>


        <p>Standard Gaussian process regression</p>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">set_default_dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">f_ackley</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">scaling</span><span class="o">=</span><span class="mf">32.768</span><span class="p">):</span>
<span class="gp">... </span>    <span class="c1"># https://www.sfu.ca/~ssurjano/ackley.html</span>
<span class="gp">... </span>    <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">2</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">scaling</span><span class="o">*</span><span class="n">x</span><span class="o">-</span><span class="n">scaling</span>
<span class="gp">... </span>    <span class="n">t1</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span>
<span class="gp">... </span>    <span class="n">t2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">c</span><span class="o">*</span><span class="n">x</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span>
<span class="gp">... </span>    <span class="n">t3</span> <span class="o">=</span> <span class="n">a</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">y</span> <span class="o">=</span> <span class="o">-</span><span class="n">t1</span><span class="o">-</span><span class="n">t2</span><span class="o">+</span><span class="n">t3</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">y</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">n</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sgp</span> <span class="o">=</span> <span class="n">StandardGP</span><span class="p">(</span><span class="n">qmcpy</span><span class="o">.</span><span class="n">DigitalNetB2</span><span class="p">(</span><span class="n">dimension</span><span class="o">=</span><span class="n">d</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_next</span> <span class="o">=</span> <span class="n">sgp</span><span class="o">.</span><span class="n">get_x_next</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_next</span> <span class="o">=</span> <span class="n">f_ackley</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sgp</span><span class="o">.</span><span class="n">add_y_next</span><span class="p">(</span><span class="n">y_next</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">17</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="o">**</span><span class="mi">7</span><span class="p">,</span><span class="n">d</span><span class="p">),</span><span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">f_ackley</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pmean</span> <span class="o">=</span> <span class="n">sgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pmean</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([128])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">pmean</span><span class="p">)</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">tensor(0.0794)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">sgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">sgp</span><span class="o">.</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="n">sgp</span><span class="o">.</span><span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">tensor(0.0524)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">sgp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="go">[&#39;iterations&#39;]</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">sgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">tensor(0.0822)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="o">**</span><span class="mi">8</span><span class="p">,</span><span class="n">d</span><span class="p">),</span><span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcov</span> <span class="o">=</span> <span class="n">sgp</span><span class="o">.</span><span class="n">post_cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcov</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([128, 256])</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pcov</span> <span class="o">=</span> <span class="n">sgp</span><span class="o">.</span><span class="n">post_cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcov</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([128, 128])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="p">(</span><span class="n">pcov</span><span class="o">.</span><span class="n">diagonal</span><span class="p">()</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pvar</span> <span class="o">=</span> <span class="n">sgp</span><span class="o">.</span><span class="n">post_var</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pvar</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([128])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">pcov</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(),</span><span class="n">pvar</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pmean</span><span class="p">,</span><span class="n">pstd</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="n">ci_low</span><span class="p">,</span><span class="n">ci_high</span> <span class="o">=</span> <span class="n">sgp</span><span class="o">.</span><span class="n">post_ci</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">confidence</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ci_low</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([128])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ci_high</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([128])</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">sgp</span><span class="o">.</span><span class="n">post_cubature_mean</span><span class="p">()</span>
<span class="go">tensor(20.0282)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sgp</span><span class="o">.</span><span class="n">post_cubature_var</span><span class="p">()</span>
<span class="go">tensor(0.0082)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pcmean</span><span class="p">,</span><span class="n">pcvar</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="n">pcci_low</span><span class="p">,</span><span class="n">pcci_high</span> <span class="o">=</span> <span class="n">sgp</span><span class="o">.</span><span class="n">post_cubature_ci</span><span class="p">(</span><span class="n">confidence</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcci_low</span>
<span class="go">tensor(19.7948)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcci_high</span>
<span class="go">tensor(20.2616)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pcov_future</span> <span class="o">=</span> <span class="n">sgp</span><span class="o">.</span><span class="n">post_cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pvar_future</span> <span class="o">=</span> <span class="n">sgp</span><span class="o">.</span><span class="n">post_var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcvar_future</span> <span class="o">=</span> <span class="n">sgp</span><span class="o">.</span><span class="n">post_cubature_var</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">x_next</span> <span class="o">=</span> <span class="n">sgp</span><span class="o">.</span><span class="n">get_x_next</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_next</span> <span class="o">=</span> <span class="n">f_ackley</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sgp</span><span class="o">.</span><span class="n">add_y_next</span><span class="p">(</span><span class="n">y_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">sgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">tensor(0.1161)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">sgp</span><span class="o">.</span><span class="n">post_cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">),</span><span class="n">pcov_future</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">sgp</span><span class="o">.</span><span class="n">post_var</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">pvar_future</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">sgp</span><span class="o">.</span><span class="n">post_cubature_var</span><span class="p">(),</span><span class="n">pcvar_future</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">sgp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">sgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">tensor(0.0626)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">x_next</span> <span class="o">=</span> <span class="n">sgp</span><span class="o">.</span><span class="n">get_x_next</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_next</span> <span class="o">=</span> <span class="n">f_ackley</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sgp</span><span class="o">.</span><span class="n">add_y_next</span><span class="p">(</span><span class="n">y_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">sgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">tensor(0.0990)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">sgp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">sgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">tensor(0.0613)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pcov_16n</span> <span class="o">=</span> <span class="n">sgp</span><span class="o">.</span><span class="n">post_cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="mi">16</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pvar_16n</span> <span class="o">=</span> <span class="n">sgp</span><span class="o">.</span><span class="n">post_var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="mi">16</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcvar_16n</span> <span class="o">=</span> <span class="n">sgp</span><span class="o">.</span><span class="n">post_cubature_var</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">16</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_next</span> <span class="o">=</span> <span class="n">sgp</span><span class="o">.</span><span class="n">get_x_next</span><span class="p">(</span><span class="mi">16</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_next</span> <span class="o">=</span> <span class="n">f_ackley</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sgp</span><span class="o">.</span><span class="n">add_y_next</span><span class="p">(</span><span class="n">y_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">sgp</span><span class="o">.</span><span class="n">post_cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">),</span><span class="n">pcov_16n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">sgp</span><span class="o">.</span><span class="n">post_var</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">pvar_16n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">sgp</span><span class="o">.</span><span class="n">post_cubature_var</span><span class="p">(),</span><span class="n">pcvar_16n</span><span class="p">)</span>
</code></pre></div>



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>seqs</code>
            </td>
            <td>
                  <code>Union[int,qmcpy.DiscreteDistribution,List]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of sequence generators. If an int <code>d</code> is passed in we use 
<div class="highlight"><pre><span></span><code><span class="p">[</span><span class="n">qmcpy</span><span class="o">.</span><span class="n">DigitalNetB2</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span> <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">SeedSequence</span><span class="p">(</span><span class="n">seed_for_seq</span><span class="p">)</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">)]</span>
</code></pre></div>
See the <a href="https://qmcpy.readthedocs.io/en/latest/algorithms.html#discrete-distribution-class" target="_blank"><code>qmcpy.DiscreteDistribution</code> docs</a> for more info. </p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_tasks</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of tasks </p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>seed_for_seq</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>seed used for digital net randomization</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>scale</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>kernel global scaling parameter</p>
              </div>
            </td>
            <td>
                  <code>1.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lengthscales</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>[<span title="d">d</span>], <span title="float">float</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>vector of kernel lengthscales. 
If a scalar is passed in then <code>lengthscales</code> is set to a constant vector. </p>
              </div>
            </td>
            <td>
                  <code>1.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>noise</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>positive noise variance i.e. nugget term</p>
              </div>
            </td>
            <td>
                  <code>0.0001</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>factor_task_kernel</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="Tensor">Tensor</span>[<span title="fastgps.standard_gp.StandardGP(num_tasks)">num_tasks</span>, <span title="fastgps.standard_gp.StandardGP(rank_factor_task_kernel)">rank_factor_task_kernel</span>], <span title="int">int</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>for <span class="arithmatex">\(F\)</span> the <code>factor_task_kernel</code> the task kernel is <span class="arithmatex">\(FF^T + \text{diag}(\boldsymbol{v})\)</span> 
where <code>rank_factor_task_kernel&lt;=num_tasks</code> and <span class="arithmatex">\(\boldsymbol{v}\)</span> is the <code>noise_task_kernel</code>.</p>
              </div>
            </td>
            <td>
                  <code>1.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>rank_factor_task_kernel</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>see the description of <code>factor_task_kernel</code> above. Defaults to 0 for single task problems and 1 for multi task problems.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>noise_task_kernel</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>[<span title="fastgps.standard_gp.StandardGP(num_tasks)">num_tasks</span>], <span title="float">float</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>see the description of <code>factor_task_kernel</code> above </p>
              </div>
            </td>
            <td>
                  <code>1.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code><span title="torch.device">device</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>torch device which is required to support <code>torch.float64</code></p>
              </div>
            </td>
            <td>
                  <code>&#39;cpu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tfs_scale</code>
            </td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span>[<span title="callable">callable</span>, <span title="callable">callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the first argument transforms to the raw value to be optimized, the second applies the inverse transform</p>
              </div>
            </td>
            <td>
                  <code>(lambda x: <span title="torch.log">log</span>(<span title="x">x</span>), lambda x: <span title="torch.exp">exp</span>(<span title="x">x</span>))</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tfs_lengthscales</code>
            </td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span>[<span title="callable">callable</span>, <span title="callable">callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the first argument transforms to the raw value to be optimized, the second applies the inverse transform</p>
              </div>
            </td>
            <td>
                  <code>(lambda x: <span title="torch.log">log</span>(<span title="x">x</span>), lambda x: <span title="torch.exp">exp</span>(<span title="x">x</span>))</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tfs_noise</code>
            </td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span>[<span title="callable">callable</span>, <span title="callable">callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the first argument transforms to the raw value to be optimized, the second applies the inverse transform</p>
              </div>
            </td>
            <td>
                  <code>(lambda x: <span title="torch.log">log</span>(<span title="x">x</span>), lambda x: <span title="torch.exp">exp</span>(<span title="x">x</span>))</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tfs_factor_task_kernel</code>
            </td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span>[<span title="callable">callable</span>, <span title="callable">callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the first argument transforms to the raw value to be optimized, the second applies the inverse transform</p>
              </div>
            </td>
            <td>
                  <code>(lambda x: <span title="x">x</span>, lambda x: <span title="x">x</span>)</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tfs_noise_task_kernel</code>
            </td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span>[<span title="callable">callable</span>, <span title="callable">callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the first argument transforms to the raw value to be optimized, the second applies the inverse transform</p>
              </div>
            </td>
            <td>
                  <code>(lambda x: <span title="torch.log">log</span>(<span title="x">x</span>), lambda x: <span title="torch.exp">exp</span>(<span title="x">x</span>))</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>requires_grad_scale</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>wheather or not to optimize the scale parameter</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>requires_grad_lengthscales</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>wheather or not to optimize lengthscale parameters</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>requires_grad_noise</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>wheather or not to optimize the noise parameter</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>requires_grad_factor_task_kernel</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>wheather or not to optimize the factor for the task kernel</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>requires_grad_noise_task_kernel</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>wheather or not to optimize the noise for the task kernel</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shape_batch</code>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>shape of the batch output for each task</p>
              </div>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span>([])</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shape_scale</code>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>shape of the scale parameter, defaults to <code>torch.Size([1])</code></p>
              </div>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span>([1])</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shape_lengthscales</code>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>shape of the lengthscales parameter, defaults to <code>torch.Size([d])</code> where <code>d</code> is the dimension</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shape_noise</code>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>shape of the noise parameter, defaults to <code>torch.Size([1])</code></p>
              </div>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span>([1])</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shape_factor_task_kernel</code>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>shape of the factor for the task kernel, defaults to <code>torch.Size([num_tasks,r])</code> where <code>r</code> is the rank, see the description of <code>factor_task_kernel</code></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shape_noise_task_kernel</code>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>shape of the noise for the task kernel, defaults to <code>torch.Size([num_tasks])</code></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>derivatives</code>
            </td>
            <td>
                  <code><span title="list">list</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of derivative orders e.g. to include a function and its gradient set 
<div class="highlight"><pre><span></span><code><span class="n">derivatives</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)]</span><span class="o">+</span><span class="p">[</span><span class="n">ej</span> <span class="k">for</span> <span class="n">ej</span> <span class="ow">in</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)]</span>
</code></pre></div></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>derivatives_coeffs</code>
            </td>
            <td>
                  <code><span title="list">list</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of derivative coefficients where if <code>derivatives[k].shape==(p,d)</code> then we should have <code>derivatives_coeffs[k].shape==(p,)</code></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adaptive_nugget</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True, use the adaptive nugget which modifies noises based on trace ratios.  </p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>data</code>
            </td>
            <td>
                  <code><span title="dict">dict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>dictory of data with keys 'x' and 'y' where data['x'] and data['y'] are both <code>torch.Tensor</code>s or list of <code>torch.Tensor</code>s with lengths equal to the number of tasks</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>compile_dist_func</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if <code>True</code>, use compile the pairwise distance function for memory efficiency when evaluating the kernel matrix.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>compile_dist_func_kwargs</code>
            </td>
            <td>
                  <code><span title="dict">dict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>keyword arguments to <code>torch.compile</code> used when <code>compile_dist_func=True</code>.</p>
              </div>
            </td>
            <td>
                  <code>{}</code>
            </td>
          </tr>
      </tbody>
    </table>







                  <details class="quote">
                    <summary>Source code in <code>fastgps/standard_gp.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">seqs</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="n">qmcpy</span><span class="o">.</span><span class="n">IIDStdUniform</span><span class="p">,</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">num_tasks</span><span class="p">:</span><span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">seed_for_seq</span><span class="p">:</span><span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span><span class="nb">float</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> 
        <span class="n">lengthscales</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> 
        <span class="n">noise</span><span class="p">:</span><span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span>
        <span class="n">factor_task_kernel</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span>
        <span class="n">rank_factor_task_kernel</span><span class="p">:</span><span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">noise_task_kernel</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">tfs_scale</span><span class="p">:</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">callable</span><span class="p">,</span><span class="nb">callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)),(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))),</span>
        <span class="n">tfs_lengthscales</span><span class="p">:</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">callable</span><span class="p">,</span><span class="nb">callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)),(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))),</span>
        <span class="n">tfs_noise</span><span class="p">:</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">callable</span><span class="p">,</span><span class="nb">callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)),(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))),</span>
        <span class="n">tfs_factor_task_kernel</span><span class="p">:</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">callable</span><span class="p">,</span><span class="nb">callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)),</span><span class="c1">#((lambda x: x**(1/3)),(lambda x: x**3)),</span>
        <span class="n">tfs_noise_task_kernel</span><span class="p">:</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">callable</span><span class="p">,</span><span class="nb">callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)),(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))),</span>
        <span class="n">requires_grad_scale</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> 
        <span class="n">requires_grad_lengthscales</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> 
        <span class="n">requires_grad_noise</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> 
        <span class="n">requires_grad_factor_task_kernel</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">requires_grad_noise_task_kernel</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shape_batch</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([]),</span>
        <span class="n">shape_scale</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span> 
        <span class="n">shape_lengthscales</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shape_noise</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span>
        <span class="n">shape_factor_task_kernel</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
        <span class="n">shape_noise_task_kernel</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">derivatives</span><span class="p">:</span><span class="nb">list</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">derivatives_coeffs</span><span class="p">:</span><span class="nb">list</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kernel_class</span><span class="p">:</span><span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;Gaussian&quot;</span><span class="p">,</span>
        <span class="n">adaptive_nugget</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span><span class="nb">dict</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">compile_dist_func</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">compile_dist_func_kwargs</span><span class="p">:</span><span class="nb">dict</span> <span class="o">=</span> <span class="p">{},</span>
        <span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        seqs (Union[int,qmcpy.DiscreteDistribution,List]]): list of sequence generators. If an int `d` is passed in we use </span>
<span class="sd">            ```python</span>
<span class="sd">            [qmcpy.DigitalNetB2(d,seed=seed) for seed in np.random.SeedSequence(seed_for_seq).spawn(num_tasks)]</span>
<span class="sd">            ```</span>
<span class="sd">            See the &lt;a href=&quot;https://qmcpy.readthedocs.io/en/latest/algorithms.html#discrete-distribution-class&quot; target=&quot;_blank&quot;&gt;`qmcpy.DiscreteDistribution` docs&lt;/a&gt; for more info. </span>
<span class="sd">        num_tasks (int): number of tasks </span>
<span class="sd">        seed_for_seq (int): seed used for digital net randomization</span>
<span class="sd">        scale (float): kernel global scaling parameter</span>
<span class="sd">        lengthscales (Union[torch.Tensor[d],float]): vector of kernel lengthscales. </span>
<span class="sd">            If a scalar is passed in then `lengthscales` is set to a constant vector. </span>
<span class="sd">        noise (float): positive noise variance i.e. nugget term</span>
<span class="sd">        factor_task_kernel (Union[Tensor[num_tasks,rank_factor_task_kernel],int]): for $F$ the `factor_task_kernel` the task kernel is $FF^T + \\text{diag}(\\boldsymbol{v})$ </span>
<span class="sd">            where `rank_factor_task_kernel&lt;=num_tasks` and $\\boldsymbol{v}$ is the `noise_task_kernel`.</span>
<span class="sd">        rank_factor_task_kernel (int): see the description of `factor_task_kernel` above. Defaults to 0 for single task problems and 1 for multi task problems.</span>
<span class="sd">        noise_task_kernel (Union[torch.Tensor[num_tasks],float]): see the description of `factor_task_kernel` above </span>
<span class="sd">        device (torch.device): torch device which is required to support `torch.float64`</span>
<span class="sd">        tfs_scale (Tuple[callable,callable]): the first argument transforms to the raw value to be optimized, the second applies the inverse transform</span>
<span class="sd">        tfs_lengthscales (Tuple[callable,callable]): the first argument transforms to the raw value to be optimized, the second applies the inverse transform</span>
<span class="sd">        tfs_noise (Tuple[callable,callable]): the first argument transforms to the raw value to be optimized, the second applies the inverse transform</span>
<span class="sd">        tfs_factor_task_kernel (Tuple[callable,callable]): the first argument transforms to the raw value to be optimized, the second applies the inverse transform</span>
<span class="sd">        tfs_noise_task_kernel (Tuple[callable,callable]): the first argument transforms to the raw value to be optimized, the second applies the inverse transform</span>
<span class="sd">        requires_grad_scale (bool): wheather or not to optimize the scale parameter</span>
<span class="sd">        requires_grad_lengthscales (bool): wheather or not to optimize lengthscale parameters</span>
<span class="sd">        requires_grad_noise (bool): wheather or not to optimize the noise parameter</span>
<span class="sd">        requires_grad_factor_task_kernel (bool): wheather or not to optimize the factor for the task kernel</span>
<span class="sd">        requires_grad_noise_task_kernel (bool): wheather or not to optimize the noise for the task kernel</span>
<span class="sd">        shape_batch (torch.Size): shape of the batch output for each task</span>
<span class="sd">        shape_scale (torch.Size): shape of the scale parameter, defaults to `torch.Size([1])`</span>
<span class="sd">        shape_lengthscales (torch.Size): shape of the lengthscales parameter, defaults to `torch.Size([d])` where `d` is the dimension</span>
<span class="sd">        shape_noise (torch.Size): shape of the noise parameter, defaults to `torch.Size([1])`</span>
<span class="sd">        shape_factor_task_kernel (torch.Size): shape of the factor for the task kernel, defaults to `torch.Size([num_tasks,r])` where `r` is the rank, see the description of `factor_task_kernel`</span>
<span class="sd">        shape_noise_task_kernel (torch.Size): shape of the noise for the task kernel, defaults to `torch.Size([num_tasks])`</span>
<span class="sd">        derivatives (list): list of derivative orders e.g. to include a function and its gradient set </span>
<span class="sd">            ```python</span>
<span class="sd">            derivatives = [torch.zeros(d,dtype=int)]+[ej for ej in torch.eye(d,dtype=int)]</span>
<span class="sd">            ```</span>
<span class="sd">        derivatives_coeffs (list): list of derivative coefficients where if `derivatives[k].shape==(p,d)` then we should have `derivatives_coeffs[k].shape==(p,)`</span>
<span class="sd">        adaptive_nugget (bool): if True, use the adaptive nugget which modifies noises based on trace ratios.  </span>
<span class="sd">        data (dict): dictory of data with keys &#39;x&#39; and &#39;y&#39; where data[&#39;x&#39;] and data[&#39;y&#39;] are both `torch.Tensor`s or list of `torch.Tensor`s with lengths equal to the number of tasks</span>
<span class="sd">        compile_dist_func (bool): if `True`, use compile the pairwise distance function for memory efficiency when evaluating the kernel matrix.</span>
<span class="sd">        compile_dist_func_kwargs (dict): keyword arguments to `torch.compile` used when `compile_dist_func=True`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">num_tasks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> 
        <span class="n">solo_task</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">default_task</span> <span class="o">=</span> <span class="mi">0</span> 
        <span class="n">num_tasks</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">num_tasks</span><span class="o">&gt;</span><span class="mi">0</span>
        <span class="n">solo_task</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">default_task</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="nb">int</span><span class="p">),</span> <span class="s2">&quot;passing in data requires seqs (the first argument) is a int specifying the dimension&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;x&quot;</span> <span class="ow">in</span> <span class="n">data</span> <span class="ow">and</span> <span class="s2">&quot;y&quot;</span> <span class="ow">in</span> <span class="n">data</span><span class="p">,</span> <span class="s2">&quot;data must be a dict with keys &#39;x&#39; and &#39;y&#39;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">]]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">],</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]]</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span><span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">])</span><span class="o">==</span><span class="n">num_tasks</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x_l</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">x_l</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">2</span> <span class="ow">and</span> <span class="n">x_l</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">==</span><span class="n">seqs</span> <span class="k">for</span> <span class="n">x_l</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">]),</span> <span class="s2">&quot;data[&#39;x&#39;] should be a list of 2d tensors of length num_tasks with each number of columns equal to the dimension&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">],</span><span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">])</span><span class="o">==</span><span class="n">num_tasks</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">y_l</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">y_l</span><span class="o">.</span><span class="n">ndim</span><span class="o">&gt;=</span><span class="mi">1</span> <span class="k">for</span> <span class="n">y_l</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]),</span> <span class="s2">&quot;data[&#39;y&#39;] should be a list of tensors of length num_tasks&quot;</span>
        <span class="n">seqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">DummyDiscreteDistrib</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">][</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">)],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="nb">int</span><span class="p">):</span>
            <span class="n">seqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">qmcpy</span><span class="o">.</span><span class="n">DigitalNetB2</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span><span class="n">order</span><span class="o">=</span><span class="s2">&quot;GRAY&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">SeedSequence</span><span class="p">(</span><span class="n">seed_for_seq</span><span class="p">)</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">)],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="n">qmcpy</span><span class="o">.</span><span class="n">DiscreteDistribution</span><span class="p">):</span>
            <span class="n">seqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">seqs</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="nb">list</span><span class="p">):</span>
            <span class="n">seqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">seqs</span><span class="o">.</span><span class="n">shape</span><span class="o">==</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">,),</span> <span class="s2">&quot;seqs should be a length num_tasks=</span><span class="si">%d</span><span class="s2"> list&quot;</span><span class="o">%</span><span class="n">num_tasks</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">seqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">replications</span><span class="o">==</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">))</span> <span class="ow">and</span> <span class="s2">&quot;each seq should have only 1 replication&quot;</span>
    <span class="n">kernel_class</span> <span class="o">=</span> <span class="n">kernel_class</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">available_kernel_classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;gaussian&#39;</span><span class="p">,</span><span class="s1">&#39;matern12&#39;</span><span class="p">,</span><span class="s1">&#39;matern32&#39;</span><span class="p">,</span><span class="s1">&#39;matern52&#39;</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">kernel_class</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">available_kernel_classes</span><span class="p">,</span> <span class="s2">&quot;kernel_class must in </span><span class="si">%s</span><span class="s2">&quot;</span><span class="o">%</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">available_kernel_classes</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_class</span> <span class="o">=</span> <span class="n">kernel_class</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">compile_dist_func</span><span class="p">,</span><span class="nb">bool</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_class</span><span class="o">==</span><span class="s2">&quot;gaussian&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unscaled_gaussian_kernel</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">lengthscales</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">x1</span><span class="o">-</span><span class="n">x2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">lengthscales</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">compile_dist_func</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">unscaled_gaussian_kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unscaled_gaussian_kernel</span><span class="p">,</span><span class="o">**</span><span class="n">compile_dist_func_kwargs</span><span class="p">)</span>
    <span class="k">elif</span> <span class="s2">&quot;matern&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_class</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parise_rel_dist_func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">lengthscales</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x1</span><span class="o">-</span><span class="n">x2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">lengthscales</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">compile_dist_func</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parise_rel_dist_func</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parise_rel_dist_func</span><span class="p">,</span><span class="o">**</span><span class="n">compile_dist_func_kwargs</span><span class="p">)</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">seqs</span><span class="p">,</span>
        <span class="n">num_tasks</span><span class="p">,</span>
        <span class="n">default_task</span><span class="p">,</span>
        <span class="n">solo_task</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">lengthscales</span><span class="p">,</span>
        <span class="n">noise</span><span class="p">,</span>
        <span class="n">factor_task_kernel</span><span class="p">,</span>
        <span class="n">rank_factor_task_kernel</span><span class="p">,</span>
        <span class="n">noise_task_kernel</span><span class="p">,</span>
        <span class="n">device</span><span class="p">,</span>
        <span class="n">tfs_scale</span><span class="p">,</span>
        <span class="n">tfs_lengthscales</span><span class="p">,</span>
        <span class="n">tfs_noise</span><span class="p">,</span>
        <span class="n">tfs_factor_task_kernel</span><span class="p">,</span>
        <span class="n">tfs_noise_task_kernel</span><span class="p">,</span>
        <span class="n">requires_grad_scale</span><span class="p">,</span>
        <span class="n">requires_grad_lengthscales</span><span class="p">,</span>
        <span class="n">requires_grad_noise</span><span class="p">,</span>
        <span class="n">requires_grad_factor_task_kernel</span><span class="p">,</span>
        <span class="n">requires_grad_noise_task_kernel</span><span class="p">,</span>
        <span class="n">shape_batch</span><span class="p">,</span>
        <span class="n">shape_scale</span><span class="p">,</span> 
        <span class="n">shape_lengthscales</span><span class="p">,</span>
        <span class="n">shape_noise</span><span class="p">,</span>
        <span class="n">shape_factor_task_kernel</span><span class="p">,</span> 
        <span class="n">shape_noise_task_kernel</span><span class="p">,</span>
        <span class="n">derivatives</span><span class="p">,</span>
        <span class="n">derivatives_coeffs</span><span class="p">,</span>
        <span class="n">adaptive_nugget</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_y_next</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">],</span><span class="n">task</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">




    <div class="doc doc-contents first">









  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="fastgps.abstract_fast_gp.AbstractFastGP" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">AbstractFastGP</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">AbstractFastGP</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">ft</span><span class="p">,</span> <span class="n">ift</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="AbstractGP (fastgps.abstract_gp.AbstractGP)" href="#fastgps.abstract_gp.AbstractGP">AbstractGP</a></code></p>








                  <details class="quote">
                    <summary>Source code in <code>fastgps/abstract_fast_gp.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">,</span>
        <span class="n">ft</span><span class="p">,</span>
        <span class="n">ift</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="c1"># alpha</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="ow">and</span> <span class="n">alpha</span><span class="o">%</span><span class="mi">1</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">alpha</span><span class="o">.</span><span class="n">shape</span><span class="o">==</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">,)),</span> <span class="s2">&quot;alpha should be an int or a torch.Tensor of length d&quot;</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">alpha</span><span class="p">):</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
    <span class="c1"># fast transforms </span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ft_unstable</span> <span class="o">=</span> <span class="n">ft</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ift_unstable</span> <span class="o">=</span> <span class="n">ift</span>
    <span class="c1"># storage and dynamic caches</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">k1parts_seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">_K1PartsSeq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">xxb_seqs</span><span class="p">[</span><span class="n">l0</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">xxb_seqs</span><span class="p">[</span><span class="n">l1</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span><span class="p">[</span><span class="n">l0</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span><span class="p">[</span><span class="n">l1</span><span class="p">])</span> <span class="k">if</span> <span class="n">l1</span><span class="o">&gt;=</span><span class="n">l0</span> <span class="k">else</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">l1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">)]</span> <span class="k">for</span> <span class="n">l0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">)],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lam_caches</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">_LamCaches</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">l0</span><span class="p">,</span><span class="n">l1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span><span class="p">[</span><span class="n">l0</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span><span class="p">[</span><span class="n">l1</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives_coeffs</span><span class="p">[</span><span class="n">l0</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">derivatives_coeffs</span><span class="p">[</span><span class="n">l1</span><span class="p">])</span> <span class="k">if</span> <span class="n">l1</span><span class="o">&gt;=</span><span class="n">l0</span> <span class="k">else</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">l1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">)]</span> <span class="k">for</span> <span class="n">l0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">)],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ytilde_cache</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">_YtildeCache</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">)],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="fastgps.abstract_fast_gp.AbstractFastGP.ft" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">ft</span>


</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">ft</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>One dimensional fast transform along the last dimenions. 
    For <code>FastGPLattice</code> this is the orthonormal Fast Fourier Transform (FFT). 
    For <code>FastGPDigitalNetB2</code> this is the orthonormal Fast Walsh Hadamard Transform (FWHT). </p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>inputs to be transformed along the last dimension. Require <code>n = x.size(-1)</code> is a power of 2. </p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>y</code></td>            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>transformed inputs with the same shape as <code>x</code></p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>fastgps/abstract_fast_gp.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">ft</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    One dimensional fast transform along the last dimenions. </span>
<span class="sd">        For `FastGPLattice` this is the orthonormal Fast Fourier Transform (FFT). </span>
<span class="sd">        For `FastGPDigitalNetB2` this is the orthonormal Fast Walsh Hadamard Transform (FWHT). </span>

<span class="sd">    Args: </span>
<span class="sd">        x (torch.Tensor): inputs to be transformed along the last dimension. Require `n = x.size(-1)` is a power of 2. </span>

<span class="sd">    Returns: </span>
<span class="sd">        y (torch.Tensor): transformed inputs with the same shape as `x` </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">xmean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ft_unstable</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">xmean</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="kc">None</span><span class="p">])</span>
    <span class="n">y</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">xmean</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">y</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="fastgps.abstract_fast_gp.AbstractFastGP.ift" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">ift</span>


</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">ift</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>One dimensional inverse fast transform along the last dimenions. 
    For <code>FastGPLattice</code> this is the orthonormal Inverse Fast Fourier Transform (IFFT). 
    For <code>FastGPDigitalNetB2</code> this is the orthonormal Fast Walsh Hadamard Transform (FWHT). </p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>inputs to be transformed along the last dimension. Require <code>n = x.size(-1)</code> is a power of 2. </p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>y</code></td>            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>transformed inputs with the same shape as <code>x</code></p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>fastgps/abstract_fast_gp.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">ift</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    One dimensional inverse fast transform along the last dimenions. </span>
<span class="sd">        For `FastGPLattice` this is the orthonormal Inverse Fast Fourier Transform (IFFT). </span>
<span class="sd">        For `FastGPDigitalNetB2` this is the orthonormal Fast Walsh Hadamard Transform (FWHT). </span>

<span class="sd">    Args: </span>
<span class="sd">        x (torch.Tensor): inputs to be transformed along the last dimension. Require `n = x.size(-1)` is a power of 2. </span>

<span class="sd">    Returns: </span>
<span class="sd">        y (torch.Tensor): transformed inputs with the same shape as `x` </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">xmean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ift_unstable</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">xmean</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="kc">None</span><span class="p">])</span>
    <span class="n">y</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">xmean</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">y</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">




    <div class="doc doc-contents first">









  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="fastgps.fast_gp_lattice.FastGPLattice" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">FastGPLattice</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">FastGPLattice</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span> <span class="n">num_tasks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed_for_seq</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">lengthscales</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span> <span class="n">factor_task_kernel</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">rank_factor_task_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">noise_task_kernel</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">tfs_scale</span><span class="o">=</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">tfs_lengthscales</span><span class="o">=</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">tfs_noise</span><span class="o">=</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">tfs_factor_task_kernel</span><span class="o">=</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">),</span> <span class="n">tfs_noise_task_kernel</span><span class="o">=</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">requires_grad_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">requires_grad_lengthscales</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">requires_grad_noise</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">requires_grad_factor_task_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">requires_grad_noise_task_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape_batch</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([]),</span> <span class="n">shape_scale</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span> <span class="n">shape_lengthscales</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape_noise</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span> <span class="n">shape_factor_task_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape_noise_task_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">derivatives</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">derivatives_coeffs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">compile_fts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">compile_fts_kwargs</span><span class="o">=</span><span class="p">{},</span> <span class="n">adaptive_nugget</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="AbstractFastGP (fastgps.abstract_fast_gp.AbstractFastGP)" href="#fastgps.abstract_fast_gp.AbstractFastGP">AbstractFastGP</a></code></p>


        <p>Fast Gaussian process regression using lattice points and shift invariant kernels</p>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">set_default_dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">f_ackley</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">scaling</span><span class="o">=</span><span class="mf">32.768</span><span class="p">):</span>
<span class="gp">... </span>    <span class="c1"># https://www.sfu.ca/~ssurjano/ackley.html</span>
<span class="gp">... </span>    <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">2</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">scaling</span><span class="o">*</span><span class="n">x</span><span class="o">-</span><span class="n">scaling</span>
<span class="gp">... </span>    <span class="n">t1</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span>
<span class="gp">... </span>    <span class="n">t2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">c</span><span class="o">*</span><span class="n">x</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span>
<span class="gp">... </span>    <span class="n">t3</span> <span class="o">=</span> <span class="n">a</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">y</span> <span class="o">=</span> <span class="o">-</span><span class="n">t1</span><span class="o">-</span><span class="n">t2</span><span class="o">+</span><span class="n">t3</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">y</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">n</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fgp</span> <span class="o">=</span> <span class="n">FastGPLattice</span><span class="p">(</span><span class="n">seqs</span> <span class="o">=</span> <span class="n">qmcpy</span><span class="o">.</span><span class="n">Lattice</span><span class="p">(</span><span class="n">dimension</span><span class="o">=</span><span class="n">d</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_next</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">get_x_next</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_next</span> <span class="o">=</span> <span class="n">f_ackley</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fgp</span><span class="o">.</span><span class="n">add_y_next</span><span class="p">(</span><span class="n">y_next</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">17</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="o">**</span><span class="mi">7</span><span class="p">,</span><span class="n">d</span><span class="p">),</span><span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">f_ackley</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pmean</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pmean</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([128])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">pmean</span><span class="p">)</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">tensor(0.0348)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">fgp</span><span class="o">.</span><span class="n">x</span><span class="p">),</span><span class="n">fgp</span><span class="o">.</span><span class="n">y</span><span class="p">,</span><span class="n">atol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">fgp</span><span class="o">.</span><span class="n">post_cubature_mean</span><span class="p">()</span>
<span class="go">tensor(20.1842)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fgp</span><span class="o">.</span><span class="n">post_cubature_var</span><span class="p">()</span>
<span class="go">tensor(7.0015e-09)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="go">[&#39;iterations&#39;]</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">tensor(0.0361)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="o">**</span><span class="mi">8</span><span class="p">,</span><span class="n">d</span><span class="p">),</span><span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcov</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcov</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([128, 256])</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pcov</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcov</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([128, 128])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="p">(</span><span class="n">pcov</span><span class="o">.</span><span class="n">diagonal</span><span class="p">()</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pvar</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_var</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pvar</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([128])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">pcov</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(),</span><span class="n">pvar</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pmean</span><span class="p">,</span><span class="n">pstd</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="n">ci_low</span><span class="p">,</span><span class="n">ci_high</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_ci</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">confidence</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ci_low</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([128])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ci_high</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([128])</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">fgp</span><span class="o">.</span><span class="n">post_cubature_mean</span><span class="p">()</span>
<span class="go">tensor(20.1842)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fgp</span><span class="o">.</span><span class="n">post_cubature_var</span><span class="p">()</span>
<span class="go">tensor(3.1129e-06)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pcmean</span><span class="p">,</span><span class="n">pcvar</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="n">pcci_low</span><span class="p">,</span><span class="n">pcci_high</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_cubature_ci</span><span class="p">(</span><span class="n">confidence</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcci_low</span>
<span class="go">tensor(20.1797)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcci_high</span>
<span class="go">tensor(20.1888)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pcov_future</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pvar_future</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcvar_future</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_cubature_var</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">x_next</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">get_x_next</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_next</span> <span class="o">=</span> <span class="n">f_ackley</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fgp</span><span class="o">.</span><span class="n">add_y_next</span><span class="p">(</span><span class="n">y_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">tensor(0.0304)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">),</span><span class="n">pcov_future</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_var</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">pvar_future</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_cubature_var</span><span class="p">(),</span><span class="n">pcvar_future</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">tensor(0.0274)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">x_next</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">get_x_next</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_next</span> <span class="o">=</span> <span class="n">f_ackley</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fgp</span><span class="o">.</span><span class="n">add_y_next</span><span class="p">(</span><span class="n">y_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">tensor(0.0277)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">tensor(0.0276)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pcov_16n</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="mi">16</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pvar_16n</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="mi">16</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcvar_16n</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_cubature_var</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">16</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_next</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">get_x_next</span><span class="p">(</span><span class="mi">16</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_next</span> <span class="o">=</span> <span class="n">f_ackley</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fgp</span><span class="o">.</span><span class="n">add_y_next</span><span class="p">(</span><span class="n">y_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">),</span><span class="n">pcov_16n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_var</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">pvar_16n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_cubature_var</span><span class="p">(),</span><span class="n">pcvar_16n</span><span class="p">)</span>
</code></pre></div>



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>seqs</code>
            </td>
            <td>
                  <code>[<span title="int">int</span>, <span title="qmcpy.Lattice">Lattice</span>, <span title="List">List</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of lattice sequence generators
with order="NATURAL" and randomize in <code>["FALSE","SHIFT"]</code>. If an int <code>d</code> is passed in we use 
<div class="highlight"><pre><span></span><code><span class="p">[</span><span class="n">qmcpy</span><span class="o">.</span><span class="n">Lattice</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span><span class="n">randomize</span><span class="o">=</span><span class="s2">&quot;SHIFT&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">SeedSequence</span><span class="p">(</span><span class="n">seed_for_seq</span><span class="p">)</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">)]</span>
</code></pre></div>
See the <a href="https://qmcpy.readthedocs.io/en/latest/algorithms.html#module-qmcpy.discrete_distribution.lattice.lattice" target="_blank"><code>qmcpy.Lattice</code> docs</a> for more info</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_tasks</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of tasks</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>seed_for_seq</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>seed used for lattice randomization</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>alpha</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>smoothness parameter</p>
              </div>
            </td>
            <td>
                  <code>2</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>scale</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>kernel global scaling parameter</p>
              </div>
            </td>
            <td>
                  <code>1.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lengthscales</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>[<span title="d">d</span>], <span title="float">float</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>vector of kernel lengthscales. 
If a scalar is passed in then <code>lengthscales</code> is set to a constant vector. </p>
              </div>
            </td>
            <td>
                  <code>1.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>noise</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>positive noise variance i.e. nugget term</p>
              </div>
            </td>
            <td>
                  <code>1e-08</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>factor_task_kernel</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="Tensor">Tensor</span>[<span title="fastgps.fast_gp_lattice.FastGPLattice(num_tasks)">num_tasks</span>, <span title="fastgps.fast_gp_lattice.FastGPLattice(rank_factor_task_kernel)">rank_factor_task_kernel</span>], <span title="int">int</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>for <span class="arithmatex">\(F\)</span> the <code>factor_task_kernel</code> the task kernel is <span class="arithmatex">\(FF^T + \text{diag}(\boldsymbol{v})\)</span> 
where <code>rank_factor_task_kernel&lt;=num_tasks</code> and <span class="arithmatex">\(\boldsymbol{v}\)</span> is the <code>noise_task_kernel</code>.</p>
              </div>
            </td>
            <td>
                  <code>1.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>rank_factor_task_kernel</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>see the description of <code>factor_task_kernel</code> above. Defaults to 0 for single task problems and 1 for multi task problems.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>noise_task_kernel</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>[<span title="fastgps.fast_gp_lattice.FastGPLattice(num_tasks)">num_tasks</span>], <span title="float">float</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>see the description of <code>factor_task_kernel</code> above</p>
              </div>
            </td>
            <td>
                  <code>1.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code><span title="torch.device">device</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>torch device which is required to support <code>torch.float64</code></p>
              </div>
            </td>
            <td>
                  <code>&#39;cpu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tfs_scale</code>
            </td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span>[<span title="callable">callable</span>, <span title="callable">callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the first argument transforms to the raw value to be optimized, the second applies the inverse transform</p>
              </div>
            </td>
            <td>
                  <code>(lambda x: <span title="torch.log">log</span>(<span title="x">x</span>), lambda x: <span title="torch.exp">exp</span>(<span title="x">x</span>))</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tfs_lengthscales</code>
            </td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span>[<span title="callable">callable</span>, <span title="callable">callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the first argument transforms to the raw value to be optimized, the second applies the inverse transform</p>
              </div>
            </td>
            <td>
                  <code>(lambda x: <span title="torch.log">log</span>(<span title="x">x</span>), lambda x: <span title="torch.exp">exp</span>(<span title="x">x</span>))</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tfs_noise</code>
            </td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span>[<span title="callable">callable</span>, <span title="callable">callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the first argument transforms to the raw value to be optimized, the second applies the inverse transform</p>
              </div>
            </td>
            <td>
                  <code>(lambda x: <span title="torch.log">log</span>(<span title="x">x</span>), lambda x: <span title="torch.exp">exp</span>(<span title="x">x</span>))</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tfs_factor_task_kernel</code>
            </td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span>[<span title="callable">callable</span>, <span title="callable">callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the first argument transforms to the raw value to be optimized, the second applies the inverse transform</p>
              </div>
            </td>
            <td>
                  <code>(lambda x: <span title="x">x</span>, lambda x: <span title="x">x</span>)</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tfs_noise_task_kernel</code>
            </td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span>[<span title="callable">callable</span>, <span title="callable">callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the first argument transforms to the raw value to be optimized, the second applies the inverse transform</p>
              </div>
            </td>
            <td>
                  <code>(lambda x: <span title="torch.log">log</span>(<span title="x">x</span>), lambda x: <span title="torch.exp">exp</span>(<span title="x">x</span>))</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>requires_grad_scale</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>wheather or not to optimize the scale parameter</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>requires_grad_lengthscales</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>wheather or not to optimize lengthscale parameters</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>requires_grad_noise</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>wheather or not to optimize the noise parameter</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>requires_grad_factor_task_kernel</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>wheather or not to optimize the factor for the task kernel</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>requires_grad_noise_task_kernel</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>wheather or not to optimize the noise for the task kernel</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shape_batch</code>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>shape of the batch output for each task</p>
              </div>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span>([])</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shape_scale</code>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>shape of the scale parameter, defaults to <code>torch.Size([1])</code></p>
              </div>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span>([1])</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shape_lengthscales</code>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>shape of the lengthscales parameter, defaults to <code>torch.Size([d])</code> where <code>d</code> is the dimension</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shape_noise</code>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>shape of the noise parameter, defaults to <code>torch.Size([1])</code></p>
              </div>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span>([1])</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shape_factor_task_kernel</code>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>shape of the factor for the task kernel, defaults to <code>torch.Size([num_tasks,r])</code> where <code>r</code> is the rank, see the description of <code>factor_task_kernel</code></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shape_noise_task_kernel</code>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>shape of the noise for the task kernel, defaults to <code>torch.Size([num_tasks])</code></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>derivatives</code>
            </td>
            <td>
                  <code><span title="list">list</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of derivative orders e.g. to include a function and its gradient set 
<div class="highlight"><pre><span></span><code><span class="n">derivatives</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)]</span><span class="o">+</span><span class="p">[</span><span class="n">ej</span> <span class="k">for</span> <span class="n">ej</span> <span class="ow">in</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)]</span>
</code></pre></div></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>derivatives_coeffs</code>
            </td>
            <td>
                  <code><span title="list">list</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of derivative coefficients where if <code>derivatives[k].shape==(p,d)</code> then we should have <code>derivatives_coeffs[k].shape==(p,)</code></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>compile_fts</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if <code>True</code>, use <code>torch.compile(qmcpy.fftbr_torch,**compile_fts)</code> and <code>torch.compile(qmcpy.ifftbr_torch,**compile_fts)</code>, otherwise use the uncompiled versions</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>compile_fts_kwargs</code>
            </td>
            <td>
                  <code><span title="dict">dict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>keyword arguments to <code>torch.compile</code>, see the <code>compile_fts argument</code></p>
              </div>
            </td>
            <td>
                  <code>{}</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adaptive_nugget</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True, use the adaptive nugget which modifies noises based on trace ratios.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
      </tbody>
    </table>







                  <details class="quote">
                    <summary>Source code in <code>fastgps/fast_gp_lattice.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">seqs</span><span class="p">:</span><span class="n">qmcpy</span><span class="o">.</span><span class="n">Lattice</span><span class="p">,</span>
        <span class="n">num_tasks</span><span class="p">:</span><span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">seed_for_seq</span><span class="p">:</span><span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span><span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span><span class="nb">float</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> 
        <span class="n">lengthscales</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> 
        <span class="n">noise</span><span class="p">:</span><span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span> 
        <span class="n">factor_task_kernel</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> 
        <span class="n">rank_factor_task_kernel</span><span class="p">:</span><span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">noise_task_kernel</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">tfs_scale</span><span class="p">:</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">callable</span><span class="p">,</span><span class="nb">callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)),(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))),</span>
        <span class="n">tfs_lengthscales</span><span class="p">:</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">callable</span><span class="p">,</span><span class="nb">callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)),(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))),</span>
        <span class="n">tfs_noise</span><span class="p">:</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">callable</span><span class="p">,</span><span class="nb">callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)),(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))),</span>
        <span class="n">tfs_factor_task_kernel</span><span class="p">:</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">callable</span><span class="p">,</span><span class="nb">callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)),</span><span class="c1">#((lambda x: x**(1/3)),(lambda x: x**3)),</span>
        <span class="n">tfs_noise_task_kernel</span><span class="p">:</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">callable</span><span class="p">,</span><span class="nb">callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)),(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))),</span>
        <span class="n">requires_grad_scale</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> 
        <span class="n">requires_grad_lengthscales</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> 
        <span class="n">requires_grad_noise</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> 
        <span class="n">requires_grad_factor_task_kernel</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">requires_grad_noise_task_kernel</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shape_batch</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([]),</span>
        <span class="n">shape_scale</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span> 
        <span class="n">shape_lengthscales</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shape_noise</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span>
        <span class="n">shape_factor_task_kernel</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
        <span class="n">shape_noise_task_kernel</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">derivatives</span><span class="p">:</span><span class="nb">list</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">derivatives_coeffs</span><span class="p">:</span><span class="nb">list</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">compile_fts</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">compile_fts_kwargs</span><span class="p">:</span><span class="nb">dict</span> <span class="o">=</span> <span class="p">{},</span>
        <span class="n">adaptive_nugget</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        seqs ([int,qmcpy.Lattice,List]): list of lattice sequence generators</span>
<span class="sd">            with order=&quot;NATURAL&quot; and randomize in `[&quot;FALSE&quot;,&quot;SHIFT&quot;]`. If an int `d` is passed in we use </span>
<span class="sd">            ```python</span>
<span class="sd">            [qmcpy.Lattice(d,seed=seed,randomize=&quot;SHIFT&quot;) for seed in np.random.SeedSequence(seed_for_seq).spawn(num_tasks)]</span>
<span class="sd">            ```</span>
<span class="sd">            See the &lt;a href=&quot;https://qmcpy.readthedocs.io/en/latest/algorithms.html#module-qmcpy.discrete_distribution.lattice.lattice&quot; target=&quot;_blank&quot;&gt;`qmcpy.Lattice` docs&lt;/a&gt; for more info</span>
<span class="sd">        num_tasks (int): number of tasks</span>
<span class="sd">        seed_for_seq (int): seed used for lattice randomization</span>
<span class="sd">        alpha (int): smoothness parameter</span>
<span class="sd">        scale (float): kernel global scaling parameter</span>
<span class="sd">        lengthscales (Union[torch.Tensor[d],float]): vector of kernel lengthscales. </span>
<span class="sd">            If a scalar is passed in then `lengthscales` is set to a constant vector. </span>
<span class="sd">        noise (float): positive noise variance i.e. nugget term</span>
<span class="sd">        factor_task_kernel (Union[Tensor[num_tasks,rank_factor_task_kernel],int]): for $F$ the `factor_task_kernel` the task kernel is $FF^T + \\text{diag}(\\boldsymbol{v})$ </span>
<span class="sd">            where `rank_factor_task_kernel&lt;=num_tasks` and $\\boldsymbol{v}$ is the `noise_task_kernel`.</span>
<span class="sd">        rank_factor_task_kernel (int): see the description of `factor_task_kernel` above. Defaults to 0 for single task problems and 1 for multi task problems.</span>
<span class="sd">        noise_task_kernel (Union[torch.Tensor[num_tasks],float]): see the description of `factor_task_kernel` above</span>
<span class="sd">        device (torch.device): torch device which is required to support `torch.float64`</span>
<span class="sd">        tfs_scale (Tuple[callable,callable]): the first argument transforms to the raw value to be optimized, the second applies the inverse transform</span>
<span class="sd">        tfs_lengthscales (Tuple[callable,callable]): the first argument transforms to the raw value to be optimized, the second applies the inverse transform</span>
<span class="sd">        tfs_noise (Tuple[callable,callable]): the first argument transforms to the raw value to be optimized, the second applies the inverse transform</span>
<span class="sd">        tfs_factor_task_kernel (Tuple[callable,callable]): the first argument transforms to the raw value to be optimized, the second applies the inverse transform</span>
<span class="sd">        tfs_noise_task_kernel (Tuple[callable,callable]): the first argument transforms to the raw value to be optimized, the second applies the inverse transform</span>
<span class="sd">        requires_grad_scale (bool): wheather or not to optimize the scale parameter</span>
<span class="sd">        requires_grad_lengthscales (bool): wheather or not to optimize lengthscale parameters</span>
<span class="sd">        requires_grad_noise (bool): wheather or not to optimize the noise parameter</span>
<span class="sd">        requires_grad_factor_task_kernel (bool): wheather or not to optimize the factor for the task kernel</span>
<span class="sd">        requires_grad_noise_task_kernel (bool): wheather or not to optimize the noise for the task kernel</span>
<span class="sd">        shape_batch (torch.Size): shape of the batch output for each task</span>
<span class="sd">        shape_scale (torch.Size): shape of the scale parameter, defaults to `torch.Size([1])`</span>
<span class="sd">        shape_lengthscales (torch.Size): shape of the lengthscales parameter, defaults to `torch.Size([d])` where `d` is the dimension</span>
<span class="sd">        shape_noise (torch.Size): shape of the noise parameter, defaults to `torch.Size([1])`</span>
<span class="sd">        shape_factor_task_kernel (torch.Size): shape of the factor for the task kernel, defaults to `torch.Size([num_tasks,r])` where `r` is the rank, see the description of `factor_task_kernel`</span>
<span class="sd">        shape_noise_task_kernel (torch.Size): shape of the noise for the task kernel, defaults to `torch.Size([num_tasks])`</span>
<span class="sd">        derivatives (list): list of derivative orders e.g. to include a function and its gradient set </span>
<span class="sd">            ```python</span>
<span class="sd">            derivatives = [torch.zeros(d,dtype=int)]+[ej for ej in torch.eye(d,dtype=int)]</span>
<span class="sd">            ```</span>
<span class="sd">        derivatives_coeffs (list): list of derivative coefficients where if `derivatives[k].shape==(p,d)` then we should have `derivatives_coeffs[k].shape==(p,)`</span>
<span class="sd">        compile_fts (bool): if `True`, use `torch.compile(qmcpy.fftbr_torch,**compile_fts)` and `torch.compile(qmcpy.ifftbr_torch,**compile_fts)`, otherwise use the uncompiled versions</span>
<span class="sd">        compile_fts_kwargs (dict): keyword arguments to `torch.compile`, see the `compile_fts argument`</span>
<span class="sd">        adaptive_nugget (bool): if True, use the adaptive nugget which modifies noises based on trace ratios.  </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">qmcpy</span><span class="o">.</span><span class="n">kernel_methods</span><span class="o">.</span><span class="n">shift_invar_ops</span><span class="o">.</span><span class="n">BERNOULLIPOLYSDICT</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="s2">&quot;alpha must be in </span><span class="si">%s</span><span class="s2">&quot;</span><span class="o">%</span><span class="nb">list</span><span class="p">(</span><span class="n">qmcpy</span><span class="o">.</span><span class="n">kernel_methods</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">shift_invar_ops</span><span class="o">.</span><span class="n">BERNOULLIPOLYSDICT</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">num_tasks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> 
        <span class="n">solo_task</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">default_task</span> <span class="o">=</span> <span class="mi">0</span> 
        <span class="n">num_tasks</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">num_tasks</span><span class="o">&gt;</span><span class="mi">0</span>
        <span class="n">solo_task</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">default_task</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="nb">int</span><span class="p">):</span>
        <span class="n">seqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">qmcpy</span><span class="o">.</span><span class="n">Lattice</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span><span class="n">randomize</span><span class="o">=</span><span class="s2">&quot;SHIFT&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">SeedSequence</span><span class="p">(</span><span class="n">seed_for_seq</span><span class="p">)</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">)],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="n">qmcpy</span><span class="o">.</span><span class="n">Lattice</span><span class="p">):</span>
        <span class="n">seqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">seqs</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="nb">list</span><span class="p">):</span>
        <span class="n">seqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">seqs</span><span class="o">.</span><span class="n">shape</span><span class="o">==</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">,),</span> <span class="s2">&quot;seqs should be a length num_tasks=</span><span class="si">%d</span><span class="s2"> list&quot;</span><span class="o">%</span><span class="n">num_tasks</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">seqs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">qmcpy</span><span class="o">.</span><span class="n">Lattice</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">)),</span> <span class="s2">&quot;each seq should be a qmcpy.Lattice instances&quot;</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">seqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">order</span><span class="o">==</span><span class="s2">&quot;NATURAL&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">)),</span> <span class="s2">&quot;each seq should be in &#39;NATURAL&#39; order &quot;</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">seqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">replications</span><span class="o">==</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">))</span> <span class="ow">and</span> <span class="s2">&quot;each seq should have only 1 replication&quot;</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">seqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">randomize</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;FALSE&#39;</span><span class="p">,</span><span class="s1">&#39;SHIFT&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">)),</span> <span class="s2">&quot;each seq should have randomize in [&#39;FALSE&#39;,&#39;SHIFT&#39;]&quot;</span>
    <span class="n">ft</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">qmcpy</span><span class="o">.</span><span class="n">fftbr_torch</span><span class="p">,</span><span class="o">**</span><span class="n">compile_fts_kwargs</span><span class="p">)</span> <span class="k">if</span> <span class="n">compile_fts</span> <span class="k">else</span> <span class="n">qmcpy</span><span class="o">.</span><span class="n">fftbr_torch</span>
    <span class="n">ift</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">qmcpy</span><span class="o">.</span><span class="n">ifftbr_torch</span><span class="p">,</span><span class="o">**</span><span class="n">compile_fts_kwargs</span><span class="p">)</span> <span class="k">if</span> <span class="n">compile_fts</span> <span class="k">else</span> <span class="n">qmcpy</span><span class="o">.</span><span class="n">ifftbr_torch</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">alpha</span><span class="p">,</span>
        <span class="n">ft</span><span class="p">,</span>
        <span class="n">ift</span><span class="p">,</span>
        <span class="n">seqs</span><span class="p">,</span>
        <span class="n">num_tasks</span><span class="p">,</span>
        <span class="n">default_task</span><span class="p">,</span>
        <span class="n">solo_task</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">lengthscales</span><span class="p">,</span>
        <span class="n">noise</span><span class="p">,</span>
        <span class="n">factor_task_kernel</span><span class="p">,</span>
        <span class="n">rank_factor_task_kernel</span><span class="p">,</span>
        <span class="n">noise_task_kernel</span><span class="p">,</span>
        <span class="n">device</span><span class="p">,</span>
        <span class="n">tfs_scale</span><span class="p">,</span>
        <span class="n">tfs_lengthscales</span><span class="p">,</span>
        <span class="n">tfs_noise</span><span class="p">,</span>
        <span class="n">tfs_factor_task_kernel</span><span class="p">,</span>
        <span class="n">tfs_noise_task_kernel</span><span class="p">,</span>
        <span class="n">requires_grad_scale</span><span class="p">,</span>
        <span class="n">requires_grad_lengthscales</span><span class="p">,</span>
        <span class="n">requires_grad_noise</span><span class="p">,</span>
        <span class="n">requires_grad_factor_task_kernel</span><span class="p">,</span>
        <span class="n">requires_grad_noise_task_kernel</span><span class="p">,</span>
        <span class="n">shape_batch</span><span class="p">,</span>
        <span class="n">shape_scale</span><span class="p">,</span> 
        <span class="n">shape_lengthscales</span><span class="p">,</span>
        <span class="n">shape_noise</span><span class="p">,</span>
        <span class="n">shape_factor_task_kernel</span><span class="p">,</span> 
        <span class="n">shape_noise_task_kernel</span><span class="p">,</span>
        <span class="n">derivatives</span><span class="p">,</span>
        <span class="n">derivatives_coeffs</span><span class="p">,</span>
        <span class="n">adaptive_nugget</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">




    <div class="doc doc-contents first">









  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="fastgps.fast_gp_digital_net_b2.FastGPDigitalNetB2" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">FastGPDigitalNetB2</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">FastGPDigitalNetB2</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span> <span class="n">num_tasks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed_for_seq</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">lengthscales</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">1e-16</span><span class="p">,</span> <span class="n">factor_task_kernel</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">rank_factor_task_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">noise_task_kernel</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">tfs_scale</span><span class="o">=</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">tfs_lengthscales</span><span class="o">=</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">tfs_noise</span><span class="o">=</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">tfs_factor_task_kernel</span><span class="o">=</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">),</span> <span class="n">tfs_noise_task_kernel</span><span class="o">=</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">requires_grad_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">requires_grad_lengthscales</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">requires_grad_noise</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">requires_grad_factor_task_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">requires_grad_noise_task_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape_batch</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([]),</span> <span class="n">shape_scale</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span> <span class="n">shape_lengthscales</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape_noise</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span> <span class="n">shape_factor_task_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape_noise_task_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">derivatives</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">derivatives_coeffs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">compile_fts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">compile_fts_kwargs</span><span class="o">=</span><span class="p">{},</span> <span class="n">adaptive_nugget</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="AbstractFastGP (fastgps.abstract_fast_gp.AbstractFastGP)" href="#fastgps.abstract_fast_gp.AbstractFastGP">AbstractFastGP</a></code></p>


        <p>Fast Gaussian process regression using digitally shifted digital nets paired with digitally shift invariant kernels</p>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">set_default_dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">f_ackley</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">scaling</span><span class="o">=</span><span class="mf">32.768</span><span class="p">):</span>
<span class="gp">... </span>    <span class="c1"># https://www.sfu.ca/~ssurjano/ackley.html</span>
<span class="gp">... </span>    <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">2</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">scaling</span><span class="o">*</span><span class="n">x</span><span class="o">-</span><span class="n">scaling</span>
<span class="gp">... </span>    <span class="n">t1</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span>
<span class="gp">... </span>    <span class="n">t2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">c</span><span class="o">*</span><span class="n">x</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span>
<span class="gp">... </span>    <span class="n">t3</span> <span class="o">=</span> <span class="n">a</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">y</span> <span class="o">=</span> <span class="o">-</span><span class="n">t1</span><span class="o">-</span><span class="n">t2</span><span class="o">+</span><span class="n">t3</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">y</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">n</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fgp</span> <span class="o">=</span> <span class="n">FastGPDigitalNetB2</span><span class="p">(</span><span class="n">qmcpy</span><span class="o">.</span><span class="n">DigitalNetB2</span><span class="p">(</span><span class="n">dimension</span><span class="o">=</span><span class="n">d</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_next</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">get_x_next</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_next</span> <span class="o">=</span> <span class="n">f_ackley</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fgp</span><span class="o">.</span><span class="n">add_y_next</span><span class="p">(</span><span class="n">y_next</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">17</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="o">**</span><span class="mi">7</span><span class="p">,</span><span class="n">d</span><span class="p">),</span><span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">f_ackley</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pmean</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pmean</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([128])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">pmean</span><span class="p">)</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">tensor(0.0284)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">fgp</span><span class="o">.</span><span class="n">x</span><span class="p">),</span><span class="n">fgp</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="go">[&#39;iterations&#39;]</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">tensor(0.0287)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="o">**</span><span class="mi">8</span><span class="p">,</span><span class="n">d</span><span class="p">),</span><span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcov</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcov</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([128, 256])</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pcov</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcov</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([128, 128])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="p">(</span><span class="n">pcov</span><span class="o">.</span><span class="n">diagonal</span><span class="p">()</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pvar</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_var</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pvar</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([128])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">pcov</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(),</span><span class="n">pvar</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pmean</span><span class="p">,</span><span class="n">pstd</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="n">ci_low</span><span class="p">,</span><span class="n">ci_high</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_ci</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">confidence</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ci_low</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([128])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ci_high</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([128])</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">fgp</span><span class="o">.</span><span class="n">post_cubature_mean</span><span class="p">()</span>
<span class="go">tensor(20.1888)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fgp</span><span class="o">.</span><span class="n">post_cubature_var</span><span class="p">()</span>
<span class="go">tensor(0.0002)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pcmean</span><span class="p">,</span><span class="n">pcvar</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="n">pcci_low</span><span class="p">,</span><span class="n">pcci_high</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_cubature_ci</span><span class="p">(</span><span class="n">confidence</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcci_low</span>
<span class="go">tensor(20.1557)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcci_high</span>
<span class="go">tensor(20.2220)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pcov_future</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pvar_future</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcvar_future</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_cubature_var</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">x_next</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">get_x_next</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_next</span> <span class="o">=</span> <span class="n">f_ackley</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fgp</span><span class="o">.</span><span class="n">add_y_next</span><span class="p">(</span><span class="n">y_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">tensor(0.0271)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">),</span><span class="n">pcov_future</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_var</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">pvar_future</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_cubature_var</span><span class="p">(),</span><span class="n">pcvar_future</span><span class="p">)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">tensor(0.0273)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">x_next</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">get_x_next</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_next</span> <span class="o">=</span> <span class="n">f_ackley</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fgp</span><span class="o">.</span><span class="n">add_y_next</span><span class="p">(</span><span class="n">y_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">tensor(0.0200)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">tensor(0.0194)</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pcov_16n</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="mi">16</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pvar_16n</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="mi">16</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pcvar_16n</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">post_cubature_var</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">16</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_next</span> <span class="o">=</span> <span class="n">fgp</span><span class="o">.</span><span class="n">get_x_next</span><span class="p">(</span><span class="mi">16</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_next</span> <span class="o">=</span> <span class="n">f_ackley</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fgp</span><span class="o">.</span><span class="n">add_y_next</span><span class="p">(</span><span class="n">y_next</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">),</span><span class="n">pcov_16n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_var</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">pvar_16n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">fgp</span><span class="o">.</span><span class="n">post_cubature_var</span><span class="p">(),</span><span class="n">pcvar_16n</span><span class="p">)</span>
</code></pre></div>



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>seqs</code>
            </td>
            <td>
                  <code>Union[int,qmcpy.DigitalNetB2,List]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of digital sequence generators in base <span class="arithmatex">\(b=2\)</span> 
with order="NATURAL" and randomize in <code>["FALSE","DS"]</code>. If an int <code>d</code> is passed in we use 
<div class="highlight"><pre><span></span><code><span class="p">[</span><span class="n">qmcpy</span><span class="o">.</span><span class="n">DigitalNetB2</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span><span class="n">randomize</span><span class="o">=</span><span class="s2">&quot;DS&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">SeedSequence</span><span class="p">(</span><span class="n">seed_for_seq</span><span class="p">)</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">)]</span>
</code></pre></div>
See the <a href="https://qmcpy.readthedocs.io/en/latest/algorithms.html#module-qmcpy.discrete_distribution.digital_net_b2.digital_net_b2" target="_blank"><code>qmcpy.DigitalNetB2</code> docs</a> for more info. 
If <code>num_tasks==1</code> then randomize may be in <code>["FALSE","DS","LMS","LMS_DS"]</code>. </p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_tasks</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of tasks </p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>seed_for_seq</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>seed used for digital net randomization</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>alpha</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>smoothness parameter</p>
              </div>
            </td>
            <td>
                  <code>2</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>scale</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>kernel global scaling parameter</p>
              </div>
            </td>
            <td>
                  <code>1.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lengthscales</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>[<span title="d">d</span>], <span title="float">float</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>vector of kernel lengthscales. 
If a scalar is passed in then <code>lengthscales</code> is set to a constant vector. </p>
              </div>
            </td>
            <td>
                  <code>1.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>noise</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>positive noise variance i.e. nugget term</p>
              </div>
            </td>
            <td>
                  <code>1e-16</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>factor_task_kernel</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="Tensor">Tensor</span>[<span title="fastgps.fast_gp_digital_net_b2.FastGPDigitalNetB2(num_tasks)">num_tasks</span>, <span title="fastgps.fast_gp_digital_net_b2.FastGPDigitalNetB2(rank_factor_task_kernel)">rank_factor_task_kernel</span>], <span title="int">int</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>for <span class="arithmatex">\(F\)</span> the <code>factor_task_kernel</code> the task kernel is <span class="arithmatex">\(FF^T + \text{diag}(\boldsymbol{v})\)</span> 
where <code>rank_factor_task_kernel&lt;=num_tasks</code> and <span class="arithmatex">\(\boldsymbol{v}\)</span> is the <code>noise_task_kernel</code>.</p>
              </div>
            </td>
            <td>
                  <code>1.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>rank_factor_task_kernel</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>see the description of <code>factor_task_kernel</code> above. Defaults to 0 for single task problems and 1 for multi task problems.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>noise_task_kernel</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>[<span title="fastgps.fast_gp_digital_net_b2.FastGPDigitalNetB2(num_tasks)">num_tasks</span>], <span title="float">float</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>see the description of <code>factor_task_kernel</code> above </p>
              </div>
            </td>
            <td>
                  <code>1.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code><span title="torch.device">device</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>torch device which is required to support <code>torch.float64</code></p>
              </div>
            </td>
            <td>
                  <code>&#39;cpu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tfs_scale</code>
            </td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span>[<span title="callable">callable</span>, <span title="callable">callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the first argument transforms to the raw value to be optimized, the second applies the inverse transform</p>
              </div>
            </td>
            <td>
                  <code>(lambda x: <span title="torch.log">log</span>(<span title="x">x</span>), lambda x: <span title="torch.exp">exp</span>(<span title="x">x</span>))</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tfs_lengthscales</code>
            </td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span>[<span title="callable">callable</span>, <span title="callable">callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the first argument transforms to the raw value to be optimized, the second applies the inverse transform</p>
              </div>
            </td>
            <td>
                  <code>(lambda x: <span title="torch.log">log</span>(<span title="x">x</span>), lambda x: <span title="torch.exp">exp</span>(<span title="x">x</span>))</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tfs_noise</code>
            </td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span>[<span title="callable">callable</span>, <span title="callable">callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the first argument transforms to the raw value to be optimized, the second applies the inverse transform</p>
              </div>
            </td>
            <td>
                  <code>(lambda x: <span title="torch.log">log</span>(<span title="x">x</span>), lambda x: <span title="torch.exp">exp</span>(<span title="x">x</span>))</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tfs_factor_task_kernel</code>
            </td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span>[<span title="callable">callable</span>, <span title="callable">callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the first argument transforms to the raw value to be optimized, the second applies the inverse transform</p>
              </div>
            </td>
            <td>
                  <code>(lambda x: <span title="x">x</span>, lambda x: <span title="x">x</span>)</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tfs_noise_task_kernel</code>
            </td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span>[<span title="callable">callable</span>, <span title="callable">callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the first argument transforms to the raw value to be optimized, the second applies the inverse transform</p>
              </div>
            </td>
            <td>
                  <code>(lambda x: <span title="torch.log">log</span>(<span title="x">x</span>), lambda x: <span title="torch.exp">exp</span>(<span title="x">x</span>))</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>requires_grad_scale</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>wheather or not to optimize the scale parameter</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>requires_grad_lengthscales</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>wheather or not to optimize lengthscale parameters</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>requires_grad_noise</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>wheather or not to optimize the noise parameter</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>requires_grad_factor_task_kernel</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>wheather or not to optimize the factor for the task kernel</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>requires_grad_noise_task_kernel</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>wheather or not to optimize the noise for the task kernel</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shape_batch</code>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>shape of the batch output for each task</p>
              </div>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span>([])</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shape_scale</code>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>shape of the scale parameter, defaults to <code>torch.Size([1])</code></p>
              </div>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span>([1])</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shape_lengthscales</code>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>shape of the lengthscales parameter, defaults to <code>torch.Size([d])</code> where <code>d</code> is the dimension</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shape_noise</code>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>shape of the noise parameter, defaults to <code>torch.Size([1])</code></p>
              </div>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span>([1])</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shape_factor_task_kernel</code>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>shape of the factor for the task kernel, defaults to <code>torch.Size([num_tasks,r])</code> where <code>r</code> is the rank, see the description of <code>factor_task_kernel</code></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shape_noise_task_kernel</code>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>shape of the noise for the task kernel, defaults to <code>torch.Size([num_tasks])</code></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>derivatives</code>
            </td>
            <td>
                  <code><span title="list">list</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of derivative orders e.g. to include a function and its gradient set 
<div class="highlight"><pre><span></span><code><span class="n">derivatives</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)]</span><span class="o">+</span><span class="p">[</span><span class="n">ej</span> <span class="k">for</span> <span class="n">ej</span> <span class="ow">in</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)]</span>
</code></pre></div></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>derivatives_coeffs</code>
            </td>
            <td>
                  <code><span title="list">list</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of derivative coefficients where if <code>derivatives[k].shape==(p,d)</code> then we should have <code>derivatives_coeffs[k].shape==(p,)</code></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>compile_fts</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if <code>True</code>, use <code>torch.compile(qmcpy.fwht_torch,**compile_fts_kwargs)</code>, otherwise use the uncompiled version</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>compile_fts_kwargs</code>
            </td>
            <td>
                  <code><span title="dict">dict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>keyword arguments to <code>torch.compile</code>, see the <code>compile_fts</code> argument</p>
              </div>
            </td>
            <td>
                  <code>{}</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adaptive_nugget</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True, use the adaptive nugget which modifies noises based on trace ratios.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
      </tbody>
    </table>







                  <details class="quote">
                    <summary>Source code in <code>fastgps/fast_gp_digital_net_b2.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">seqs</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="n">qmcpy</span><span class="o">.</span><span class="n">DigitalNetB2</span><span class="p">,</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">num_tasks</span><span class="p">:</span><span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">seed_for_seq</span><span class="p">:</span><span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span><span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span><span class="nb">float</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> 
        <span class="n">lengthscales</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> 
        <span class="n">noise</span><span class="p">:</span><span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-16</span><span class="p">,</span>
        <span class="n">factor_task_kernel</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span>
        <span class="n">rank_factor_task_kernel</span><span class="p">:</span><span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">noise_task_kernel</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">tfs_scale</span><span class="p">:</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">callable</span><span class="p">,</span><span class="nb">callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)),(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))),</span>
        <span class="n">tfs_lengthscales</span><span class="p">:</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">callable</span><span class="p">,</span><span class="nb">callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)),(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))),</span>
        <span class="n">tfs_noise</span><span class="p">:</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">callable</span><span class="p">,</span><span class="nb">callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)),(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))),</span>
        <span class="n">tfs_factor_task_kernel</span><span class="p">:</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">callable</span><span class="p">,</span><span class="nb">callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)),</span><span class="c1">#((lambda x: x**(1/3)),(lambda x: x**3)),</span>
        <span class="n">tfs_noise_task_kernel</span><span class="p">:</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">callable</span><span class="p">,</span><span class="nb">callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)),(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))),</span>
        <span class="n">requires_grad_scale</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> 
        <span class="n">requires_grad_lengthscales</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> 
        <span class="n">requires_grad_noise</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> 
        <span class="n">requires_grad_factor_task_kernel</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">requires_grad_noise_task_kernel</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shape_batch</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([]),</span>
        <span class="n">shape_scale</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span> 
        <span class="n">shape_lengthscales</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shape_noise</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span>
        <span class="n">shape_factor_task_kernel</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
        <span class="n">shape_noise_task_kernel</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">derivatives</span><span class="p">:</span><span class="nb">list</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">derivatives_coeffs</span><span class="p">:</span><span class="nb">list</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">compile_fts</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">compile_fts_kwargs</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{},</span>
        <span class="n">adaptive_nugget</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        seqs (Union[int,qmcpy.DigitalNetB2,List]]): list of digital sequence generators in base $b=2$ </span>
<span class="sd">            with order=&quot;NATURAL&quot; and randomize in `[&quot;FALSE&quot;,&quot;DS&quot;]`. If an int `d` is passed in we use </span>
<span class="sd">            ```python</span>
<span class="sd">            [qmcpy.DigitalNetB2(d,seed=seed,randomize=&quot;DS&quot;) for seed in np.random.SeedSequence(seed_for_seq).spawn(num_tasks)]</span>
<span class="sd">            ```</span>
<span class="sd">            See the &lt;a href=&quot;https://qmcpy.readthedocs.io/en/latest/algorithms.html#module-qmcpy.discrete_distribution.digital_net_b2.digital_net_b2&quot; target=&quot;_blank&quot;&gt;`qmcpy.DigitalNetB2` docs&lt;/a&gt; for more info. </span>
<span class="sd">            If `num_tasks==1` then randomize may be in `[&quot;FALSE&quot;,&quot;DS&quot;,&quot;LMS&quot;,&quot;LMS_DS&quot;]`. </span>
<span class="sd">        num_tasks (int): number of tasks </span>
<span class="sd">        seed_for_seq (int): seed used for digital net randomization</span>
<span class="sd">        alpha (int): smoothness parameter</span>
<span class="sd">        scale (float): kernel global scaling parameter</span>
<span class="sd">        lengthscales (Union[torch.Tensor[d],float]): vector of kernel lengthscales. </span>
<span class="sd">            If a scalar is passed in then `lengthscales` is set to a constant vector. </span>
<span class="sd">        noise (float): positive noise variance i.e. nugget term</span>
<span class="sd">        factor_task_kernel (Union[Tensor[num_tasks,rank_factor_task_kernel],int]): for $F$ the `factor_task_kernel` the task kernel is $FF^T + \\text{diag}(\\boldsymbol{v})$ </span>
<span class="sd">            where `rank_factor_task_kernel&lt;=num_tasks` and $\\boldsymbol{v}$ is the `noise_task_kernel`.</span>
<span class="sd">        rank_factor_task_kernel (int): see the description of `factor_task_kernel` above. Defaults to 0 for single task problems and 1 for multi task problems.</span>
<span class="sd">        noise_task_kernel (Union[torch.Tensor[num_tasks],float]): see the description of `factor_task_kernel` above </span>
<span class="sd">        device (torch.device): torch device which is required to support `torch.float64`</span>
<span class="sd">        tfs_scale (Tuple[callable,callable]): the first argument transforms to the raw value to be optimized, the second applies the inverse transform</span>
<span class="sd">        tfs_lengthscales (Tuple[callable,callable]): the first argument transforms to the raw value to be optimized, the second applies the inverse transform</span>
<span class="sd">        tfs_noise (Tuple[callable,callable]): the first argument transforms to the raw value to be optimized, the second applies the inverse transform</span>
<span class="sd">        tfs_factor_task_kernel (Tuple[callable,callable]): the first argument transforms to the raw value to be optimized, the second applies the inverse transform</span>
<span class="sd">        tfs_noise_task_kernel (Tuple[callable,callable]): the first argument transforms to the raw value to be optimized, the second applies the inverse transform</span>
<span class="sd">        requires_grad_scale (bool): wheather or not to optimize the scale parameter</span>
<span class="sd">        requires_grad_lengthscales (bool): wheather or not to optimize lengthscale parameters</span>
<span class="sd">        requires_grad_noise (bool): wheather or not to optimize the noise parameter</span>
<span class="sd">        requires_grad_factor_task_kernel (bool): wheather or not to optimize the factor for the task kernel</span>
<span class="sd">        requires_grad_noise_task_kernel (bool): wheather or not to optimize the noise for the task kernel</span>
<span class="sd">        shape_batch (torch.Size): shape of the batch output for each task</span>
<span class="sd">        shape_scale (torch.Size): shape of the scale parameter, defaults to `torch.Size([1])`</span>
<span class="sd">        shape_lengthscales (torch.Size): shape of the lengthscales parameter, defaults to `torch.Size([d])` where `d` is the dimension</span>
<span class="sd">        shape_noise (torch.Size): shape of the noise parameter, defaults to `torch.Size([1])`</span>
<span class="sd">        shape_factor_task_kernel (torch.Size): shape of the factor for the task kernel, defaults to `torch.Size([num_tasks,r])` where `r` is the rank, see the description of `factor_task_kernel`</span>
<span class="sd">        shape_noise_task_kernel (torch.Size): shape of the noise for the task kernel, defaults to `torch.Size([num_tasks])`</span>
<span class="sd">        derivatives (list): list of derivative orders e.g. to include a function and its gradient set </span>
<span class="sd">            ```python</span>
<span class="sd">            derivatives = [torch.zeros(d,dtype=int)]+[ej for ej in torch.eye(d,dtype=int)]</span>
<span class="sd">            ```</span>
<span class="sd">        derivatives_coeffs (list): list of derivative coefficients where if `derivatives[k].shape==(p,d)` then we should have `derivatives_coeffs[k].shape==(p,)`</span>
<span class="sd">        compile_fts (bool): if `True`, use `torch.compile(qmcpy.fwht_torch,**compile_fts_kwargs)`, otherwise use the uncompiled version</span>
<span class="sd">        compile_fts_kwargs (dict): keyword arguments to `torch.compile`, see the `compile_fts` argument</span>
<span class="sd">        adaptive_nugget (bool): if True, use the adaptive nugget which modifies noises based on trace ratios.  </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">num_tasks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> 
        <span class="n">solo_task</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">default_task</span> <span class="o">=</span> <span class="mi">0</span> 
        <span class="n">num_tasks</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">num_tasks</span><span class="o">&gt;</span><span class="mi">0</span>
        <span class="n">solo_task</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">default_task</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="nb">int</span><span class="p">):</span>
        <span class="n">seqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">qmcpy</span><span class="o">.</span><span class="n">DigitalNetB2</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span><span class="n">randomize</span><span class="o">=</span><span class="s2">&quot;DS&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">SeedSequence</span><span class="p">(</span><span class="n">seed_for_seq</span><span class="p">)</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">)],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="n">qmcpy</span><span class="o">.</span><span class="n">DigitalNetB2</span><span class="p">):</span>
        <span class="n">seqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">seqs</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="nb">list</span><span class="p">):</span>
        <span class="n">seqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">seqs</span><span class="o">.</span><span class="n">shape</span><span class="o">==</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">,),</span> <span class="s2">&quot;seqs should be a length num_tasks=</span><span class="si">%d</span><span class="s2"> list&quot;</span><span class="o">%</span><span class="n">num_tasks</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">seqs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">qmcpy</span><span class="o">.</span><span class="n">DigitalNetB2</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">)),</span> <span class="s2">&quot;each seq should be a qmcpy.DigitalNetB2 instances&quot;</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">seqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">order</span><span class="o">==</span><span class="s2">&quot;NATURAL&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">)),</span> <span class="s2">&quot;each seq should be in &#39;NATURAL&#39; order &quot;</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">seqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">replications</span><span class="o">==</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">))</span> <span class="ow">and</span> <span class="s2">&quot;each seq should have only 1 replication&quot;</span>
    <span class="k">if</span> <span class="n">num_tasks</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">seqs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">randomize</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;FALSE&#39;</span><span class="p">,</span><span class="s1">&#39;DS&#39;</span><span class="p">,</span><span class="s1">&#39;LMS&#39;</span><span class="p">,</span><span class="s1">&#39;LMS_DS&#39;</span><span class="p">],</span> <span class="s2">&quot;seq should have randomize in [&#39;FALSE&#39;,&#39;DS&#39;,&#39;LMS&#39;,&#39;LMS_DS&#39;]&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">seqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">randomize</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;FALSE&#39;</span><span class="p">,</span><span class="s1">&#39;DS&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">)),</span> <span class="s2">&quot;each seq should have randomize in [&#39;FALSE&#39;,&#39;DS&#39;]&quot;</span>
    <span class="n">ts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">seqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">t</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">)])</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">ts</span><span class="o">&lt;</span><span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;each seq must have t&lt;64&quot;</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">ts</span><span class="o">==</span><span class="n">ts</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;all seqs should have the same t&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t</span> <span class="o">=</span> <span class="n">ts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">ift</span> <span class="o">=</span> <span class="n">ft</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">qmcpy</span><span class="o">.</span><span class="n">fwht_torch</span><span class="p">,</span><span class="o">**</span><span class="n">compile_fts_kwargs</span><span class="p">)</span> <span class="k">if</span> <span class="n">compile_fts</span> <span class="k">else</span> <span class="n">qmcpy</span><span class="o">.</span><span class="n">fwht_torch</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">alpha</span><span class="p">,</span>
        <span class="n">ft</span><span class="p">,</span>
        <span class="n">ift</span><span class="p">,</span>
        <span class="n">seqs</span><span class="p">,</span>
        <span class="n">num_tasks</span><span class="p">,</span>
        <span class="n">default_task</span><span class="p">,</span>
        <span class="n">solo_task</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">lengthscales</span><span class="p">,</span>
        <span class="n">noise</span><span class="p">,</span>
        <span class="n">factor_task_kernel</span><span class="p">,</span>
        <span class="n">rank_factor_task_kernel</span><span class="p">,</span>
        <span class="n">noise_task_kernel</span><span class="p">,</span>
        <span class="n">device</span><span class="p">,</span>
        <span class="n">tfs_scale</span><span class="p">,</span>
        <span class="n">tfs_lengthscales</span><span class="p">,</span>
        <span class="n">tfs_noise</span><span class="p">,</span>
        <span class="n">tfs_factor_task_kernel</span><span class="p">,</span>
        <span class="n">tfs_noise_task_kernel</span><span class="p">,</span>
        <span class="n">requires_grad_scale</span><span class="p">,</span>
        <span class="n">requires_grad_lengthscales</span><span class="p">,</span>
        <span class="n">requires_grad_noise</span><span class="p">,</span>
        <span class="n">requires_grad_factor_task_kernel</span><span class="p">,</span>
        <span class="n">requires_grad_noise_task_kernel</span><span class="p">,</span>
        <span class="n">shape_batch</span><span class="p">,</span>
        <span class="n">shape_scale</span><span class="p">,</span> 
        <span class="n">shape_lengthscales</span><span class="p">,</span>
        <span class="n">shape_noise</span><span class="p">,</span>
        <span class="n">shape_factor_task_kernel</span><span class="p">,</span> 
        <span class="n">shape_noise_task_kernel</span><span class="p">,</span>
        <span class="n">derivatives</span><span class="p">,</span>
        <span class="n">derivatives_coeffs</span><span class="p">,</span>
        <span class="n">adaptive_nugget</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">assert</span> <span class="p">(</span><span class="mi">1</span><span class="o">&lt;=</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="o">&lt;=</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="ow">not</span> <span class="p">(</span><span class="n">deriv</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span> <span class="k">for</span> <span class="n">deriv</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">derivatives</span><span class="p">):</span> <span class="k">assert</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="o">&gt;=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;using derivatives requires (alpha&gt;=2).all()&quot;</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>




  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.expand", "toc.follow", "navigation.sections", "toc.integrate"], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>