{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Batch Multitask Lattice GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastgp\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (128, 1)\n",
      "y.shape = (2, 4, 128)\n",
      "z.shape = (256, 1)\n"
     ]
    }
   ],
   "source": [
    "def f(l,x):\n",
    "    weights = 2**torch.arange(1,l+2)\n",
    "    return torch.vstack([\n",
    "        (torch.sin(weights*np.pi*x)/weights).sum(1),\n",
    "        (torch.cos(weights*np.pi*x)/weights).sum(1),\n",
    "    ])\n",
    "num_tasks = 4\n",
    "d = 1 # dimension\n",
    "rng = torch.Generator().manual_seed(17)\n",
    "x = torch.rand((2**7,d),generator=rng) # random testing locations\n",
    "y = torch.cat([f(l,x)[:,None,:] for l in range(num_tasks)],1) # true values at random testing locations\n",
    "z = torch.rand((2**8,d),generator=rng) # other random locations at which to evaluate covariance\n",
    "print(\"x.shape = %s\"%str(tuple(x.shape)))\n",
    "print(\"y.shape = %s\"%str(tuple(y.shape)))\n",
    "print(\"z.shape = %s\"%str(tuple(z.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Fast GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "\tx_next[0].shape = (16, 1)\n",
      "\ty_next[0].shape = (2, 16)\n",
      "i = 1\n",
      "\tx_next[1].shape = (32, 1)\n",
      "\ty_next[1].shape = (2, 32)\n",
      "i = 2\n",
      "\tx_next[2].shape = (64, 1)\n",
      "\ty_next[2].shape = (2, 64)\n",
      "i = 3\n",
      "\tx_next[3].shape = (128, 1)\n",
      "\ty_next[3].shape = (2, 128)\n"
     ]
    }
   ],
   "source": [
    "fgp = fastgp.FastGPLattice(d,seed_for_seq=7,num_tasks=num_tasks,shape_batch=[2,])\n",
    "x_next = fgp.get_x_next(n=2**torch.arange(4,4+num_tasks))\n",
    "y_next = [f(l,x_next[l]) for l in range(num_tasks)]\n",
    "fgp.add_y_next(y_next)\n",
    "for i in range(len(x_next)):\n",
    "    print(\"i = %d\"%i)\n",
    "    print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))\n",
    "    print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pmean.shape = (2, 4, 128)\n",
      "l2 relative error:\n",
      "tensor([[2.3902e-02, 5.6744e-04, 4.1042e-05, 8.4138e-06],\n",
      "        [2.3065e-02, 7.6313e-04, 4.6725e-05, 9.5641e-06]])\n"
     ]
    }
   ],
   "source": [
    "pmean = fgp.post_mean(x)\n",
    "print(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\n",
    "print(\"l2 relative error:\\n%s\"%str(torch.linalg.norm(y-pmean,dim=-1)/torch.linalg.norm(y,dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     iter of 5.0e+03 | NMLL       | norm term  | logdet term\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "            0.00e+00 | -1.86e+03  | 1.86e+01   | -2.76e+03 \n",
      "            5.00e+00 | -2.92e+03  | 2.23e+02   | -4.02e+03 \n",
      "            1.00e+01 | -3.08e+03  | 5.74e+02   | -4.53e+03 \n",
      "            1.50e+01 | -3.12e+03  | 4.48e+02   | -4.45e+03 \n",
      "            2.00e+01 | -3.14e+03  | 4.73e+02   | -4.50e+03 \n",
      "            2.50e+01 | -3.15e+03  | 4.85e+02   | -4.52e+03 \n",
      "            3.00e+01 | -3.15e+03  | 4.91e+02   | -4.52e+03 \n",
      "            3.50e+01 | -3.15e+03  | 4.79e+02   | -4.51e+03 \n",
      "            4.00e+01 | -3.15e+03  | 4.76e+02   | -4.51e+03 \n",
      "            4.50e+01 | -3.15e+03  | 4.72e+02   | -4.51e+03 \n",
      "            5.00e+01 | -3.15e+03  | 4.81e+02   | -4.52e+03 \n",
      "            5.50e+01 | -3.15e+03  | 4.70e+02   | -4.51e+03 \n",
      "            6.00e+01 | -3.15e+03  | 4.71e+02   | -4.51e+03 \n",
      "            6.50e+01 | -3.15e+03  | 4.74e+02   | -4.51e+03 \n",
      "            7.00e+01 | -3.15e+03  | 4.79e+02   | -4.51e+03 \n",
      "            7.50e+01 | -3.15e+03  | 4.76e+02   | -4.51e+03 \n",
      "            8.00e+01 | -3.15e+03  | 4.77e+02   | -4.51e+03 \n",
      "            8.50e+01 | -3.15e+03  | 4.66e+02   | -4.50e+03 \n",
      "            9.00e+01 | -3.16e+03  | 4.73e+02   | -4.51e+03 \n",
      "            9.50e+01 | -3.16e+03  | 4.75e+02   | -4.51e+03 \n",
      "            1.00e+02 | -3.16e+03  | 4.78e+02   | -4.52e+03 \n",
      "            1.05e+02 | -3.16e+03  | 4.66e+02   | -4.50e+03 \n",
      "            1.10e+02 | -3.16e+03  | 4.76e+02   | -4.52e+03 \n",
      "            1.15e+02 | -3.16e+03  | 4.93e+02   | -4.53e+03 \n",
      "            1.20e+02 | -3.16e+03  | 4.74e+02   | -4.51e+03 \n",
      "            1.25e+02 | -3.16e+03  | 4.72e+02   | -4.51e+03 \n",
      "            1.30e+02 | -3.16e+03  | 4.74e+02   | -4.51e+03 \n",
      "            1.35e+02 | -3.16e+03  | 4.77e+02   | -4.52e+03 \n",
      "            1.40e+02 | -3.16e+03  | 4.65e+02   | -4.51e+03 \n",
      "            1.45e+02 | -3.16e+03  | 4.76e+02   | -4.52e+03 \n",
      "            1.50e+02 | -3.16e+03  | 4.64e+02   | -4.51e+03 \n",
      "            1.55e+02 | -3.16e+03  | 4.74e+02   | -4.52e+03 \n",
      "            1.60e+02 | -3.16e+03  | 4.77e+02   | -4.52e+03 \n",
      "            1.65e+02 | -3.16e+03  | 4.79e+02   | -4.53e+03 \n",
      "            1.70e+02 | -3.16e+03  | 4.97e+02   | -4.54e+03 \n",
      "            1.75e+02 | -3.16e+03  | 4.82e+02   | -4.53e+03 \n",
      "            1.80e+02 | -3.16e+03  | 4.72e+02   | -4.52e+03 \n",
      "            1.85e+02 | -3.17e+03  | 4.74e+02   | -4.52e+03 \n",
      "            1.90e+02 | -3.17e+03  | 4.78e+02   | -4.53e+03 \n",
      "            1.95e+02 | -3.17e+03  | 4.80e+02   | -4.53e+03 \n",
      "            2.00e+02 | -3.17e+03  | 4.79e+02   | -4.53e+03 \n",
      "            2.05e+02 | -3.17e+03  | 4.74e+02   | -4.52e+03 \n",
      "            2.10e+02 | -3.17e+03  | 4.77e+02   | -4.53e+03 \n",
      "            2.15e+02 | -3.17e+03  | 4.80e+02   | -4.53e+03 \n",
      "            2.20e+02 | -3.18e+03  | 4.80e+02   | -4.54e+03 \n",
      "            2.25e+02 | -3.18e+03  | 4.70e+02   | -4.53e+03 \n",
      "            2.30e+02 | -3.18e+03  | 4.67e+02   | -4.53e+03 \n",
      "            2.35e+02 | -3.18e+03  | 4.79e+02   | -4.54e+03 \n",
      "            2.40e+02 | -3.18e+03  | 4.79e+02   | -4.54e+03 \n",
      "            2.45e+02 | -3.18e+03  | 4.80e+02   | -4.54e+03 \n",
      "            2.50e+02 | -3.18e+03  | 4.80e+02   | -4.55e+03 \n",
      "            2.55e+02 | -3.18e+03  | 5.07e+02   | -4.57e+03 \n",
      "            2.60e+02 | -3.19e+03  | 4.83e+02   | -4.55e+03 \n",
      "            2.65e+02 | -3.19e+03  | 4.79e+02   | -4.55e+03 \n",
      "            2.70e+02 | -3.19e+03  | 4.71e+02   | -4.54e+03 \n",
      "            2.75e+02 | -3.19e+03  | 4.73e+02   | -4.54e+03 \n",
      "            2.80e+02 | -3.19e+03  | 4.73e+02   | -4.54e+03 \n",
      "            2.85e+02 | -3.19e+03  | 4.73e+02   | -4.54e+03 \n",
      "            2.90e+02 | -3.19e+03  | 4.75e+02   | -4.55e+03 \n",
      "            2.95e+02 | -3.20e+03  | 4.78e+02   | -4.56e+03 \n",
      "            3.00e+02 | -3.21e+03  | 4.78e+02   | -4.57e+03 \n",
      "            3.05e+02 | -3.24e+03  | 4.80e+02   | -4.61e+03 \n",
      "            3.10e+02 | -3.08e+03  | 1.03e+03   | -4.98e+03 \n",
      "            3.15e+02 | -3.26e+03  | 5.18e+02   | -4.66e+03 \n",
      "            3.20e+02 | -3.26e+03  | 5.33e+02   | -4.67e+03 \n",
      "            3.25e+02 | -3.26e+03  | 4.96e+02   | -4.64e+03 \n",
      "            3.30e+02 | -3.26e+03  | 4.73e+02   | -4.61e+03 \n",
      "            3.35e+02 | -3.26e+03  | 4.76e+02   | -4.62e+03 \n",
      "            3.37e+02 | -3.26e+03  | 4.76e+02   | -4.62e+03 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mll_hist', 'scale_hist', 'lengthscales_hist', 'task_kernel_hist']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fgp.fit()\n",
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pmean.shape = (2, 4, 128)\n",
      "pvar.shape = (4, 128)\n",
      "q = 2.58\n",
      "ci_low.shape = (2, 4, 128)\n",
      "ci_high.shape = (2, 4, 128)\n",
      "l2 relative error:\n",
      "tensor([[3.1859e-04, 4.2987e-05, 2.4242e-05, 7.9237e-06],\n",
      "        [2.9426e-04, 5.1505e-05, 2.3703e-05, 8.3381e-06]])\n",
      "pcov.shape = (4, 4, 128, 128)\n",
      "pcov2.shape = (4, 4, 128, 256)\n"
     ]
    }
   ],
   "source": [
    "pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99)\n",
    "print(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\n",
    "print(\"pvar.shape = %s\"%str(tuple(pvar.shape)))\n",
    "print(\"q = %.2f\"%q)\n",
    "print(\"ci_low.shape = %s\"%str(tuple(ci_low.shape)))\n",
    "print(\"ci_high.shape = %s\"%str(tuple(ci_high.shape)))\n",
    "print(\"l2 relative error:\\n%s\"%str(torch.linalg.norm(y-pmean,dim=-1)/torch.linalg.norm(y,dim=-1)))\n",
    "pcov = fgp.post_cov(x,x)\n",
    "print(\"pcov.shape = %s\"%str(tuple(pcov.shape)))\n",
    "_range0,_rangen1 = torch.arange(pcov.size(0)),torch.arange(pcov.size(-1))\n",
    "assert torch.allclose(pcov[_range0,_range0][:,_rangen1,_rangen1],pvar) and (pvar>=0).all()\n",
    "pcov2 = fgp.post_cov(x,z)\n",
    "print(\"pcov2.shape = %s\"%str(tuple(pcov2.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcmean:\n",
      "tensor([[-2.4317e-18,  1.3818e-17, -3.7545e-17, -7.3243e-18],\n",
      "        [ 4.2735e-18,  1.2888e-17,  3.9807e-17, -2.9305e-17]])\n",
      "\n",
      "pcvar:\n",
      "tensor([7.2855e-09, 1.0397e-08, 3.9861e-09, 7.0173e-10])\n",
      "\n",
      "cci_low:\n",
      "tensor([[-2.1986e-04, -2.6264e-04, -1.6263e-04, -6.8234e-05],\n",
      "        [-2.1986e-04, -2.6264e-04, -1.6263e-04, -6.8234e-05]])\n",
      "\n",
      "cci_high:\n",
      "tensor([[2.1986e-04, 2.6264e-04, 1.6263e-04, 6.8234e-05],\n",
      "        [2.1986e-04, 2.6264e-04, 1.6263e-04, 6.8234e-05]])\n"
     ]
    }
   ],
   "source": [
    "pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99)\n",
    "print(\"pcmean:\\n%s\"%str(pcmean))\n",
    "print(\"\\npcvar:\\n%s\"%str(pcvar))\n",
    "print(\"\\ncci_low:\\n%s\"%str(cci_low))\n",
    "print(\"\\ncci_high:\\n%s\"%str(cci_high))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project and Increase Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_new = fgp.n*2**torch.arange(num_tasks-1,-1,-1)\n",
    "pcov_future = fgp.post_cov(x,z,n=n_new)\n",
    "pvar_future = fgp.post_var(x,n=n_new)\n",
    "pcvar_future = fgp.post_cubature_var(n=n_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 112])\n",
      "torch.Size([2, 96])\n",
      "torch.Size([2, 64])\n",
      "torch.Size([2, 0])\n",
      "l2 relative error:\n",
      "tensor([[1.2902e-07, 5.1031e-07, 2.0854e-06, 5.6811e-06],\n",
      "        [1.4225e-07, 5.5493e-07, 2.1187e-06, 6.1776e-06]])\n"
     ]
    }
   ],
   "source": [
    "x_next = fgp.get_x_next(n_new)\n",
    "y_next = [f(l,x_next[l]) for l in range(num_tasks)]\n",
    "for _y in y_next:\n",
    "    print(_y.shape)\n",
    "fgp.add_y_next(y_next)\n",
    "print(\"l2 relative error:\\n%s\"%str(torch.linalg.norm(y-fgp.post_mean(x),dim=-1)/torch.linalg.norm(y,dim=-1)))\n",
    "assert torch.allclose(fgp.post_cov(x,z),pcov_future)\n",
    "assert torch.allclose(fgp.post_var(x),pvar_future)\n",
    "assert torch.allclose(fgp.post_cubature_var(),pcvar_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2 relative error:\n",
      "tensor([[1.0450e-07, 3.3484e-07, 1.1859e-06, 5.3415e-06],\n",
      "        [1.1310e-07, 3.5550e-07, 1.2836e-06, 5.8452e-06]])\n"
     ]
    }
   ],
   "source": [
    "data = fgp.fit(verbose=False,store_mll_hist=False,store_scale_hist=False,store_lengthscales_hist=False,store_noise_hist=False)\n",
    "print(\"l2 relative error:\\n%s\"%str(torch.linalg.norm(y-fgp.post_mean(x),dim=-1)/torch.linalg.norm(y,dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_new = fgp.n*2**torch.arange(num_tasks)\n",
    "pcov_new = fgp.post_cov(x,z,n=n_new)\n",
    "pvar_new = fgp.post_var(x,n=n_new)\n",
    "pcvar_new = fgp.post_cubature_var(n=n_new)\n",
    "x_next = fgp.get_x_next(n_new)\n",
    "y_next = [f(l,x_next[l]) for l in range(num_tasks)]\n",
    "fgp.add_y_next(y_next)\n",
    "assert torch.allclose(fgp.post_cov(x,z),pcov_new)\n",
    "assert torch.allclose(fgp.post_var(x),pvar_new)\n",
    "assert torch.allclose(fgp.post_cubature_var(),pcvar_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fgp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
