{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Batch Multitask Net GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastgp\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (128, 1)\n",
      "y.shape = (2, 4, 128)\n",
      "z.shape = (256, 1)\n"
     ]
    }
   ],
   "source": [
    "def f(l,x):\n",
    "    weights = 2**torch.arange(1,l+2)\n",
    "    return torch.vstack([\n",
    "        (torch.sin(weights*np.pi*x)/weights).sum(1),\n",
    "        (torch.cos(weights*np.pi*x)/weights).sum(1),\n",
    "    ])\n",
    "num_tasks = 4\n",
    "d = 1 # dimension\n",
    "rng = torch.Generator().manual_seed(17)\n",
    "x = torch.rand((2**7,d),generator=rng) # random testing locations\n",
    "y = torch.cat([f(l,x)[:,None,:] for l in range(num_tasks)],1) # true values at random testing locations\n",
    "z = torch.rand((2**8,d),generator=rng) # other random locations at which to evaluate covariance\n",
    "print(\"x.shape = %s\"%str(tuple(x.shape)))\n",
    "print(\"y.shape = %s\"%str(tuple(y.shape)))\n",
    "print(\"z.shape = %s\"%str(tuple(z.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Fast GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "\tx_next[0].shape = (32, 1)\n",
      "\ty_next[0].shape = (2, 32)\n",
      "i = 1\n",
      "\tx_next[1].shape = (64, 1)\n",
      "\ty_next[1].shape = (2, 64)\n",
      "i = 2\n",
      "\tx_next[2].shape = (128, 1)\n",
      "\ty_next[2].shape = (2, 128)\n",
      "i = 3\n",
      "\tx_next[3].shape = (256, 1)\n",
      "\ty_next[3].shape = (2, 256)\n"
     ]
    }
   ],
   "source": [
    "fgp = fastgp.FastGPDigitalNetB2(d,seed_for_seq=7,num_tasks=num_tasks)\n",
    "x_next = fgp.get_x_next(n=2**torch.arange(5,5+num_tasks))\n",
    "y_next = [f(l,x_next[l]) for l in range(num_tasks)]\n",
    "fgp.add_y_next(y_next)\n",
    "assert len(x_next)==len(y_next)\n",
    "for i in range(len(x_next)):\n",
    "    print(\"i = %d\"%i)\n",
    "    print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))\n",
    "    print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pmean.shape = (2, 4, 128)\n",
      "l2 relative error:\n",
      "tensor([[0.0781, 0.0383, 0.0265, 0.0227],\n",
      "        [0.0797, 0.0374, 0.0279, 0.0283]])\n"
     ]
    }
   ],
   "source": [
    "pmean = fgp.post_mean(x)\n",
    "print(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\n",
    "print(\"l2 relative error:\\n%s\"%str(torch.linalg.norm(y-pmean,dim=-1)/torch.linalg.norm(y,dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     iter of 5.0e+03 | NMLL       | noise      | scale      | lengthscales         | task_kernel \n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "            0.00e+00 | 5.02e+03   | 1.00e-16   | 1.00e+00   | [1.0e+00]            | [[2.0e+00 1.0e+00 1.0e+00 1.0e+00] [1.0e+00 2.0e+00 1.0e+00 1.0e+00] [1.0e+00 1.0e+00 2.0e+00 1.0e+00] [1.0e+00 1.0e+00 1.0e+00 2.0e+00]]\n",
      "            5.00e+00 | -3.57e+04  | 1.00e-16   | 4.75e-01   | [4.8e-01]            | [[5.4e-01 6.5e-02 6.5e-02 6.5e-02] [6.5e-02 5.4e-01 6.5e-02 6.5e-02] [6.5e-02 6.5e-02 5.4e-01 6.5e-02] [6.5e-02 6.5e-02 6.5e-02 5.4e-01]]\n",
      "            1.00e+01 | -6.87e+04  | 1.00e-16   | 7.46e-02   | [7.5e-02]            | [[9.5e-02 2.0e-02 2.0e-02 -5.2e-03] [2.0e-02 9.5e-02 2.0e-02 -5.2e-03] [2.0e-02 2.0e-02 9.5e-02 -5.2e-03] [-5.2e-03 -5.2e-03 -5.2e-03 7.6e-02]]\n",
      "            1.50e+01 | -9.05e+04  | 1.00e-16   | 1.13e-01   | [1.1e-01]            | [[1.2e-01 7.5e-02 7.5e-02 2.2e-02] [7.5e-02 2.0e-01 1.3e-01 3.8e-02] [7.5e-02 1.3e-01 2.0e-01 3.8e-02] [2.2e-02 3.8e-02 3.8e-02 1.2e-01]]\n",
      "            2.00e+01 | -9.47e+04  | 1.00e-16   | 9.80e-02   | [9.8e-02]            | [[1.5e-01 6.1e-02 7.1e-02 4.9e-02] [6.1e-02 1.2e-01 1.1e-01 7.8e-02] [7.1e-02 1.1e-01 1.5e-01 9.2e-02] [4.9e-02 7.8e-02 9.2e-02 1.3e-01]]\n",
      "            2.50e+01 | -9.63e+04  | 1.00e-16   | 9.40e-02   | [9.4e-02]            | [[1.7e-01 4.3e-02 5.8e-02 5.5e-02] [4.3e-02 1.6e-01 9.9e-02 9.4e-02] [5.8e-02 9.9e-02 1.4e-01 1.3e-01] [5.5e-02 9.4e-02 1.3e-01 1.5e-01]]\n",
      "            3.00e+01 | -9.63e+04  | 1.00e-16   | 9.42e-02   | [9.5e-02]            | [[1.8e-01 4.4e-02 5.8e-02 5.4e-02] [4.4e-02 1.4e-01 9.8e-02 9.1e-02] [5.8e-02 9.8e-02 1.3e-01 1.2e-01] [5.4e-02 9.1e-02 1.2e-01 1.4e-01]]\n",
      "            3.50e+01 | -9.63e+04  | 1.00e-16   | 9.28e-02   | [9.5e-02]            | [[1.8e-01 4.2e-02 5.7e-02 5.2e-02] [4.2e-02 1.4e-01 9.5e-02 8.8e-02] [5.7e-02 9.5e-02 1.3e-01 1.2e-01] [5.2e-02 8.8e-02 1.2e-01 1.4e-01]]\n",
      "            4.00e+01 | -9.64e+04  | 1.00e-16   | 9.35e-02   | [9.6e-02]            | [[1.7e-01 4.3e-02 5.7e-02 5.2e-02] [4.3e-02 1.4e-01 9.5e-02 8.7e-02] [5.7e-02 9.5e-02 1.3e-01 1.2e-01] [5.2e-02 8.7e-02 1.2e-01 1.4e-01]]\n",
      "            4.50e+01 | -9.64e+04  | 1.00e-16   | 9.34e-02   | [9.7e-02]            | [[1.7e-01 4.2e-02 5.7e-02 5.2e-02] [4.2e-02 1.4e-01 9.5e-02 8.7e-02] [5.7e-02 9.5e-02 1.3e-01 1.2e-01] [5.2e-02 8.7e-02 1.2e-01 1.4e-01]]\n",
      "            5.00e+01 | -9.64e+04  | 1.00e-16   | 9.26e-02   | [9.8e-02]            | [[1.7e-01 4.2e-02 5.7e-02 5.1e-02] [4.2e-02 1.4e-01 9.5e-02 8.6e-02] [5.7e-02 9.5e-02 1.3e-01 1.2e-01] [5.1e-02 8.6e-02 1.2e-01 1.4e-01]]\n",
      "            5.50e+01 | -9.64e+04  | 1.00e-16   | 9.15e-02   | [1.0e-01]            | [[1.7e-01 4.2e-02 5.6e-02 5.1e-02] [4.2e-02 1.4e-01 9.4e-02 8.6e-02] [5.6e-02 9.4e-02 1.3e-01 1.2e-01] [5.1e-02 8.6e-02 1.2e-01 1.4e-01]]\n",
      "            6.00e+01 | -9.64e+04  | 1.00e-16   | 8.98e-02   | [1.0e-01]            | [[1.7e-01 4.2e-02 5.6e-02 5.1e-02] [4.2e-02 1.4e-01 9.4e-02 8.5e-02] [5.6e-02 9.4e-02 1.3e-01 1.1e-01] [5.1e-02 8.5e-02 1.1e-01 1.4e-01]]\n",
      "            6.50e+01 | -9.64e+04  | 1.00e-16   | 8.59e-02   | [1.1e-01]            | [[1.7e-01 4.2e-02 5.6e-02 5.1e-02] [4.2e-02 1.4e-01 9.4e-02 8.5e-02] [5.6e-02 9.4e-02 1.3e-01 1.1e-01] [5.1e-02 8.5e-02 1.1e-01 1.3e-01]]\n",
      "            7.00e+01 | -9.64e+04  | 1.00e-16   | 7.69e-02   | [1.2e-01]            | [[1.7e-01 4.2e-02 5.6e-02 5.1e-02] [4.2e-02 1.4e-01 9.3e-02 8.5e-02] [5.6e-02 9.3e-02 1.3e-01 1.1e-01] [5.1e-02 8.5e-02 1.1e-01 1.3e-01]]\n",
      "            7.50e+01 | -9.64e+04  | 1.00e-16   | 5.83e-02   | [1.6e-01]            | [[1.7e-01 4.2e-02 5.6e-02 5.1e-02] [4.2e-02 1.4e-01 9.3e-02 8.5e-02] [5.6e-02 9.3e-02 1.3e-01 1.1e-01] [5.1e-02 8.5e-02 1.1e-01 1.3e-01]]\n",
      "            8.00e+01 | -9.64e+04  | 1.00e-16   | 4.21e-02   | [2.1e-01]            | [[1.7e-01 4.1e-02 5.6e-02 5.1e-02] [4.1e-02 1.4e-01 9.3e-02 8.5e-02] [5.6e-02 9.3e-02 1.3e-01 1.1e-01] [5.1e-02 8.5e-02 1.1e-01 1.3e-01]]\n",
      "            8.50e+01 | -9.65e+04  | 1.00e-16   | 4.28e-02   | [2.1e-01]            | [[1.7e-01 4.1e-02 5.6e-02 5.1e-02] [4.1e-02 1.4e-01 9.3e-02 8.5e-02] [5.6e-02 9.3e-02 1.3e-01 1.1e-01] [5.1e-02 8.5e-02 1.1e-01 1.3e-01]]\n",
      "            9.00e+01 | -9.65e+04  | 1.00e-16   | 4.31e-02   | [2.1e-01]            | [[1.7e-01 4.1e-02 5.6e-02 5.1e-02] [4.1e-02 1.4e-01 9.3e-02 8.5e-02] [5.6e-02 9.3e-02 1.3e-01 1.1e-01] [5.1e-02 8.5e-02 1.1e-01 1.3e-01]]\n",
      "            9.50e+01 | -9.65e+04  | 1.00e-16   | 4.24e-02   | [2.2e-01]            | [[1.7e-01 4.1e-02 5.6e-02 5.1e-02] [4.1e-02 1.4e-01 9.3e-02 8.5e-02] [5.6e-02 9.3e-02 1.3e-01 1.1e-01] [5.1e-02 8.5e-02 1.1e-01 1.3e-01]]\n",
      "            1.00e+02 | -9.65e+04  | 1.00e-16   | 4.05e-02   | [2.3e-01]            | [[1.7e-01 4.1e-02 5.6e-02 5.1e-02] [4.1e-02 1.4e-01 9.3e-02 8.5e-02] [5.6e-02 9.3e-02 1.3e-01 1.1e-01] [5.1e-02 8.5e-02 1.1e-01 1.3e-01]]\n",
      "            1.05e+02 | -9.65e+04  | 1.00e-16   | 3.61e-02   | [2.6e-01]            | [[1.7e-01 4.1e-02 5.6e-02 5.1e-02] [4.1e-02 1.4e-01 9.3e-02 8.5e-02] [5.6e-02 9.3e-02 1.3e-01 1.1e-01] [5.1e-02 8.5e-02 1.1e-01 1.3e-01]]\n",
      "            1.10e+02 | -9.65e+04  | 1.00e-16   | 2.71e-02   | [3.4e-01]            | [[1.7e-01 4.1e-02 5.6e-02 5.1e-02] [4.1e-02 1.4e-01 9.3e-02 8.5e-02] [5.6e-02 9.3e-02 1.3e-01 1.1e-01] [5.1e-02 8.5e-02 1.1e-01 1.3e-01]]\n",
      "            1.15e+02 | -9.66e+04  | 1.00e-16   | 1.33e-02   | [6.9e-01]            | [[1.7e-01 4.1e-02 5.6e-02 5.1e-02] [4.1e-02 1.4e-01 9.3e-02 8.5e-02] [5.6e-02 9.3e-02 1.3e-01 1.1e-01] [5.1e-02 8.5e-02 1.1e-01 1.3e-01]]\n",
      "            1.20e+02 | -9.68e+04  | 1.00e-16   | 2.25e-03   | [4.1e+00]            | [[1.7e-01 4.1e-02 5.6e-02 5.1e-02] [4.1e-02 1.4e-01 9.3e-02 8.5e-02] [5.6e-02 9.3e-02 1.3e-01 1.1e-01] [5.1e-02 8.5e-02 1.1e-01 1.3e-01]]\n",
      "            1.25e+02 | -9.72e+04  | 1.00e-16   | 2.72e-05   | [3.4e+02]            | [[1.7e-01 4.1e-02 5.6e-02 5.1e-02] [4.1e-02 1.4e-01 9.3e-02 8.5e-02] [5.6e-02 9.3e-02 1.3e-01 1.1e-01] [5.1e-02 8.5e-02 1.1e-01 1.3e-01]]\n",
      "            1.30e+02 | -9.26e+04  | 1.00e-16   | 3.65e-07   | [6.1e+04]            | [[1.7e-01 4.1e-02 5.6e-02 5.1e-02] [4.1e-02 1.4e-01 9.3e-02 8.5e-02] [5.6e-02 9.3e-02 1.3e-01 1.1e-01] [5.1e-02 8.5e-02 1.1e-01 1.3e-01]]\n",
      "            1.35e+02 | -9.63e+04  | 1.00e-16   | 2.80e-07   | [4.7e+04]            | [[1.7e-01 4.1e-02 5.6e-02 5.1e-02] [4.1e-02 1.4e-01 9.3e-02 8.5e-02] [5.6e-02 9.3e-02 1.3e-01 1.1e-01] [5.1e-02 8.5e-02 1.1e-01 1.3e-01]]\n",
      "            1.37e+02 | -9.68e+04  | 1.00e-16   | 2.09e-07   | [3.5e+04]            | [[1.7e-01 4.1e-02 5.6e-02 5.1e-02] [4.1e-02 1.4e-01 9.3e-02 8.5e-02] [5.6e-02 9.3e-02 1.3e-01 1.1e-01] [5.1e-02 8.5e-02 1.1e-01 1.3e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mll_hist', 'scale_hist', 'lengthscales_hist', 'task_kernel_hist']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fgp.fit()\n",
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pmean.shape = (2, 4, 128)\n",
      "pvar.shape = (4, 128)\n",
      "q = 2.58\n",
      "ci_low.shape = (2, 4, 128)\n",
      "ci_high.shape = (2, 4, 128)\n",
      "l2 relative error:\n",
      "tensor([[0.0656, 0.0390, 0.0252, 0.0241],\n",
      "        [0.0715, 0.0370, 0.0306, 0.0315]])\n",
      "pcov.shape = (4, 4, 128, 128)\n",
      "pcov2.shape = (4, 4, 128, 256)\n"
     ]
    }
   ],
   "source": [
    "pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99)\n",
    "print(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\n",
    "print(\"pvar.shape = %s\"%str(tuple(pvar.shape)))\n",
    "print(\"q = %.2f\"%q)\n",
    "print(\"ci_low.shape = %s\"%str(tuple(ci_low.shape)))\n",
    "print(\"ci_high.shape = %s\"%str(tuple(ci_high.shape)))\n",
    "print(\"l2 relative error:\\n%s\"%str(torch.linalg.norm(y-pmean,dim=-1)/torch.linalg.norm(y,dim=-1)))\n",
    "pcov = fgp.post_cov(x,x)\n",
    "print(\"pcov.shape = %s\"%str(tuple(pcov.shape)))\n",
    "_range0,_rangen1 = torch.arange(pcov.size(0)),torch.arange(pcov.size(-1))\n",
    "assert torch.allclose(pcov[_range0,_range0][:,_rangen1,_rangen1],pvar) and (pvar>=0).all()\n",
    "pcov2 = fgp.post_cov(x,z)\n",
    "print(\"pcov2.shape = %s\"%str(tuple(pcov2.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcmean:\n",
      "tensor([[ 1.2705e-20,  9.0633e-20,  4.9551e-20,  4.6587e-20],\n",
      "        [-1.8381e-19, -3.1001e-19, -3.7439e-19, -3.8625e-19]])\n",
      "\n",
      "pcvar:\n",
      "tensor([3.5095e-08, 2.8624e-08, 2.5938e-08, 2.7794e-08])\n",
      "\n",
      "cci_low:\n",
      "tensor([[-0.0005, -0.0004, -0.0004, -0.0004],\n",
      "        [-0.0005, -0.0004, -0.0004, -0.0004]])\n",
      "\n",
      "cci_high:\n",
      "tensor([[0.0005, 0.0004, 0.0004, 0.0004],\n",
      "        [0.0005, 0.0004, 0.0004, 0.0004]])\n"
     ]
    }
   ],
   "source": [
    "pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99)\n",
    "print(\"pcmean:\\n%s\"%str(pcmean))\n",
    "print(\"\\npcvar:\\n%s\"%str(pcvar))\n",
    "print(\"\\ncci_low:\\n%s\"%str(cci_low))\n",
    "print(\"\\ncci_high:\\n%s\"%str(cci_high))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project and Increase Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_new = fgp.n*2**torch.arange(num_tasks-1,-1,-1)\n",
    "pcov_future = fgp.post_cov(x,z,n=n_new)\n",
    "pvar_future = fgp.post_var(x,n=n_new)\n",
    "pcvar_future = fgp.post_cubature_var(n=n_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 224])\n",
      "torch.Size([2, 192])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 0])\n",
      "l2 relative error:\n",
      "tensor([[0.0163, 0.0163, 0.0195, 0.0239],\n",
      "        [0.0157, 0.0180, 0.0228, 0.0321]])\n"
     ]
    }
   ],
   "source": [
    "x_next = fgp.get_x_next(n_new)\n",
    "y_next = [f(l,x_next[l]) for l in range(num_tasks)]\n",
    "for _y in y_next:\n",
    "    print(_y.shape)\n",
    "fgp.add_y_next(y_next)\n",
    "print(\"l2 relative error:\\n%s\"%str(torch.linalg.norm(y-fgp.post_mean(x),dim=-1)/torch.linalg.norm(y,dim=-1)))\n",
    "assert torch.allclose(fgp.post_cov(x,z),pcov_future)\n",
    "assert torch.allclose(fgp.post_var(x),pvar_future)\n",
    "assert torch.allclose(fgp.post_cubature_var(),pcvar_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2 relative error:\n",
      "tensor([[0.0165, 0.0161, 0.0200, 0.0240],\n",
      "        [0.0154, 0.0171, 0.0225, 0.0312]])\n"
     ]
    }
   ],
   "source": [
    "data = fgp.fit(verbose=False,store_mll_hist=False,store_scale_hist=False,store_lengthscales_hist=False,store_noise_hist=False)\n",
    "print(\"l2 relative error:\\n%s\"%str(torch.linalg.norm(y-fgp.post_mean(x),dim=-1)/torch.linalg.norm(y,dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_new = fgp.n*2**torch.arange(num_tasks)\n",
    "pcov_new = fgp.post_cov(x,z,n=n_new)\n",
    "pvar_new = fgp.post_var(x,n=n_new)\n",
    "pcvar_new = fgp.post_cubature_var(n=n_new)\n",
    "x_next = fgp.get_x_next(n_new)\n",
    "y_next = [f(l,x_next[l]) for l in range(num_tasks)]\n",
    "fgp.add_y_next(y_next)\n",
    "assert torch.allclose(fgp.post_cov(x,z),pcov_new)\n",
    "assert torch.allclose(fgp.post_var(x),pvar_new)\n",
    "assert torch.allclose(fgp.post_cubature_var(),pcvar_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fgp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
